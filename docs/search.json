[
  {
    "objectID": "tutorials/challenge_two_submission.html",
    "href": "tutorials/challenge_two_submission.html",
    "title": "3. Create submissions",
    "section": "",
    "text": "In this notebook we will explore the datasets of the AnDi 2 challenge and show how to properly create a submission. We will consider that you have already read the paper and know the basics of the challenge. This notebook has three sections:"
  },
  {
    "objectID": "tutorials/challenge_two_submission.html#reading-videos-and-trajectories-the-public-data",
    "href": "tutorials/challenge_two_submission.html#reading-videos-and-trajectories-the-public-data",
    "title": "3. Create submissions",
    "section": "Reading videos and trajectories: the public data",
    "text": "Reading videos and trajectories: the public data\nFirst things first: you need to download the public data to perform the predictions available in the competition’s Codalab webpage (link not yet available). We will showcase here how to do so in the Development phase data. For that, go to the competition, then Participate > Files and download the Public Data for Phase #1 Development. Once unzipped, the dataset should have the following file structure:\npublic_dat\n│\n└─── track_1 (videos)\n|   │\n|   └─── exp_Y\n│        │\n│        └─── videos_fov_X.tiff (video for each fov)\n│ \n│\n└─── track_2 (trajectories)\n    │\n    └─── exp_Y\n        │\n        └─── traj_fov_X.csv (trajectories for each FOV) \nwhere Y goes from 0 to 9 and X from 0 to 29. This means that we have 10 experiments with 30 FOVs each.\n\npublic_data_path = 'public_data/' # make sure the folder has this name or change it\n\n\nTrack 1: videos\nTrack 1 focuses on videos. Each of the tiffs contains a video mimicking a typical single particle tracking experiment (see the paper for details). Let’s load one of the videos. You can do so with the following function:\n\nfrom andi_datasets.utils_videos import import_tiff_video\nimport matplotlib.pyplot as plt\nimport numpy as np\nnp.random.seed(0)\n\nvideo = import_tiff_video(public_data_path+'track_1/exp_0/videos_fov_1.tiff')\n\nThe first frame contains the label of the VIP particles. The pixel indicates the initial position of the particle and the value provides the particle’s index.\n\nVIP particles are the ones you will need to characterize in the single trajectory task (see more on this below).\n\n\nplt.matshow(video[0])\nplt.xlabel('pixel');plt.ylabel('pixel');\n\n\n\n\nTo access the indices of the VIP particles, take the unique values of the initial frame:\n\nnp.unique(video[0])\n\narray([ 16,  18,  19,  20,  21,  22,  24,  27,  28,  29, 255], dtype=uint8)\n\n\nFrom the previous numbers, 255 is the background and the rest are the indices of the VIP particles.\nWe can visualize the videos with the built-in function play_video.\n\nfrom andi_datasets.utils_videos import play_video\n\nLet’s see how the video looks like! play_video expects an object with shape (num_frames, pixels, pixels, channels). In this case, we will need to add an additional axis for the channels (grayscale).\n\nYou may need to install pillow (simply pip install pillow) to play the following video.\n\n\nplay_video(video[1:,:,:,np.newaxis])\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nAs you can see, there many more particles besides the VIP particles. These will help you find the ensemble properties of the experiment (more on this below).\n\n\nTrack 2: trajectories\nTrack 2 focuses on trajectories, as we did in the previous challenge. The trajectories come in a csv file with four columns: - traj_idx: index of the trajectory - frame: frame at which each of the time steps was recorded - x and y: pixel x and y position.\nLet’s look at the data:\n\nimport pandas as pd\n\ndf = pd.read_csv(public_data_path+'track_2/exp_0/trajs_fov_0.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      traj_idx\n      frame\n      x\n      y\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      93.654332\n      104.089646\n    \n    \n      1\n      0.0\n      1.0\n      93.602970\n      103.040118\n    \n    \n      2\n      0.0\n      2.0\n      94.925046\n      104.376142\n    \n    \n      3\n      0.0\n      3.0\n      93.559753\n      102.780234\n    \n    \n      4\n      0.0\n      4.0\n      92.713807\n      102.126589\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4429\n      25.0\n      195.0\n      179.430772\n      132.665685\n    \n    \n      4430\n      25.0\n      196.0\n      175.963130\n      130.836672\n    \n    \n      4431\n      25.0\n      197.0\n      177.304783\n      129.842511\n    \n    \n      4432\n      25.0\n      198.0\n      176.402691\n      131.515115\n    \n    \n      4433\n      25.0\n      199.0\n      175.538531\n      132.183690\n    \n  \n\n4434 rows × 4 columns\n\n\n\nSimilar to the previous example, the trajectory indices can be obtained with:\n\ntraj_idx = df.traj_idx.unique()\n\nThe data for each trajectory can be accessed by masking over the trajectory index.\n\nidx = 0\nmask = df.traj_idx == idx\ntraj = df[mask]\n\n\ntraj\n\n\n\n\n\n  \n    \n      \n      traj_idx\n      frame\n      x\n      y\n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      93.654332\n      104.089646\n    \n    \n      1\n      0.0\n      1.0\n      93.602970\n      103.040118\n    \n    \n      2\n      0.0\n      2.0\n      94.925046\n      104.376142\n    \n    \n      3\n      0.0\n      3.0\n      93.559753\n      102.780234\n    \n    \n      4\n      0.0\n      4.0\n      92.713807\n      102.126589\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      195\n      0.0\n      195.0\n      89.878535\n      101.287746\n    \n    \n      196\n      0.0\n      196.0\n      89.408663\n      101.777232\n    \n    \n      197\n      0.0\n      197.0\n      91.881616\n      102.291826\n    \n    \n      198\n      0.0\n      198.0\n      91.168064\n      102.299316\n    \n    \n      199\n      0.0\n      199.0\n      89.729272\n      104.437141\n    \n  \n\n200 rows × 4 columns"
  },
  {
    "objectID": "tutorials/challenge_two_submission.html#creating-and-submitting-predictions",
    "href": "tutorials/challenge_two_submission.html#creating-and-submitting-predictions",
    "title": "3. Create submissions",
    "section": "Creating and submitting predictions",
    "text": "Creating and submitting predictions\nNow that you know how to read the data, let’s make some predictions! For the sake of this tutorial, we will generate dummy data to populate a fake submission file. The submission file must be structured as follows:\nPATH\n│\n└─── track_1 \n|   │\n|   └─── exp_Y\n│        │\n│        └─── fov_X.txt\n│        │\n│        └─── ensemble_labels.txt\n│ \n│\n└─── track_2 \n    │\n    └─── exp_Y\n        │\n        └─── fov_X.txt\n        │\n        └─── ensemble_labels.txt\nThe fov_X.txt files contain the individual predictions for each FOV: every VIP particle in track 1, and every single trajectory in track 2. The ensemble_labels.txt contains the predictions at the ensemble level for the whole experiment.\nIn our case, we will gather our submission files in the following root path:\n\nimport os\npath_results = 'res/'\nif not os.path.exists(path_results):\n    os.makedirs(path_results)\n\nImportant: You can choose to which track / task you want to participate. For example:\n-> If you want to focus on Track 1, your submission only needs to contain the folder track_1.\n-> If you want to focus on the Ensemble task, the folders only need to contain the ensemble_labels.txt file (no fov_X.txt needed)\n\nEnsemble task\nIn the ensemble task, you must provide the diffusive properties of each state present in the dataset. The submission file should have the following structure:\n\n\n\n\n\n\n\n\n\nmodel: modelXXX;\nnum_state: YYY\n\n\n\n\n\n\n\\(\\mu_\\alpha^1\\);\n\\(\\mu_\\alpha^2\\);\n\\(\\mu_\\alpha^3\\);\n…\n\n\n\\(\\sigma_\\alpha^1\\);\n\\(\\sigma_\\alpha^2\\);\n\\(\\sigma_\\alpha^3\\);\n…\n\n\n\\(\\mu_K^1\\);\n\\(\\mu_K^2\\);\n\\(\\mu_K^3\\);\n…\n\n\n\\(\\sigma_K^1\\);\n\\(\\sigma_K^2\\);\n\\(\\sigma_K^3\\);\n…\n\n\n\\(N_1\\);\n\\(N_2\\);\n\\(N_3\\);\n…\n\n\n\nThe first row contains the diffusion model prediction for the experiment and the number of diffusive states. Below, every column contains the properties for each of the predicted diffusive states:\n\n\\(\\mu_\\alpha^i\\), \\(\\sigma_\\alpha^i\\): mean and variance of the anomalous diffusion exponent.\n\\(\\mu_K^i\\), \\(\\sigma_K^i\\): mean and variance of the diffusion coefficient.\n\\(N_i\\): relative weight of the state (e.g. time spent in it).\n\nIt is important to use ; as delimiter of the file.\nFor the model prediction, the available diffusion models are (and should be written as):\n\nfrom andi_datasets.datasets_phenom import datasets_phenom\ndatasets_phenom().avail_models_name\n\n['single_state',\n 'multi_state',\n 'immobile_traps',\n 'dimerization',\n 'confinement']\n\n\nBoth \\(\\alpha\\) and \\(D\\) are bound to some minimal and maximal values:\n\nfrom andi_datasets.models_phenom import models_phenom\nprint(f' Min, max D: {models_phenom().bound_D}\\n',\n      f'Min, max alpha: {models_phenom().bound_alpha}')\n\n Min, max D: [1e-12, 1000000.0]\n Min, max alpha: [0, 1.999]\n\n\nThe state weights DO NOT need to be normalized, the scoring program will take care of normalizing by dividing each weight \\(N_i\\) by their sum (i.e., \\(\\sum_i N_i\\)).\nLet’s create a dummy submission. For each track (1 and 2) and experiment (10 in this example), we will create a file with two states and random parameters:\n\nfor track in [1,2]:\n    \n    # Create the folder of the track if it does not exists\n    path_track = path_results + f'track_{track}/'\n    if not os.path.exists(path_track):\n        os.makedirs(path_track)\n        \n    for exp in range(10):\n        # Create the folder of the experiment if it does not exits\n        path_exp = path_track+f'exp_{exp}/'\n        if not os.path.exists(path_exp):\n            os.makedirs(path_exp)\n        file_name = path_exp + 'ensemble_labels.txt'\n        \n        with open(file_name, 'a') as f:\n            # Save the model (random) and the number of states (2 in this case)\n            model_name = np.random.choice(datasets_phenom().avail_models_name, size = 1)[0]\n            f.write(f'model: {model_name}; num_state: {2} \\n')\n\n            # Create some dummy data for 2 states. This means 2 columns\n            # and 5 rows\n            data = np.random.rand(5, 2)\n            \n            data[-1,:] /= data[-1,:].sum()\n\n            # Save the data in the corresponding ensemble file\n            np.savetxt(f, data, delimiter = ';')\n\nFor instance the file res/track_1/exp_0/ensemble_labels.txt should look something like:\nmodel: confinement; num_state: 2 \n5.928446182250183272e-01;8.442657485810173279e-01\n8.579456176227567843e-01;8.472517387841254077e-01\n6.235636967859723434e-01;3.843817072926998257e-01\n2.975346065444722798e-01;5.671297731744318060e-02\n3.633859973276627464e-01;6.366140026723372536e-01\n\n\nSingle trajectory task\nIn this track, the main objective is to predict the transient properties of each individual trajectory. The trajectories are made of segments with, at least, 5 frames. Your goal is to predict the diffusion coefficient, anomalous exponent and changepoint of each of the segments. The prediction file should look like:\nidx_traj1, K_1, alpha_1, state_1, CP_1, K_2, alpha_2, .... state_N, T\nidx_traj2, K_1, alpha_1, state_1, CP_1, K_2, alpha_2, .... state_N, T\nidx_traj3, K_1, alpha_1, state_1, CP_1, K_2, alpha_2, .... state_N, T\n...\nBeware of the comma , delimiter here. idx_trajX is the index of the trajectory, and K_x, alpha_x and state_x are the diffusion coefficient, anomalous exponent and diffusive state of the \\(x\\)-th. T is the total length of the trajectory.\n\nImportant: for Track 1 (videos), the predictions must only be done for the VIP particles. The idx_traj should coincide with the index given in the first frame of the .tiff file, as done above. For Track 2 (trajectories), you must predict all the trajectories in the .csv and make sure to correctly match the indices given there to the ones you write in the submission file.\n\nFor \\(K\\) and \\(\\alpha\\), the bounds are exactly the same as the ones showed above. For the states, we consider 4 different states, each represent by a number:\n\n0: Immobile, the particle is spatially trapped. This corresponds to the quenched-trap model (QTM) (models_phenom.immobile_traps in the code)\n1: Confined, the particle is confined within a disk of certain radius. This corresponds to the transient-confinement model (TCM) (models_phenom.confinement in the code)\n2: Free, the particle diffuses normally or anomalously without constraints. Multiple models can generate these states, e.g. single-state model (SSM), Multi-state model (MSM) or the dimerization model (DIM) (models_phenom.single_state, models_phenom.multi_state and models_phenom.dimerization, respectively).\n3: Directed, the particle diffuses with an \\(\\alpha \\geq 1.9\\). All the models above can produce these states.\n\nTo learn more about each model and state, you can check this tutorial.\nAs before, we will create a dummy submission. To illustrate the fact that you can participate just in some Tracks / Tasks, we will generate predictions only for Track 2. While we will not do predictions for Track 1 here, remember that for that track you only need to predict the VIP particles!\n\n# Define the number of experiments and number of FOVS\nN_EXP = 10 \nN_FOVS = 30\n\n# We only to track 2 in this example\ntrack = 2\n\n# The results go in the same folders generated above\npath_results = 'res/'\npath_track = path_results + f'track_{track}/'\n\nfor exp in range(N_EXP):\n    \n    path_exp = path_track + f'exp_{exp}/'\n    \n    for fov in range(N_FOVS):\n        \n        # We read the corresponding csv file from the public data and extract the indices of the trajectories:\n        df = pd.read_csv(public_data_path+f'track_2/exp_{exp}/trajs_fov_{fov}.csv')\n        traj_idx = df.traj_idx.unique()\n        \n        submission_file = path_exp + f'fov_{fov}.txt'\n        \n        with open(submission_file, 'a') as f:\n            \n            # Loop over each index\n            for idx in traj_idx:\n                \n                # Get the lenght of the trajectory\n                length_traj = df[df.traj_idx == traj_idx[0]].shape[0]\n                # Assign one changepoints for each traj at 0.25 of its length\n                CP = int(length_traj*0.25)\n                \n                prediction_traj = [idx.astype(int), \n                                   np.random.rand()*10, # K1\n                                   np.random.rand(), # alpha1\n                                   np.random.randint(4), # state1\n                                   CP, # changepoint\n                                   np.random.rand()*10, # K2\n                                   np.random.rand(), # alpha2\n                                   np.random.randint(4), # state2\n                                   length_traj # Total length of the trajectory\n                                  ]\n                \n                formatted_numbers = ','.join(map(str, prediction_traj))\n                f.write(formatted_numbers + '\\n')\n\nThe first lines of res/track_2/exp_0/fov_0.txt should be:\n0,3.2001715082246784,0.38346389417189797,3,50,3.2468297206646533,0.5197111936582762,0,200\n1,8.726506554473954,0.27354203481563577,1,50,8.853376596095856,0.6798794564067695,3,200\n2,6.874882763878153,0.21550767711355845,1,50,2.294418344710454,0.8802976031150046,0,200\n\n\nCreating the submission and uploading to codalab\nNow you are ready to create a submission file! In order to upload the submission to Codalab, you will need to zip the folder.\n\nImportant: You must compress the file so that track_1 and/or track_2 are directly on the parent folder of the compressed file. Be careful not compress the parent folder and then have a zip that has a res folder in the first level with the track folders inside!\n\nThen, go to the competition webpage in Codalab > Participate > Submit/View Results and use the submission button to submit the compressed file. After some time, you will see the results in the Leaderboard. If something went wrong, you will be able to see it in the same Submission page."
  },
  {
    "objectID": "tutorials/challenge_two_submission.html#create-and-work-on-your-own-datasets",
    "href": "tutorials/challenge_two_submission.html#create-and-work-on-your-own-datasets",
    "title": "3. Create submissions",
    "section": "Create and work on your own datasets",
    "text": "Create and work on your own datasets\nIn the Codalab webpage we also provide a Starting Kit, containing both videos and trajectories, but most importantly their respective labels. You can use these to train your own models. Moreover, we will show how to create a dataset similar to the one given in the competition page to train and/or validate your models at will.\n\nThe starting kit\nYou can download the starting kit in the competition webpage > Participate > Files.\nstarting_kit\n│\n└─── track_1 (videos)\n|   │\n|   └─── exp_Y\n│        │\n│        └─── videos_fov_X.tiff (video for each fov)\n│        │\n│        └─── ensemble_labels.txt (information at the ensemble level for the whole experiment)\n│        │\n│        └─── traj_labs_fov_X.txt (information for each trajectory for the whole experiment)\n│        │\n│        └─── ens_labs_fov_X.txt (information at the ensemble level for each FOV)\n│        │\n│        └─── vip_idx_fov_X.txt (index of VIP particles)\n│ \n│\n└─── track_2 (trajectories)\n    │\n    └─── exp_Y\n        │\n        └─── traj_fov_X.txt (trajectories for each FOV) \n        │\n        └─── ensemble_labels.txt (information at the ensemble level for the whole experiment)\n        │\n        └─── traj_labs_fov_X.txt (information for each trajectory for the whole experiment)\n        │\n        └─── ens_labs_fov_X.txt (information at the ensemble level for each FOV)\nRather than working with the Startking Kit, we will instead show how to generate a similar and customizable dataset with the andi_datasets library.\n\n\nGenerating the ANDI 2 challenge dataset\nHere we showcase how to generate the exact same dataset used as training / public dataset in the Codalab competition.\n\n# Functions needed from andi_datasets\nfrom andi_datasets.datasets_challenge import challenge_phenom_dataset, _get_dic_andi2, _defaults_andi2\n\n\nDefining the dataset properties\nSetting a single FOV per experiment: In order to have the most heterogeneous dataset, we will avoid overlapping FOVs by generating various experiments for each condition and then a single FOV for each of this. Then, the generator function will take of reorganizing the dataset in the proper challenge structure. This makes the simulations much more efficient, since we need to consider environments with less particles to generate every fov.\nOur goal is to have 10 experiments (2 per phenomenological model) with 30 FOVs each. This means that we will generate 300 independent experiments, but considering that batches of 30 have the exact same properties.\nThe first experiment for each model will be sampled using the default parameters (given by _get_dic_andi2 and defined in _defaults_andi2). Then, we will show how to customize the parameters for the second experiment of each model. These second ones will be “easy” with clearly differentiated diffusie states. We will also do some tweaks to make them interesting!\nWe can take a look at _defaults_andi2 to get the defaults values:\n\n_defaults_andi2??\n\nInit signature: _defaults_andi2()\nSource:        \nclass _defaults_andi2: \n    '''\n    This class defines the default values set for the ANDI 2 challenge.\n    '''\n    def __init__(self):        \n        # General parameters\n\n        self.T = 500                   # Length of simulated trajectories\n        self._min_T = 20               # Minimal length of output trajectories\n        self.FOV_L = 128               # Length side of the FOV (px)\n        self.L = 1.8*self.FOV_L          # Length of the simulated environment\n        self.D = 1                     # Baseline diffusion coefficient (px^2/frame)\n        self.density = 2               # Particle density   \n        self.N = 50                    # Number of particle in the whole experiment\n        self.sigma_noise = 0.12        # Variance of the localization noise\n\n        self.label_filter = lambda x: label_filter(x, window_size = 5, min_seg = 3)\nFile:           ~/GitHub/ANDI_datasets/andi_datasets/datasets_challenge.py\nType:           type\nSubclasses:     \n\n\nAs we saw in this tutorial, the details for every experiment need to be provided in a list of dictionaries. Let’s start with the first experiment for each diffusion model using the default parameters.\n\nMODELS = np.arange(5)\nNUM_FOVS = 30\nPATH = 'andi2_dataset/' # Chose your path!\n\ndics = []\n\nfor m in MODELS:   \n    dic = _get_dic_andi2(m+1)\n\n    # Fix length and number of trajectories \n    dic['T'] = 200 \n    dic['N'] = 100\n\n    # Add repeated fovs for the experiment\n    for _ in range(NUM_FOVS):\n        dics.append(dic)\n\nNow let’s add the second experiment for each model with their customized settings.\n\nfor m in MODELS:   \n    dic = _get_dic_andi2(m+1)\n\n    # Fix length and number of trajectories \n    dic['T'] = 200 \n    dic['N'] = 100\n\n    #### SINGLE STATE ####\n    if m == 0:  \n        dic['alphas'] = np.array([1.5, 0.01])\n        dic['Ds'] = np.array([0.01, 0.01])\n            \n            \n    #### MULTI STATE ####\n    if m == 1:\n        # 3-state model with different alphas            \n        dic['Ds'] = np.array([[0.99818417, 0.01],\n                              [0.08012007, 0.01],\n                              [1.00012007, 0.01]])\n\n        dic['alphas'] = np.array([[0.84730977, 0.01],\n                                  [0.39134136, 0.01],\n                                  [1.51354654, 0.01]])\n\n        dic['M'] = np.array([[0.98, 0.01, 0.01],\n                             [0.01, 0.98, 0.01],\n                             [0.01, 0.01, 0.98]]) \n        \n    #### IMMOBILE TRAPS ####\n    if m == 2:\n        dic['alphas'] = np.array([1.9, 0.01])\n            \n    \n    #### DIMERIZATION ####\n    if m == 3:\n        dic['Ds'] = np.array([[1.2, 0.01],\n                              [0.02, 0.01]])\n\n        dic['alphas'] = np.array([[1.5, 0.01],\n                                  [0.5, 0.01]])\n        dic['Pu'] = 0.02\n        \n    #### CONFINEMENT ####\n    if m == 4:\n        dic['Ds'] = np.array([[1.02, 0.01],\n                              [0.01, 0.01]])\n\n        dic['alphas'] = np.array([[1.8, 0.01],\n                                  [0.9, 0.01]])\n        \n        dic['trans'] = 0.2\n\n    # Add repeated fovs for the experiment\n    for _ in range(NUM_FOVS):\n        dics.append(dic)\n\nWe can now use the function datasets_challenge.challenge_phenom_dataset to generate our dataset. If not set, this functions saves files in PATH (see the documentation of the function to know more). For training your methods, this may be appropriate. However, in some cases you may want to keep the same structure as shown in the Starting Kit. To do so, we use the variables files_reorg = True and subsequent:\n\ndfs_traj, videos, labs_traj, labs_ens = challenge_phenom_dataset(save_data = True, # If to save the files\n                                                                dics = dics, # Dictionaries with the info of each experiment (and FOV in this case)\n                                                                path = PATH, # Parent folder where to save all data\n                                                                return_timestep_labs = True, get_video = True, \n                                                                num_fovs = 1, # Number of FOVs                                                                \n                                                                num_vip=10, # Number of VIP particles\n                                                                files_reorg = True, # We reorganize the folders for challenge structure\n                                                                path_reorg = 'ref/', # Folder inside PATH where to reorganized\n                                                                save_labels_reorg = True, # The labels for the two tasks will also be saved in the reorganization                                                                 \n                                                                delete_raw = True # If deleting the original raw dataset\n                                                                )\n\n\n\n\nCreating dataset for Exp_0 (single_state).\nGenerating video for EXP 0 FOV 0\nCreating dataset for Exp_1 (multi_state).\nGenerating video for EXP 1 FOV 0\n\n\nAside of the generated data, we also get some outputs from this function:\n\ndfs_traj: a list of dataframes, one for each experiment / fov, which contains the info that was saved in the .csv. If return_timestep_labs = True, then the dataframe also contains the diffusive parameters of each frame.\n\n\ndfs_traj[0]\n\n\n\n\n\n  \n    \n      \n      traj_idx\n      frame\n      x\n      y\n      alpha\n      D\n      state\n    \n  \n  \n    \n      0\n      0.0\n      18.0\n      124.892324\n      19.204098\n      0.592591\n      0.885770\n      2.0\n    \n    \n      1\n      0.0\n      19.0\n      126.511436\n      19.305087\n      0.592591\n      0.885770\n      2.0\n    \n    \n      2\n      0.0\n      20.0\n      126.197563\n      21.030422\n      0.592591\n      0.885770\n      2.0\n    \n    \n      3\n      0.0\n      21.0\n      129.700339\n      20.707369\n      0.592591\n      0.885770\n      2.0\n    \n    \n      4\n      0.0\n      22.0\n      129.239501\n      21.795457\n      0.592591\n      0.885770\n      2.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6221\n      38.0\n      195.0\n      66.258704\n      20.676069\n      0.553593\n      0.895989\n      2.0\n    \n    \n      6222\n      38.0\n      196.0\n      67.272088\n      20.338184\n      0.553593\n      0.895989\n      2.0\n    \n    \n      6223\n      38.0\n      197.0\n      66.160404\n      21.348238\n      0.553593\n      0.895989\n      2.0\n    \n    \n      6224\n      38.0\n      198.0\n      65.895880\n      19.667382\n      0.553593\n      0.895989\n      2.0\n    \n    \n      6225\n      38.0\n      199.0\n      66.039573\n      21.945488\n      0.553593\n      0.895989\n      2.0\n    \n  \n\n6226 rows × 7 columns\n\n\n\n\nvideos: contains the arrays from where the videos will be generated. They have similar shape as the ones we loaded at the beginning of this notebook:\n\n\nplay_video(videos[0][1:])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nlabs_traj a list of lists containing the labels for each trajectory in similar form as the ones you are asked to submit as fov.txt:\n\n\nlabs_traj[0][:10]\n\n[[0, 1.1236012281768861, 1.0331045678718274, 2.0, 46],\n [1, 1.1595223945376247, 0.9422760723440122, 2.0, 200],\n [2, 1.0752862372656447, 0.9858771716836071, 2.0, 174],\n [3, 1.0752862372656447, 0.9858771716836071, 2.0, 22],\n [4, 1.029927660196135, 0.8408724154169402, 2.0, 200],\n [5, 0.8447116390690012, 1.1005734714909277, 2.0, 27],\n [6, 0.8447116390690012, 1.1005734714909277, 2.0, 24],\n [7, 0.8447116390690012, 1.1005734714909277, 2.0, 38],\n [8, 0.8447116390690012, 1.1005734714909277, 2.0, 26],\n [9, 0.8447116390690012, 1.1005734714909277, 2.0, 34]]\n\n\n\nlabs_ens a list of numpy arrays which contain the ensemble labels for each fov generated:\n\n\nlabs_ens\n\n[array([[1.0488135e+00],\n        [1.0000000e-02],\n        [1.0000000e+00],\n        [1.0000000e-02],\n        [6.0540000e+03]]),\n array([[8.47309770e-01, 3.91341360e-01, 1.51354654e+00],\n        [1.00000000e-02, 1.00000000e-02, 1.00000000e-02],\n        [9.98184170e-01, 8.01200700e-02, 1.00012007e+00],\n        [1.00000000e-02, 1.00000000e-02, 1.00000000e-02],\n        [1.67800000e+03, 1.53800000e+03, 1.94000000e+03]])]\n\n\n\n\n\nScoring your predictions\nAll the scoring programs used in codalab are also available in the library. Here is an example with the dataset we just generated.\nTo simplify, we will use the same dataset we just generated as submission. We will consider here only Track 2. As groundtruth and predictions are the same, the metrics will always be perfect!\nThe structure for a prediction is the same as the one presented in Section Creating and submitting predictions. For the program to work, the groundtruth /reference must be in a folder called ref and the predictions / results in a folder called res, both placed in the PATH. The whole structure should be as following:\nPATH\n│\n└─── ref (the one you created above. Should contain labels, i.e. save_labels = True)\n|\n└─── res (the one containing your predictions, same structure as for a submission)\n    │\n    └─── track_1 \n    |   │\n    |   └─── exp_Y\n    │        │\n    │        └─── fov_X.txt (predictions for VIP particles for each FOV)\n    │        │\n    │        └─── ensemble_labels.txt (predictions at the ensemble level for the whole experiment)\n    │ \n    │\n    └─── track_2 \n        │\n        └─── exp_Y\n            │\n            └─── fov_X.txt (predictions for single trajectories for each FOV)\n            │\n            └─── ensemble_labels.txt (predictions at the ensemble level for the whole experiment)\nAs we commented above, you can choose to participate just in one of the tracks (video or trajectories) or tasks (ensemble or single trajectory). This will not rise an error in the scoring program, just None predictions to the rest. As we are here participating just in Track 2, all scores for Track 1 will be set to their maximum possible values (these can be found in the challenge description or in utils_challenge._get_error_bounds.\n\nNote: You can skip the following if you have created your own predictions.\n\nTo proceed, make a duplicate of the ref folder, name the new copy res, and delete res/track_1 folder. Then, we can use the following function to transform the reference dataset into a submission dataset (mostly changes the names of the files traj_labs_fov_X.txt into fov_X.txt.\n\nfrom andi_datasets.utils_challenge import transform_ref_to_res\n\ntransform_ref_to_res(base_path = PATH + 'res/',\n                     track = 'track_2',\n                     num_fovs = NUM_FOVS)\n\nTo score your predictions, you just need to run:\n\nfrom andi_datasets.utils_challenge import codalab_scoring\n\nPATH = 'andi2_dataset/'\n\ncodalab_scoring(INPUT_DIR = PATH,\n                OUTPUT_DIR = '.')\n\nThe warnings give you important information about missing tracks, tasks and others. The results are then give in two formats:\n\nscores.txt: this file contains the average values over all experiments for the different Tracks and Tasks. If you followed this example and didn’t include you own predictions, it should look like this:\n\ntr1.ta1.alpha: 2\ntr1.ta1.D: 100000.0\ntr1.ta1.state: 0\ntr1.ta1.cp: 10\ntr1.ta1.JI: 0\ntr1.ta2.alpha: 1.999\ntr1.ta2.D: 1000000.0\ntr2.ta1.cp: 0.0\ntr2.ta1.JI: 1.0\ntr2.ta1.alpha: 0.0\ntr2.ta1.D: 0.0\ntr2.ta1.state: 1.0\ntr2.ta2.alpha: 0.0\ntr2.ta2.D: 0.0\ntrX.taY.ZZ refer to the track X, task Y and property ZZ. As you can see, because in our example we only considered Track 2, all results for Track 1 are set the their maximum possible values. The rest are the best score that can be achieved in each task (as reference = submissions).\n\nhtml/scores.html: contains a summary of the results for each track / task. It contains the following tables, with numbers similar to:\n\n\n\nTask 1: single\n\n\n  <th>num_trajs</th>      <th>RMSE CP</th>      <th>JSC CP</th>      <th>alpha</th>      <th>D</th>      <th>state</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>129</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>    </tr>    <tr>      <td>1</td>      <td>95</td>      <td>0.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>1.0</td>    </tr>  </tbody></table></center>\n\nTask 2: ensemble\n\n\n\n\n\n\nExperiment\n\n\n  <th>alpha</th>      <th>D</th>    </tr>  </thead>  <tbody>    <tr>      <td>0.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>    <tr>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>  </tbody></table> </center>Now you can adapt this code to locally test your submissions. We also encourage you to check the documentation of utils_challenge.run_single_task and utils_challenge.run_ensemble_task to know more of what is going on in the scoring program!\n\n\n\nExperiment"
  },
  {
    "objectID": "tutorials/andi_and_ergodicity.html",
    "href": "tutorials/andi_and_ergodicity.html",
    "title": "Firsts steps in anomalous diffusion",
    "section": "",
    "text": "In this notebook, you will find the basic tools to learn how analyze diffusion from trajectories. First, few simple comments about the andi_datasets package. We will use this python package to create trajectories of different anomalous diffusion models. You can find all details in the documentation webpage.\nLet’s start by importing the models_theory module and checking the available anomalous diffusion models.\n\nfrom andi_datasets.models_theory import models_theory\n\n\ndimensions = 1\nmodels = models_theory()._oneD()\n\navailable_models = inspect.getmembers(models, inspect.ismethod)\n\nprint(\n    \"The availailabe models for generating trajectories in \"\n    + str(dimensions)\n    + \"D  are:\\n\"\n)\nfor x in available_models:\n    print(\"- \" + x[0] + \"\\n\")\n\nThe availailabe models for generating trajectories in 1D  are:\n\n- attm\n\n- ctrw\n\n- fbm\n\n- lw\n\n- sbm\n\n\n\nThe andi-datasets package allows us to generate trajectories according to these five theoretical models. We will learn more about them along the notebook. To generate a single trajectory of, for instance, Fractional Brownian motion (fbm) with anomalous diffusion exponent \\(\\alpha=0.5\\) we just need to do:\n\nT = 200  # Length of the trajectories\nalpha = 0.5  # Anomalous diffusion exponent\nmodel = 2  # This corresponds to the index of fbm in the previous list\n\ntraj = models.fbm(T=T, alpha=alpha)\nplt.plot(traj)\n\n\n\n\nMoreover, the andi_datasets library allows us to create bigger datasets of trajectories with the datasets_theory module. Below, we show how to do this. The object dataset is a numpy matrix whose first two columns indicate the exponent and model of each trajectory (we name them labels) and the rest of the matrix is the trajectory itself (we call it trajs). Let’s check how the first 5 trajectories look like:\n\nfrom andi_datasets.datasets_theory import datasets_theory\n\nDT = datasets_theory()\n\n\nN = 200  # Number of trajectories\n\ndataset = DT.create_dataset(T=T, N_models=N, exponents=alpha, models=[2])\n\nlabels = dataset[:, :2]\ntrajs = dataset[:, 2:]\n\n# dispay a few\nfor traj in trajs[:5]:\n    plt.plot(traj)\nplt.xlabel(\"time\")\nplt.ylabel(\"position\")\n\nText(0, 0.5, 'position')"
  },
  {
    "objectID": "tutorials/andi_and_ergodicity.html#anomalous-diffusion-exponent",
    "href": "tutorials/andi_and_ergodicity.html#anomalous-diffusion-exponent",
    "title": "Firsts steps in anomalous diffusion",
    "section": "Anomalous diffusion exponent",
    "text": "Anomalous diffusion exponent\nIn this section, we will show the basic tools to estimate the anomalous diffusion exponent of a set of trajectories.\n\nTime-averaged mean squared displacement (TA-MSD)\nWe will now create a function that calculates the TA-MSD for a given trajectory:\n(Note that this function calculates the TA-MSD for a single trajectory. Using the theoretical definition and the function below, can you rewrite the function such that it calculates the TA-MSD for a set of trajectories in an efficient way?)\n\ndef TAMSD(traj, t_lags):\n    \"\"\"\n    Calculates the time average mean squared displacement of a set of trajectories\n    Inputs: - traj: trajectory to calculate TA-MSD\n            - t_lags: time lags used for the TA-MSD\n    Outputs: - TA-MSD\n    \"\"\"\n    tamsd = np.zeros_like(t_lags, dtype=float)\n    for idx, t in enumerate(t_lags):\n        for p in range(len(traj) - t):\n            tamsd[idx] += (traj[p] - traj[p + t]) ** 2\n        tamsd[idx] /= len(traj) - t\n    return tamsd\n\nLet’s use the dataset we created above and calculate the TAMSD of each trajectory and plot it in log-scale. As the trajectories have 200 points, let’s choose the time lages \\(\\Delta \\in [1,20]\\). In addition, we will also plot two lines: one linearly proportional to the \\(\\Delta\\) and one proportional to \\(\\Delta^\\alpha\\). This will give you information about the ergodicity! Can you tell from here if fbm is ergodic or not?\n\nt_lags = np.arange(1, 101)\n\nfor traj in trajs[:5]:\n    tamsd = TAMSD(traj, t_lags)\n    plt.loglog(t_lags, tamsd)\n\nplt.loglog(t_lags, 0.1 * t_lags, c=\"k\", label=r\"$\\sim \\Delta$\")\nplt.loglog(t_lags, 0.1 * t_lags**alpha, c=\"k\", ls=\"--\", label=r\"$\\sim \\Delta^\\alpha$\")\nplt.xlabel(r\"Time lag ($\\Delta$)\")\nplt.ylabel(\"TA-MSD\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nWe can also calculate the ensemble averaged (EA-MSD) and time-ensemble averaged mean squared displacement (TEA-MSD):\n\n## EA-MSD ##\neamsd = np.mean(trajs**2, axis=0)\n\nplt.loglog(eamsd)\nplt.loglog(np.arange(1, T), 0.1 * np.arange(1, T), c=\"k\", label=r\"$\\sim$ time\")\nplt.loglog(\n    np.arange(1, T),\n    0.1 * np.arange(1, T) ** alpha,\n    c=\"k\",\n    ls=\"--\",\n    label=r\"$\\sim$ time$^\\alpha$\",\n)\nplt.xlabel(r\"time\")\nplt.ylabel(\"EA-MSD\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\n## TEA-MSD ##\nt_lags = np.arange(1, 21)\nteamsd = np.zeros_like(t_lags, dtype=\"float64\")\nfor traj in trajs:\n    teamsd += TAMSD(traj, t_lags)\nteamsd /= N\n\n\nplt.loglog(t_lags, teamsd, lw=3)\nplt.loglog(t_lags, 0.07 * t_lags, c=\"k\", label=r\"$\\sim \\Delta$\")\nplt.loglog(\n    t_lags, 0.07 * t_lags**alpha, c=\"k\", ls=\"--\", label=r\"$\\sim \\Delta^\\alpha$\"\n)\nplt.xlabel(r\"time lag ($\\Delta$)\")\nplt.ylabel(\"TEA-MSD\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nA visual inspection seems to tell us that FBM is ergodic! To confirm it, let’s fit the TA-MSD and check if we recover the anomalous diffusion exponent we set above. For that, we will use a fitting function from numpy, performing a linear fit in log-log space:\n\nfitted_alpha = []\nfor traj in trajs:\n    tamsd = TAMSD(traj, t_lags)\n    fitted_alpha.append(np.polyfit(np.log(t_lags), np.log(tamsd), 1)[0])\n\nplt.scatter(range(N), fitted_alpha, label=r\"Fitted $\\alpha$\")\nplt.axhline(alpha, label=r\"True $\\alpha$\")\nplt.legend()\nplt.xlabel(\"Trajectory number\")\nplt.ylabel(r\"$\\alpha$\")\nplt.ylim(0, 1)\n\n(0.0, 1.0)\n\n\n\n\n\nThe TA-MSD fit seems to work…\n\n\nTrajectories with localization error\nThe trajectories we used aboved were examples from a purely theoretical fbm. However, in real life, we have the appearance of different sources of noise, for example the localization noise. A simple way of simulating it, consists of adding Gaussian noise to each of the positions of a trajectory. Below, we provide an example with noise with standard deviation $ _{} = 0.5$:\n\nsigma_n = 0.5\nnoisy_trajs = trajs + np.random.randn(N, T) * sigma_n\n\n\nplt.plot(trajs[0, :], label=\"Original trajectory\")\nplt.plot(noisy_trajs[0, :], label=\"Noisy trajectory\", alpha=0.8)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Position\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nLet’s see what happens when we try to predict the anomalous diffusion exponent for this noisy trajectories:\n\nfitted_alpha_noisy = []\nfor traj in noisy_trajs:\n    tamsd = TAMSD(traj, t_lags)\n    fitted_alpha_noisy.append(np.polyfit(np.log(t_lags), np.log(tamsd), 1)[0])\n\nplt.scatter(range(N), fitted_alpha, label=r\"Fitted $\\alpha$ (normal trajs.)\")\nplt.scatter(range(N), fitted_alpha_noisy, label=r\"Fitted $\\alpha$ (noisy trajs.)\")\n\nplt.axhline(alpha, label=r\"True $\\alpha$\")\nplt.legend()\nplt.xlabel(\"Trajectory number\")\nplt.ylabel(r\"$\\alpha$\")\nplt.ylim(0, 1)\n\n(0.0, 1.0)\n\n\n\n\n\nYou can see that presence of noise largely affects the calculation of the anomalous diffusion exponent. This shows the need for better techniques to correctly characterize anomalous diffusion. In the last part of this notebook, we will show a novel approach based on ML, with much better results! Having an independent estimation of the noise allows for a correction and provides a better quantification of \\(\\alpha\\):\n\nfitted_alpha_noisy_corr = []\nfor traj in noisy_trajs:\n    tamsd = TAMSD(traj, t_lags) - 2 * (sigma_n**2)\n    i_ok = np.where(tamsd > 0)\n    fitted_alpha_noisy_corr.append(\n        np.polyfit(np.log(t_lags[i_ok]), np.log(tamsd[i_ok]), 1)[0]\n    )\n\nplt.scatter(range(N), fitted_alpha, label=r\"Fitted $\\alpha$ (normal trajs.)\")\nplt.scatter(range(N), fitted_alpha_noisy, label=r\"Fitted $\\alpha$ (noisy trajs.)\")\nplt.scatter(\n    range(N),\n    fitted_alpha_noisy_corr,\n    label=r\"Fitted $\\alpha$ (noisy trajs. with corr.)\",\n)\n\nplt.axhline(alpha, label=r\"True $\\alpha$\")\nplt.legend()\nplt.xlabel(\"Trajectory number\")\nplt.ylabel(r\"$\\alpha$\")\nplt.ylim(0, 1)\n\n(0.0, 1.0)"
  },
  {
    "objectID": "tutorials/andi_and_ergodicity.html#ergodicity-breaking",
    "href": "tutorials/andi_and_ergodicity.html#ergodicity-breaking",
    "title": "Firsts steps in anomalous diffusion",
    "section": "Ergodicity breaking",
    "text": "Ergodicity breaking\nSo far, we have focused on an ergodic model, FBM. However, there exist diffusion models that break ergodicity. This means that the time-averaged mean squared displacement (TA-MSD) and the ensemble-average mean squared displacement (EA-MSD) are not equivalent. Let’s explore this intriguing feature with a very well known non-ergodic model: the continuous-time random walk (CTRW). We will start by creating a dataset of CTRW trajectories.\n\nT = 200  # Length of the trajectories\nN = 1000  # Number of trajectories\nalpha = 0.5  # Anomalous diffusion exponent\n\n# Now we want CTRW trajectories (see list of available models above):\nmodel = 1\n\ndataset = DT.create_dataset(T=T, N_models=N, exponents=alpha, models=model)\nlabels = dataset[:, :2]\ntrajs = dataset[:, 2:]\n\nThis is how CTRW trajectories look like. You will recognize the characteristic waiting times.\n\nfor traj in trajs[:5]:\n    plt.plot(traj)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Position\")\n\nText(0, 0.5, 'Position')\n\n\n\n\n\n\nEnsemble-averaged mean squared displacement (EA-MSD)\nTo showcase the appearance of ergodicity breaking, let’s calculate the EA-MSD and the time-ensemble-averaged mean squared displacement (TEA-MSD). For the former, if all trajectories start at zero, it is as easy as to do the mean of the squared positions. For the latter, we will use the function we defined previously. We will use the same time lags we used for the FBM case.\n\n## EA-MSD ##\neamsd = np.mean(trajs**2, axis=0)\n\n## TEA-MSD ##\nt_lags = np.arange(1, 21)\nteamsd = np.zeros_like(t_lags, dtype=\"float64\")\nfor traj in trajs:\n    teamsd += TAMSD(traj, t_lags)\nteamsd /= N\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5), constrained_layout=True)\n\nax[0].loglog(t_lags, teamsd)\nax[0].loglog(t_lags, 1.1 * teamsd[0] * t_lags, c=\"k\", label=r\"$\\sim \\Delta$\")\nax[0].loglog(\n    t_lags,\n    1.1 * teamsd[0] * t_lags**alpha,\n    c=\"k\",\n    ls=\"--\",\n    label=r\"$\\sim \\Delta^\\alpha$\",\n)\nplt.setp(ax[0], xlabel=r\"Time lag ($\\Delta$)\", ylabel=\"TEA-MSD\")\nax[0].legend()\n\n\nax[1].loglog(eamsd)\nax[1].loglog(np.arange(1, T), eamsd[1] * np.arange(1, T), c=\"k\", label=r\"$\\sim t$\")\nax[1].loglog(\n    np.arange(1, T),\n    eamsd[1] * np.arange(1, T) ** alpha,\n    c=\"k\",\n    ls=\"--\",\n    label=r\"$\\sim t^\\alpha$\",\n)\nplt.setp(ax[1], xlabel=r\"Time\", ylabel=\"EA-MSD\")\nax[1].legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nAs we can clearly see, the TEA-MSD scales linearly with the time lags \\(\\Delta\\), while the EA-MSD scales as \\(t^\\alpha\\), thus showing the ergodicity breaking.\n\n\nErgodicity breaking parameter\nA quantitative approach to ergodicity breaking is the calculation of the ergodicity breaking parameter (EB). Here is how to calculate it:\n\ndef EB(trajs, t_lag):\n    eb = []\n    # We loop over T, which is the point at which we cut the trajectory\n    for T in range(t_lag + 1, trajs.shape[1]):\n        cut_trajs = trajs[:, :T]\n        # Now we calculate the TAMSD for every trajectory at the\n        # given t_lag\n        tamsd = np.zeros(trajs.shape[0])\n        for idx, traj in enumerate(cut_trajs):\n            for p in range(len(traj) - t_lag):\n                tamsd[idx] += (traj[p] - traj[p + t_lag]) ** 2\n        tamsd /= len(traj) - t_lag\n        # Compute EB and append\n        eb.append(np.mean(tamsd**2) / np.mean(tamsd) ** 2 - 1)\n    return eb\n\nLet’s compute the EB for trajectories similar to the ones we used in the previous section. The expected value at \\(t\\rightarrow\\infty\\) is given by Eq.(8) of this paper and for \\(\\alpha = 0.5\\) we should have EB \\(\\approx 0.570\\). Let’s also simulate some FBM trajectories to check that for ergodic processes, the EB goes to zero even when the anomalous diffusion exponent is different than one. We will use as time lag \\(\\Delta = 2\\). You can check the effect of varying \\(\\Delta\\) on the EB in the previous paper (see e.g. Fig. 2(b)).\nNote: the EB is rather slowe to calculate because it requires a few loops. If you managed to create the efficient function for the TA-MSD, now is the time to use it!\n\nT = 200  # Length of the trajectories\nN = 500  # Number of trajectories\nalpha = 0.5  # Anomalous diffusion exponent\n\nfbm = DT.create_dataset(T=T, N_models=N, exponents=alpha, models=2)[:, 2:]\nctrw = DT.create_dataset(T=T, N_models=N, exponents=alpha, models=1)[:, 2:]\n\n\nEB_ctrw = EB(ctrw, t_lag=2)\nEB_fbm = EB(fbm, t_lag=2)\n\n\nplt.plot(EB_ctrw, label=\"CTRW\")\nplt.plot(EB_fbm, label=\"FBM\")\nplt.axhline(0.570, c=\"k\", alpha=0.4, label=r\"EB($t\\rightarrow\\infty$) CTRW\")\nplt.ylabel(\"EB(t)\")\nplt.xlabel(\"Time\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nAs you can see, the EB for the FBM trajectories quickly goes to 0. However, for the CTRW it displays a value different than zero that will converge to the theoretical value if we consider much longer trajectories. Feel free to explore how long you have to wait until reaching such value!"
  },
  {
    "objectID": "tutorials/andi_and_ergodicity.html#machine-learning-approach-to-single-trajectory-characterization",
    "href": "tutorials/andi_and_ergodicity.html#machine-learning-approach-to-single-trajectory-characterization",
    "title": "Firsts steps in anomalous diffusion",
    "section": "Machine learning approach to single trajectory characterization",
    "text": "Machine learning approach to single trajectory characterization\nAs we just saw, analyzing diffusion processes at the single trajectory level has many problems related to, e.g. noise, trajectory length, ergodicity breaking,… In the last years, we have seen how machine learning can help us solve many of this. Here, we will show an example on how to use a convolutional neural network to estimate the anomalous diffusion exponent of a dataset of trajectories.\n\nTraining and test dataset\nLet’s start by creating the datasets that we will use for training and evaluating the trained machine. We will consider for this example a dataset of FBM trajectories, with exponents \\(\\in [0.2,1.4]\\) and length of 200 steps. For the training, we need a big dataset so we will generate 10000 trajectories:\n\nnum_alphas = 100\nexponents = np.linspace(0.2, 1.81, num_alphas)\nN_models = (\n    100  # this is the number of trajectories per exponent, which makes a total of 10000\n)\nT = 200  # Length of the trajectories\n\ndataset = DT.create_dataset(T=T, N_models=N_models, exponents=exponents, models=2)\n\nWe will now transform the previous dataset to a machine learning dataset which we can then input to the neural network. The shape constraints are set by the machines that we will define later. Moreover, we will split the trajectories into a training and validation datasets with a 80%/20% ratio. We will use the function normalize from the andi-datasets package to ensure that all the trajectories are on the same scale. This function divides the displacements of a trajectory by their standard deviation. While this operation changes the diffusion coefficient of any trajectory to \\(D=1\\), the anomalous diffusion exponent remains the same. Furthermore, it is also important to randomly shuffle our dataset.\n\nfrom andi_datasets.utils_trajectories import normalize\n\nratio = int(0.8 * dataset.shape[0])\nnp.random.shuffle(dataset)\n# NN inputs: trajectories\nX_train = normalize(dataset[:ratio, 2:]).reshape(ratio, T, 1)\nX_valid = normalize(dataset[ratio:, 2:]).reshape(N - ratio, T, 1)\n\n# NN outputs: their anomalous exponent\nY_train = dataset[:ratio, 1]\nY_valid = dataset[ratio:, 1]\n\n\n\nConvolutional neural network architecture\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Conv1D, Flatten, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2 as regularizer_l2\n\n\nmodel = Sequential()\n\n\nmodel.add(\n    Conv1D(\n        filters=3,\n        kernel_size=3,\n        strides=1,\n        input_shape=(T, 1),\n        kernel_initializer=\"uniform\",\n        activation=\"relu\",\n        kernel_regularizer=regularizer_l2(l=0.001),\n    )\n)\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\n\nmodel.add(\n    Conv1D(\n        filters=8,\n        kernel_size=5,\n        strides=1,\n        kernel_initializer=\"uniform\",\n        activation=\"relu\",\n        kernel_regularizer=regularizer_l2(l=0.001),\n    )\n)\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\n\nmodel.add(\n    Conv1D(\n        filters=3,\n        kernel_size=2,\n        strides=1,\n        kernel_initializer=\"uniform\",\n        activation=\"relu\",\n        kernel_regularizer=regularizer_l2(l=0.001),\n    )\n)\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(\n    Dense(64 * 2, activation=\"sigmoid\", kernel_regularizer=regularizer_l2(l=0.001))\n)\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\n\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation=\"sigmoid\"))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\n\n2023-11-14 19:09:03.088654: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nWe want to predict the anomalous diffusion exponent, therefore the output layer needs to have size one. Since we are performing a regression, it is also important that its activation function is linear.\n\nmodel.add(Dense(1, activation=\"linear\"))\n\nNow that we have all the layers, we can take a look at a visual summary of the CNN. The plot below also gives us the input/output shape for every layer.\n\nfrom keras.utils import plot_model\n\nplot_model(model, show_shapes=True, show_layer_names=False)\n\n\n\n\nLet’s compile the network. For the loss function, we will use the mean absolute error (MAE). Another possible choice is the mean squared error (MSE). We will keep track of it as metric. We will use Adam as optimizer.\n\nmodel.compile(loss=\"mean_absolute_error\", optimizer=Adam())\n\nWe are ready to start the training!\n\nbatch_size = 200\nepochs = 150\n\ntraining = model.fit(\n    X_train,\n    Y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=2,\n    validation_data=(X_valid, Y_valid),\n)\n\nEpoch 1/150\n40/40 - 4s - loss: 1.0776 - val_loss: 1.0489 - 4s/epoch - 95ms/step\nEpoch 2/150\n40/40 - 1s - loss: 0.6571 - val_loss: 0.6707 - 924ms/epoch - 23ms/step\nEpoch 3/150\n40/40 - 1s - loss: 0.5107 - val_loss: 0.5904 - 930ms/epoch - 23ms/step\nEpoch 4/150\n40/40 - 1s - loss: 0.4412 - val_loss: 0.5660 - 979ms/epoch - 24ms/step\nEpoch 5/150\n40/40 - 1s - loss: 0.4746 - val_loss: 0.5355 - 960ms/epoch - 24ms/step\nEpoch 6/150\n40/40 - 1s - loss: 0.4391 - val_loss: 0.5038 - 1s/epoch - 26ms/step\nEpoch 7/150\n40/40 - 1s - loss: 0.4058 - val_loss: 0.4957 - 1s/epoch - 25ms/step\nEpoch 8/150\n40/40 - 1s - loss: 0.3720 - val_loss: 0.4970 - 978ms/epoch - 24ms/step\nEpoch 9/150\n40/40 - 1s - loss: 0.3379 - val_loss: 0.4810 - 1s/epoch - 28ms/step\nEpoch 10/150\n40/40 - 1s - loss: 0.3009 - val_loss: 0.4905 - 1s/epoch - 25ms/step\nEpoch 11/150\n40/40 - 1s - loss: 0.2737 - val_loss: 0.4872 - 1s/epoch - 27ms/step\nEpoch 12/150\n40/40 - 1s - loss: 0.2662 - val_loss: 0.4298 - 1s/epoch - 30ms/step\nEpoch 13/150\n40/40 - 1s - loss: 0.2577 - val_loss: 0.4815 - 1s/epoch - 30ms/step\nEpoch 14/150\n40/40 - 1s - loss: 0.2298 - val_loss: 0.4412 - 1s/epoch - 28ms/step\nEpoch 15/150\n40/40 - 1s - loss: 0.2446 - val_loss: 0.7912 - 1s/epoch - 29ms/step\nEpoch 16/150\n40/40 - 1s - loss: 0.2358 - val_loss: 0.3735 - 1s/epoch - 31ms/step\nEpoch 17/150\n40/40 - 1s - loss: 0.2182 - val_loss: 0.7752 - 1s/epoch - 33ms/step\nEpoch 18/150\n40/40 - 1s - loss: 0.3127 - val_loss: 0.5923 - 1s/epoch - 33ms/step\nEpoch 19/150\n40/40 - 1s - loss: 0.2710 - val_loss: 0.5222 - 1s/epoch - 31ms/step\nEpoch 20/150\n40/40 - 1s - loss: 0.2604 - val_loss: 0.4660 - 1s/epoch - 34ms/step\nEpoch 21/150\n40/40 - 1s - loss: 0.2501 - val_loss: 0.4412 - 1s/epoch - 33ms/step\nEpoch 22/150\n40/40 - 1s - loss: 0.2353 - val_loss: 0.3730 - 1s/epoch - 33ms/step\nEpoch 23/150\n40/40 - 1s - loss: 0.2254 - val_loss: 0.4952 - 1s/epoch - 32ms/step\nEpoch 24/150\n40/40 - 1s - loss: 0.2157 - val_loss: 0.4668 - 1s/epoch - 34ms/step\nEpoch 25/150\n40/40 - 1s - loss: 0.1893 - val_loss: 0.7237 - 1s/epoch - 35ms/step\nEpoch 26/150\n40/40 - 1s - loss: 0.1861 - val_loss: 0.6026 - 1s/epoch - 36ms/step\nEpoch 27/150\n40/40 - 1s - loss: 0.1741 - val_loss: 0.6197 - 1s/epoch - 37ms/step\nEpoch 28/150\n40/40 - 2s - loss: 0.1644 - val_loss: 0.4755 - 2s/epoch - 40ms/step\nEpoch 29/150\n40/40 - 1s - loss: 0.1596 - val_loss: 0.6264 - 1s/epoch - 34ms/step\nEpoch 30/150\n40/40 - 1s - loss: 0.1537 - val_loss: 0.4262 - 1s/epoch - 37ms/step\nEpoch 31/150\n40/40 - 2s - loss: 0.1474 - val_loss: 0.3201 - 2s/epoch - 41ms/step\nEpoch 32/150\n40/40 - 2s - loss: 0.1631 - val_loss: 0.4230 - 2s/epoch - 42ms/step\nEpoch 33/150\n40/40 - 2s - loss: 0.1718 - val_loss: 0.4186 - 2s/epoch - 45ms/step\nEpoch 34/150\n40/40 - 2s - loss: 0.1615 - val_loss: 0.4465 - 2s/epoch - 44ms/step\nEpoch 35/150\n40/40 - 2s - loss: 0.1510 - val_loss: 0.4831 - 2s/epoch - 43ms/step\nEpoch 36/150\n40/40 - 2s - loss: 0.1481 - val_loss: 0.4437 - 2s/epoch - 44ms/step\nEpoch 37/150\n40/40 - 2s - loss: 0.1422 - val_loss: 0.3656 - 2s/epoch - 44ms/step\nEpoch 38/150\n40/40 - 2s - loss: 0.1359 - val_loss: 0.3648 - 2s/epoch - 42ms/step\nEpoch 39/150\n40/40 - 2s - loss: 0.1335 - val_loss: 0.2728 - 2s/epoch - 44ms/step\nEpoch 40/150\n40/40 - 2s - loss: 0.1266 - val_loss: 0.1640 - 2s/epoch - 45ms/step\nEpoch 41/150\n40/40 - 2s - loss: 0.1275 - val_loss: 0.1925 - 2s/epoch - 44ms/step\nEpoch 42/150\n40/40 - 2s - loss: 0.1220 - val_loss: 0.1425 - 2s/epoch - 42ms/step\nEpoch 43/150\n40/40 - 2s - loss: 0.1220 - val_loss: 0.1806 - 2s/epoch - 44ms/step\nEpoch 44/150\n40/40 - 2s - loss: 0.1166 - val_loss: 0.1944 - 2s/epoch - 45ms/step\nEpoch 45/150\n40/40 - 1s - loss: 0.1182 - val_loss: 0.3756 - 1s/epoch - 34ms/step\nEpoch 46/150\n40/40 - 1s - loss: 0.1166 - val_loss: 0.1599 - 1s/epoch - 35ms/step\nEpoch 47/150\n40/40 - 1s - loss: 0.1119 - val_loss: 0.1603 - 1s/epoch - 35ms/step\nEpoch 48/150\n40/40 - 1s - loss: 0.1176 - val_loss: 0.1493 - 1s/epoch - 37ms/step\nEpoch 49/150\n40/40 - 1s - loss: 0.1217 - val_loss: 0.3627 - 1s/epoch - 32ms/step\nEpoch 50/150\n40/40 - 1s - loss: 0.1188 - val_loss: 0.1309 - 1s/epoch - 30ms/step\nEpoch 51/150\n40/40 - 1s - loss: 0.1176 - val_loss: 0.1973 - 1s/epoch - 36ms/step\nEpoch 52/150\n40/40 - 1s - loss: 0.1205 - val_loss: 0.1243 - 1s/epoch - 35ms/step\nEpoch 53/150\n40/40 - 2s - loss: 0.1118 - val_loss: 0.1669 - 2s/epoch - 41ms/step\nEpoch 54/150\n40/40 - 2s - loss: 0.1157 - val_loss: 0.1736 - 2s/epoch - 39ms/step\nEpoch 55/150\n40/40 - 1s - loss: 0.1120 - val_loss: 0.1066 - 1s/epoch - 37ms/step\nEpoch 56/150\n40/40 - 2s - loss: 0.1099 - val_loss: 0.1359 - 2s/epoch - 39ms/step\nEpoch 57/150\n40/40 - 2s - loss: 0.1063 - val_loss: 0.1376 - 2s/epoch - 41ms/step\nEpoch 58/150\n40/40 - 2s - loss: 0.1122 - val_loss: 0.1726 - 2s/epoch - 40ms/step\nEpoch 59/150\n40/40 - 2s - loss: 0.1074 - val_loss: 0.1334 - 2s/epoch - 38ms/step\nEpoch 60/150\n40/40 - 1s - loss: 0.1116 - val_loss: 0.1161 - 1s/epoch - 35ms/step\nEpoch 61/150\n40/40 - 1s - loss: 0.1095 - val_loss: 0.1588 - 1s/epoch - 35ms/step\nEpoch 62/150\n40/40 - 1s - loss: 0.1080 - val_loss: 0.1213 - 1s/epoch - 35ms/step\nEpoch 63/150\n40/40 - 1s - loss: 0.1053 - val_loss: 0.1313 - 1s/epoch - 34ms/step\nEpoch 64/150\n40/40 - 1s - loss: 0.1064 - val_loss: 0.1111 - 1s/epoch - 34ms/step\nEpoch 65/150\n40/40 - 2s - loss: 0.1043 - val_loss: 0.1004 - 2s/epoch - 38ms/step\nEpoch 66/150\n40/40 - 1s - loss: 0.1023 - val_loss: 0.0952 - 1s/epoch - 37ms/step\nEpoch 67/150\n40/40 - 2s - loss: 0.1076 - val_loss: 0.1418 - 2s/epoch - 39ms/step\nEpoch 68/150\n40/40 - 2s - loss: 0.1083 - val_loss: 0.1502 - 2s/epoch - 42ms/step\nEpoch 69/150\n40/40 - 2s - loss: 0.1068 - val_loss: 0.1323 - 2s/epoch - 40ms/step\nEpoch 70/150\n40/40 - 1s - loss: 0.1067 - val_loss: 0.2158 - 1s/epoch - 34ms/step\nEpoch 71/150\n40/40 - 2s - loss: 0.1054 - val_loss: 0.0978 - 2s/epoch - 42ms/step\nEpoch 72/150\n40/40 - 2s - loss: 0.1031 - val_loss: 0.1014 - 2s/epoch - 44ms/step\nEpoch 73/150\n40/40 - 2s - loss: 0.1034 - val_loss: 0.1709 - 2s/epoch - 45ms/step\nEpoch 74/150\n40/40 - 2s - loss: 0.1033 - val_loss: 0.1046 - 2s/epoch - 44ms/step\nEpoch 75/150\n40/40 - 2s - loss: 0.1032 - val_loss: 0.1073 - 2s/epoch - 43ms/step\nEpoch 76/150\n40/40 - 2s - loss: 0.1031 - val_loss: 0.1120 - 2s/epoch - 43ms/step\nEpoch 77/150\n40/40 - 2s - loss: 0.1036 - val_loss: 0.1003 - 2s/epoch - 45ms/step\nEpoch 78/150\n40/40 - 2s - loss: 0.1069 - val_loss: 0.1085 - 2s/epoch - 46ms/step\nEpoch 79/150\n40/40 - 2s - loss: 0.1032 - val_loss: 0.1251 - 2s/epoch - 48ms/step\nEpoch 80/150\n40/40 - 2s - loss: 0.1031 - val_loss: 0.1122 - 2s/epoch - 47ms/step\nEpoch 81/150\n40/40 - 2s - loss: 0.0997 - val_loss: 0.0950 - 2s/epoch - 47ms/step\nEpoch 82/150\n40/40 - 2s - loss: 0.1032 - val_loss: 0.1099 - 2s/epoch - 49ms/step\nEpoch 83/150\n40/40 - 2s - loss: 0.1058 - val_loss: 0.1288 - 2s/epoch - 54ms/step\nEpoch 84/150\n40/40 - 2s - loss: 0.1010 - val_loss: 0.1032 - 2s/epoch - 48ms/step\nEpoch 85/150\n40/40 - 2s - loss: 0.1064 - val_loss: 0.3960 - 2s/epoch - 51ms/step\nEpoch 86/150\n40/40 - 2s - loss: 0.1077 - val_loss: 0.3022 - 2s/epoch - 51ms/step\nEpoch 87/150\n40/40 - 2s - loss: 0.1078 - val_loss: 0.1335 - 2s/epoch - 49ms/step\nEpoch 88/150\n40/40 - 2s - loss: 0.1049 - val_loss: 0.1304 - 2s/epoch - 50ms/step\nEpoch 89/150\n40/40 - 2s - loss: 0.1049 - val_loss: 0.1388 - 2s/epoch - 54ms/step\nEpoch 90/150\n40/40 - 2s - loss: 0.1050 - val_loss: 0.1238 - 2s/epoch - 55ms/step\nEpoch 91/150\n40/40 - 2s - loss: 0.1059 - val_loss: 0.1098 - 2s/epoch - 56ms/step\nEpoch 92/150\n40/40 - 2s - loss: 0.1075 - val_loss: 0.1322 - 2s/epoch - 48ms/step\nEpoch 93/150\n40/40 - 2s - loss: 0.1015 - val_loss: 0.0992 - 2s/epoch - 50ms/step\nEpoch 94/150\n40/40 - 2s - loss: 0.1040 - val_loss: 0.1460 - 2s/epoch - 45ms/step\nEpoch 95/150\n40/40 - 2s - loss: 0.1038 - val_loss: 0.1273 - 2s/epoch - 45ms/step\nEpoch 96/150\n40/40 - 2s - loss: 0.1057 - val_loss: 0.1482 - 2s/epoch - 44ms/step\nEpoch 97/150\n40/40 - 2s - loss: 0.1042 - val_loss: 0.0913 - 2s/epoch - 43ms/step\nEpoch 98/150\n40/40 - 2s - loss: 0.1009 - val_loss: 0.1003 - 2s/epoch - 47ms/step\nEpoch 99/150\n40/40 - 2s - loss: 0.1037 - val_loss: 0.1466 - 2s/epoch - 46ms/step\nEpoch 100/150\n40/40 - 2s - loss: 0.1016 - val_loss: 0.0858 - 2s/epoch - 48ms/step\nEpoch 101/150\n40/40 - 2s - loss: 0.1007 - val_loss: 0.0930 - 2s/epoch - 45ms/step\nEpoch 102/150\n40/40 - 2s - loss: 0.1013 - val_loss: 0.0959 - 2s/epoch - 51ms/step\nEpoch 103/150\n40/40 - 2s - loss: 0.1007 - val_loss: 0.1000 - 2s/epoch - 49ms/step\nEpoch 104/150\n40/40 - 2s - loss: 0.0993 - val_loss: 0.0902 - 2s/epoch - 48ms/step\nEpoch 105/150\n40/40 - 2s - loss: 0.1024 - val_loss: 0.0910 - 2s/epoch - 49ms/step\nEpoch 106/150\n40/40 - 2s - loss: 0.1030 - val_loss: 0.0924 - 2s/epoch - 49ms/step\nEpoch 107/150\n40/40 - 2s - loss: 0.1003 - val_loss: 0.0903 - 2s/epoch - 52ms/step\nEpoch 108/150\n40/40 - 2s - loss: 0.0992 - val_loss: 0.0949 - 2s/epoch - 50ms/step\nEpoch 109/150\n40/40 - 2s - loss: 0.1033 - val_loss: 0.0903 - 2s/epoch - 49ms/step\nEpoch 110/150\n40/40 - 2s - loss: 0.0993 - val_loss: 0.0923 - 2s/epoch - 51ms/step\nEpoch 111/150\n40/40 - 2s - loss: 0.0990 - val_loss: 0.0841 - 2s/epoch - 55ms/step\nEpoch 112/150\n40/40 - 2s - loss: 0.0988 - val_loss: 0.0877 - 2s/epoch - 52ms/step\nEpoch 113/150\n40/40 - 2s - loss: 0.0980 - val_loss: 0.0974 - 2s/epoch - 54ms/step\nEpoch 114/150\n40/40 - 2s - loss: 0.0997 - val_loss: 0.0953 - 2s/epoch - 56ms/step\nEpoch 115/150\n40/40 - 2s - loss: 0.0990 - val_loss: 0.1004 - 2s/epoch - 56ms/step\nEpoch 116/150\n40/40 - 2s - loss: 0.1007 - val_loss: 0.0956 - 2s/epoch - 61ms/step\nEpoch 117/150\n40/40 - 2s - loss: 0.1006 - val_loss: 0.1016 - 2s/epoch - 47ms/step\nEpoch 118/150\n40/40 - 2s - loss: 0.1027 - val_loss: 0.0995 - 2s/epoch - 51ms/step\nEpoch 119/150\n40/40 - 2s - loss: 0.0983 - val_loss: 0.1040 - 2s/epoch - 47ms/step\nEpoch 120/150\n40/40 - 2s - loss: 0.0987 - val_loss: 0.1108 - 2s/epoch - 41ms/step\nEpoch 121/150\n40/40 - 2s - loss: 0.1021 - val_loss: 0.0987 - 2s/epoch - 40ms/step\nEpoch 122/150\n40/40 - 2s - loss: 0.0967 - val_loss: 0.0907 - 2s/epoch - 39ms/step\nEpoch 123/150\n40/40 - 2s - loss: 0.1022 - val_loss: 0.1021 - 2s/epoch - 39ms/step\nEpoch 124/150\n40/40 - 2s - loss: 0.1006 - val_loss: 0.0961 - 2s/epoch - 47ms/step\nEpoch 125/150\n40/40 - 2s - loss: 0.1004 - val_loss: 0.1090 - 2s/epoch - 45ms/step\nEpoch 126/150\n40/40 - 2s - loss: 0.1039 - val_loss: 0.0967 - 2s/epoch - 49ms/step\nEpoch 127/150\n40/40 - 2s - loss: 0.1018 - val_loss: 0.0919 - 2s/epoch - 49ms/step\nEpoch 128/150\n40/40 - 2s - loss: 0.0998 - val_loss: 0.0915 - 2s/epoch - 46ms/step\nEpoch 129/150\n40/40 - 2s - loss: 0.0997 - val_loss: 0.0976 - 2s/epoch - 54ms/step\nEpoch 130/150\n40/40 - 2s - loss: 0.0986 - val_loss: 0.0926 - 2s/epoch - 47ms/step\nEpoch 131/150\n40/40 - 2s - loss: 0.0987 - val_loss: 0.0999 - 2s/epoch - 40ms/step\nEpoch 132/150\n40/40 - 1s - loss: 0.0962 - val_loss: 0.0829 - 1s/epoch - 37ms/step\nEpoch 133/150\n40/40 - 2s - loss: 0.1017 - val_loss: 0.1017 - 2s/epoch - 40ms/step\nEpoch 134/150\n40/40 - 2s - loss: 0.0991 - val_loss: 0.0955 - 2s/epoch - 44ms/step\nEpoch 135/150\n40/40 - 2s - loss: 0.0962 - val_loss: 0.0911 - 2s/epoch - 41ms/step\nEpoch 136/150\n40/40 - 2s - loss: 0.0985 - val_loss: 0.0968 - 2s/epoch - 46ms/step\nEpoch 137/150\n40/40 - 2s - loss: 0.0971 - val_loss: 0.1021 - 2s/epoch - 46ms/step\nEpoch 138/150\n40/40 - 2s - loss: 0.0976 - val_loss: 0.0908 - 2s/epoch - 46ms/step\nEpoch 139/150\n40/40 - 2s - loss: 0.0985 - val_loss: 0.1038 - 2s/epoch - 38ms/step\nEpoch 140/150\n40/40 - 1s - loss: 0.0971 - val_loss: 0.0885 - 1s/epoch - 31ms/step\nEpoch 141/150\n40/40 - 2s - loss: 0.0934 - val_loss: 0.0890 - 2s/epoch - 43ms/step\nEpoch 142/150\n40/40 - 2s - loss: 0.0993 - val_loss: 0.0831 - 2s/epoch - 46ms/step\nEpoch 143/150\n40/40 - 2s - loss: 0.0981 - val_loss: 0.0970 - 2s/epoch - 48ms/step\nEpoch 144/150\n40/40 - 2s - loss: 0.0966 - val_loss: 0.0892 - 2s/epoch - 47ms/step\nEpoch 145/150\n40/40 - 2s - loss: 0.0959 - val_loss: 0.0887 - 2s/epoch - 48ms/step\nEpoch 146/150\n40/40 - 2s - loss: 0.0968 - val_loss: 0.0868 - 2s/epoch - 52ms/step\nEpoch 147/150\n40/40 - 2s - loss: 0.0950 - val_loss: 0.0898 - 2s/epoch - 46ms/step\nEpoch 148/150\n40/40 - 2s - loss: 0.0994 - val_loss: 0.0963 - 2s/epoch - 45ms/step\nEpoch 149/150\n40/40 - 2s - loss: 0.0984 - val_loss: 0.0879 - 2s/epoch - 49ms/step\nEpoch 150/150\n40/40 - 2s - loss: 0.0930 - val_loss: 0.0867 - 2s/epoch - 48ms/step\n\n\nTo see if the training worked, let’s take a look at the training and validation dataset loss:\n\nacc = training.history[\"loss\"]\nval_acc = training.history[\"val_loss\"]\n\nplt.plot(np.arange(len(training.history[\"loss\"])), acc, label=\"Training loss\")\nplt.plot(np.arange(len(training.history[\"loss\"])), val_acc, label=\"Validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nLet’s create a test dataset with few exponents and check the predictions:\n\ntest_exponents = np.linspace(0.5, 1.5, 4)\n\ntest_dataset = DT.create_dataset(T=T, N_models=1000, exponents=test_exponents, models=2)\nX_test = normalize(test_dataset[:, 2:]).reshape(test_dataset.shape[0], T, 1)\nY_test = test_dataset[:, 1]\npreds_test = model.predict(X_test)\n\n125/125 [==============================] - 1s 3ms/step\n\n\nNow we can plot de distribution of predicted exponents for each of the groundtruth ones:\n\nfig, ax = plt.subplots(figsize=(15, 3))\nfor idx, u in enumerate(np.unique(Y_test)):\n    ax.hist(\n        preds_test[Y_test == u],\n        facecolor=f\"C{idx}\",\n        bins=100,\n        alpha=0.4,\n        label=\"Predicted value\" if idx == 0 else \"\",\n    )\n    plt.axvline(u, ls=\"--\", c=f\"C{idx}\", label=\"True value\" if idx == 0 else \"\")\nplt.legend()\nplt.xlabel(\"Anomalous exponent\")\n\nText(0.5, 0, 'Anomalous exponent')\n\n\n\n\n\nThe methods is able to predict the exponent quite correctly! Can you improve the model? Give it a try!\n\n\nExtracting other parameters with ML\nNow that we have defined our network, it is really easy to transform it for any other task. For instance, let’s see how to create a machine that distinguishes between ergodic and non-ergodic trajectories. First, we need to create a new dataset with all the andi-datasets diffusion models. Due to the constraints of the package, we will need to create the subdiffusive and superdiffusive datasets separately and then merge them:\n\nN = 200  # this is the number of trajectories per exponent, which makes a total of 10000\nT = 200  # Length of the trajectories\n\nsubdiffusive_exponents = np.arange(0.2, 1, 0.2)  # We only consider subdiffusion\nsubdiffusive_models = [0, 1, 2, 4]  # All models but LW\n\nsub_dataset = DT.create_dataset(\n    T=T, N_models=N, exponents=subdiffusive_exponents, models=subdiffusive_models\n)\n\nsuperdiffusive_exponents = np.arange(1, 2, 0.2)  # We only consider superdiffusion\nsuperdiffusive_models = [2, 3, 4]  # All models but CTRW and ATTM\n\nsuper_dataset = DT.create_dataset(\n    T=T, N_models=N, exponents=superdiffusive_exponents, models=superdiffusive_models\n)\n\ndataset = np.vstack((sub_dataset, super_dataset))\n\nThe labels now need to be transform such that for ATTM, CTRW and SBM we have 0 (non-ergodic) and for FBM and LW we will have 1 (ergodic):\n\nratio = int(0.8 * dataset.shape[0])\nnp.random.shuffle(dataset)\n# NN inputs: trajectories\nX_train = normalize(dataset[:ratio, 2:]).reshape(ratio, T, 1)\nX_valid = normalize(dataset[ratio:, 2:]).reshape(N - ratio, T, 1)\n\n# NN outputs\n# First we take the models\nY_train = dataset[:ratio, 0]\nY_valid = dataset[ratio:, 0]\n\nwhere_ergodic = np.argwhere((Y_train == 2) | (Y_train == 3))\nwhere_nonergodic = np.argwhere((Y_train == 0) | (Y_train == 1) | (Y_train == 4))\nY_train[where_ergodic] = 0\nY_train[where_nonergodic] = 0\n\nwhere_ergodic = np.argwhere((Y_valid == 2) | (Y_valid == 3))\nwhere_nonergodic = np.argwhere((Y_valid == 0) | (Y_valid == 1) | (Y_valid == 4))\nY_valid[where_ergodic] = 0\nY_valid[where_nonergodic] = 0\n\nFinally, we can use the same architecture as before, but will need to change the last layer to two neurons with softmax activation, as e.g.:\n\nmodel.add(Dense(2, activation=\"softmax\"))\n\nAre you ready to train the newtwork by yourself?"
  },
  {
    "objectID": "tutorials/challenge_one_submission.html",
    "href": "tutorials/challenge_one_submission.html",
    "title": "2. Create submissions",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nThe best way of dealing with the available datasets is by means of the andi-datasets python package, which can be installed using pip install andi-datasets. Then, the dataset can be created from the function challenge_theory_dataset, allocated in the datasets_theory module.\nFor this tutorial, I have first downloaded the datasets available in the Challenge webpage (www.andi-challenge.org). Note that you need to register before being able to access the data. You will have access to two datasets, one for training, which is labeled, and one for scoring, which is used to rank the participants in the competition. In this case, I downloaded each of the datasets in the following folders:\nTo load the training dataset, we only need to do:\nIn the case you are working with a validation dataset, and do not have the labels (in this case stored in the files refX.txt), you can use the optional parameter load_labels = False, as follows:"
  },
  {
    "objectID": "tutorials/challenge_one_submission.html#the-good-old-way-the-tmsd-fitting",
    "href": "tutorials/challenge_one_submission.html#the-good-old-way-the-tmsd-fitting",
    "title": "2. Create submissions",
    "section": "The ‘good old’ way: the tMSD fitting",
    "text": "The ‘good old’ way: the tMSD fitting\nOne way to extract the anomalous exponent is by fitting the tMSD: \\[\n\\mbox{tMSD}(\\Delta) = \\frac{1}{T-\\Delta} \\sum_{i=1}^{T-\\Delta}(x(t_i + \\Delta)-x(t_i))^2,\n\\] where \\(\\Delta\\) is defined as the time lag and \\(T\\) is length of the trajectory.\n\ndef TMSD(traj, t_lags):\n    ttt = np.zeros_like(t_lags, dtype= float)\n    for idx, t in enumerate(t_lags): \n        for p in range(len(traj)-t):\n            ttt[idx] += (traj[p]-traj[p+t])**2            \n        ttt[idx] /= len(traj)-t    \n    return ttt\n\nWe know that (usually) \\[\\mbox{tMSD}(\\Delta) \\sim \\Delta ^ \\alpha,\\] hence we can use it to extract the anomalous exponent. Let us check this on trajectories from two models: ATTM and FBM. For that we can again use the andi package, and access the diffusion models directly:\n\nfrom andi.models_theory import models_theory\nMT = models_theory()\n\n# We create one ATTM and one FBM trajectory with alpha = 0.2\nattm = MT._oneD().attm(T = 1000, alpha = 0.2)\nfbm = MT._oneD().fbm(T = 1000, alpha = 0.2)\n\n# We calculate their tMSD\nt_lags = np.arange(2, 20)\nattm_tmsd = TMSD(attm, t_lags = t_lags)\nfbm_tmsd = TMSD(fbm, t_lags = t_lags)\n\nLet’s plot the tMSD:\n\nfig, ax = plt.subplots(1,2, figsize = (10, 4))\n\nax[0].loglog(t_lags, fbm_tmsd, '-o', lw = 1)\nax[0].loglog(t_lags, t_lags**0.2/(t_lags[0]**0.2)*fbm_tmsd[0], ls = '--')\nax[0].loglog(t_lags, t_lags/(t_lags[0])*fbm_tmsd[0], ls = '--')\nax[0].set_title(r'FBM $\\rightarrow$ Ergodic process')\n\nax[1].loglog(t_lags, attm_tmsd, '-o', lw = 1,label = 'tMSD')\nax[1].loglog(t_lags, t_lags**0.2/(t_lags[0]**0.2)*attm_tmsd[0], ls = '--', label = r'$\\sim \\Delta^{0.2}$')\nax[1].loglog(t_lags, t_lags/(t_lags[0])*attm_tmsd[0], ls = '--', label = r'$\\sim \\Delta$')\nax[1].set_title(r'ATTM $\\rightarrow$ Non-ergodic process')\nax[1].legend(fontsize = 16)\n\nplt.setp(ax, xlabel = r'$\\Delta$', ylabel = 'tMSD');\nfig.tight_layout()\n\n\n\n\nWe see that the tMSD works very well for ergodic processes, but fails horribly for non-ergodic, for which we usually have that \\(tMSD\\sim\\Delta\\). Nevertheless, let’s use it to fit the exponent of the 1D training dataset:\n\nt_lags = np.arange(2,10)\npredictions = []\n\nfor traj in X1[0]:\n    tmsd = TMSD(traj, t_lags)\n    predictions.append(np.polyfit(np.log(t_lags), np.log(tmsd),1)[0])\n    \nprint('MAE = '+str(np.round(np.mean(np.abs(np.array(predictions)-Y1[0])), 4)))\n\nMAE = 0.3342\n\n\nLet’s see how is the error distributed:\n\nplt.hist(np.array(predictions)-Y1[0], bins = 50);\nplt.xlabel('Error')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\nWe can now use the same method to predict the exponent of the validation dataset V1, for 1D\n\nt_lags = np.arange(1,10)\npredictions_task1_1d = []\n\nfor traj in validation[0][0]:\n    tmsd = TMSD(traj, t_lags)\n    predictions_task1_1d.append(np.polyfit(np.log(t_lags), np.log(tmsd),1)[0])\n\nTo make a submission, you only need to write a .txt file for which: - The name is the task: task1.txt, task2.txt, task3.txt - The first column is the dimension (1,2 or 3) - The following columns are the results - Delimiter should be ;\n\npred_to_txt = np.ones((len(predictions_task1_1d), 2))\npred_to_txt[:, 1] = predictions_task1_1d\n\nnp.savetxt('task1.txt', pred_to_txt.astype(float), fmt = '%1.5f', delimiter = ';')\n\n\nThen, we zip it and submit!"
  },
  {
    "objectID": "tutorials/challenge_one_submission.html#the-new-trend-machine-learning",
    "href": "tutorials/challenge_one_submission.html#the-new-trend-machine-learning",
    "title": "2. Create submissions",
    "section": "The new trend: machine learning",
    "text": "The new trend: machine learning\nThere are various approaches to model classification: statistical tests to differentiate between CTRW and FBM, Bayesian inference,…etc. In this example we will use the latest proposal: Machine Learning.\nOne of the main difficulties of the ANDI challenge is that we have trajectories of all lengths! Having ML models able to accomodate such feature is one of the main challenges the participants will face.\nFor the sake of simplicity, I will solve here an easier problem: classifying between the subdiffusive models (ATTM, FBM, CTRW, SBM), with exponents \\(\\in \\ [0.1, 1]\\), with trajectories of all \\(30\\) points. To generate such dataset, I can use another function from the andi-datasets package: create_dataset from the datasets_theory class. You can check all the details of this function in this tutorial notebook.\n\nfrom andi_datasets.datasets_theory import datasets_theory\nDT = datasets_theory()\n# Here I load a dataset that I have already generated. To create a new one, you just new to put load_trajectories = False\n# Check the tutorials in the github for all the details\ndataset = DT.create_dataset(T = 30, N_models = 1000, exponents = np.arange(0.1,1,0.05), models = [0,1,2,4], \n                              load_trajectories = True, path = '/home/gmunoz/andi_data/datasets/')\n\nAs usually done in Machine Learning, we shuffle and create trainina/test set with 80-20% ratios:\n\nnp.random.shuffle(dataset)\n\nratio = int(0.8*dataset.shape[0])\n# We normalize the trajectories so all of them are in the same 'scale'\nfrom andi.utils_trajectories import normalize\nX_a = andi.normalize(dataset[:ratio, 2:]).reshape(ratio, T, 1)\nX_e = andi.normalize(dataset[ratio:, 2:]).reshape(N-ratio, T, 1)\n\ndataset[dataset[:,0] == 4, 0] = 3\nY_a = to_categorical(dataset[:ratio, 0])\nY_e = to_categorical(dataset[ratio:, 0])\n\n(14400, 30, 1)\n\n\nWe import the necessary packages for creating our neural network:\n\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Conv1D, Dropout, BatchNormalization, Flatten\n\nfrom keras.regularizers import l2 as regularizer_l2\n\nfrom keras.optimizers import Adam\n\nNow let’s create a typical Convolutional neural network with keras, with some L2 regularizers and Dropout and Batch Normalization layers.\n\nmodel = Sequential()\n\n# Here we define the architecture of the Neural Network\nmodel.add(Conv1D(filters=3, kernel_size=3 ,strides=1,   \n                 input_shape=(T, 1),\n                 kernel_initializer= 'uniform',      \n                 activation= 'relu', kernel_regularizer = regularizer_l2(l = 0.001)))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\nmodel.add(Conv1D(filters=8, kernel_size=5 ,strides=1,  \n                 kernel_initializer= 'uniform',      \n                 activation= 'relu', kernel_regularizer = regularizer_l2(l = 0.001)))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))   \nmodel.add(Conv1D(filters=3, kernel_size=2 ,strides=1,  \n                 kernel_initializer= 'uniform',      \n                 activation= 'relu', kernel_regularizer = regularizer_l2(l = 0.001)))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64*2, activation='sigmoid', kernel_regularizer = regularizer_l2(l = 0.001)))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='sigmoid'))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\n\n# Last layer needs to have same size as number of processes\nnumber_process = 4\n\nmodel.add(Dense(number_process, activation='softmax'))\n\n# We add loss function + Adam optimizer\nmodel.compile(loss='binary_crossentropy',\n              optimizer=Adam(),\n              metrics=['accuracy'])\n\nLet’s train the model:\n\nbatch_size = 200\nepochs = 150\n\nhistory = model.fit(X_a, Y_a,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=2,\n                    validation_data=(X_e, Y_e))\n\nmodel.save('model_classification_subdiffusive.h5')\n\nTrain on 57600 samples, validate on 14400 samples\nEpoch 1/150\n - 5s - loss: 0.5244 - accuracy: 0.7870 - val_loss: 0.6578 - val_accuracy: 0.7500\nEpoch 2/150\n - 4s - loss: 0.4146 - accuracy: 0.8187 - val_loss: 0.6470 - val_accuracy: 0.7402\nEpoch 3/150\n - 3s - loss: 0.3785 - accuracy: 0.8263 - val_loss: 0.4752 - val_accuracy: 0.7945\nEpoch 4/150\n - 4s - loss: 0.3654 - accuracy: 0.8290 - val_loss: 0.4057 - val_accuracy: 0.8157\nEpoch 5/150\n - 4s - loss: 0.3511 - accuracy: 0.8332 - val_loss: 0.3586 - val_accuracy: 0.8340\nEpoch 6/150\n - 3s - loss: 0.3471 - accuracy: 0.8351 - val_loss: 0.3385 - val_accuracy: 0.8392\nEpoch 7/150\n - 4s - loss: 0.3395 - accuracy: 0.8380 - val_loss: 0.3169 - val_accuracy: 0.8479\nEpoch 8/150\n - 4s - loss: 0.3346 - accuracy: 0.8406 - val_loss: 0.3121 - val_accuracy: 0.8524\nEpoch 9/150\n - 3s - loss: 0.3311 - accuracy: 0.8421 - val_loss: 0.3325 - val_accuracy: 0.8401\nEpoch 10/150\n - 4s - loss: 0.3319 - accuracy: 0.8413 - val_loss: 0.3093 - val_accuracy: 0.8509\nEpoch 11/150\n - 4s - loss: 0.3314 - accuracy: 0.8418 - val_loss: 0.3074 - val_accuracy: 0.8524\nEpoch 12/150\n - 3s - loss: 0.3286 - accuracy: 0.8427 - val_loss: 0.3098 - val_accuracy: 0.8501\nEpoch 13/150\n - 2s - loss: 0.3309 - accuracy: 0.8420 - val_loss: 0.3088 - val_accuracy: 0.8518\nEpoch 14/150\n - 2s - loss: 0.3356 - accuracy: 0.8399 - val_loss: 0.3089 - val_accuracy: 0.8519\nEpoch 15/150\n - 4s - loss: 0.3245 - accuracy: 0.8447 - val_loss: 0.3044 - val_accuracy: 0.8540\nEpoch 16/150\n - 3s - loss: 0.3288 - accuracy: 0.8427 - val_loss: 0.3072 - val_accuracy: 0.8522\nEpoch 17/150\n - 4s - loss: 0.3319 - accuracy: 0.8416 - val_loss: 0.3087 - val_accuracy: 0.8508\nEpoch 18/150\n - 4s - loss: 0.3237 - accuracy: 0.8450 - val_loss: 0.3048 - val_accuracy: 0.8534\nEpoch 19/150\n - 4s - loss: 0.3255 - accuracy: 0.8445 - val_loss: 0.3117 - val_accuracy: 0.8490\nEpoch 20/150\n - 4s - loss: 0.3257 - accuracy: 0.8443 - val_loss: 0.3086 - val_accuracy: 0.8512\nEpoch 21/150\n - 3s - loss: 0.3266 - accuracy: 0.8440 - val_loss: 0.3075 - val_accuracy: 0.8525\nEpoch 22/150\n - 4s - loss: 0.3264 - accuracy: 0.8438 - val_loss: 0.3026 - val_accuracy: 0.8560\nEpoch 23/150\n - 4s - loss: 0.3234 - accuracy: 0.8460 - val_loss: 0.3101 - val_accuracy: 0.8514\nEpoch 24/150\n - 4s - loss: 0.3223 - accuracy: 0.8466 - val_loss: 0.3036 - val_accuracy: 0.8531\nEpoch 25/150\n - 4s - loss: 0.3303 - accuracy: 0.8427 - val_loss: 0.3171 - val_accuracy: 0.8473\nEpoch 26/150\n - 4s - loss: 0.3215 - accuracy: 0.8468 - val_loss: 0.3066 - val_accuracy: 0.8532\nEpoch 27/150\n - 3s - loss: 0.3219 - accuracy: 0.8457 - val_loss: 0.3066 - val_accuracy: 0.8514\nEpoch 28/150\n - 4s - loss: 0.3239 - accuracy: 0.8452 - val_loss: 0.3014 - val_accuracy: 0.8575\nEpoch 29/150\n - 4s - loss: 0.3219 - accuracy: 0.8470 - val_loss: 0.3125 - val_accuracy: 0.8499\nEpoch 30/150\n - 3s - loss: 0.3225 - accuracy: 0.8461 - val_loss: 0.3702 - val_accuracy: 0.8294\nEpoch 31/150\n - 2s - loss: 0.3262 - accuracy: 0.8450 - val_loss: 0.3037 - val_accuracy: 0.8542\nEpoch 32/150\n - 4s - loss: 0.3222 - accuracy: 0.8465 - val_loss: 0.3134 - val_accuracy: 0.8510\nEpoch 33/150\n - 3s - loss: 0.3216 - accuracy: 0.8467 - val_loss: 0.3010 - val_accuracy: 0.8572\nEpoch 34/150\n - 4s - loss: 0.3206 - accuracy: 0.8478 - val_loss: 0.3023 - val_accuracy: 0.8550\nEpoch 35/150\n - 4s - loss: 0.3196 - accuracy: 0.8477 - val_loss: 0.2991 - val_accuracy: 0.8566\nEpoch 36/150\n - 3s - loss: 0.3197 - accuracy: 0.8477 - val_loss: 0.3001 - val_accuracy: 0.8576\nEpoch 37/150\n - 4s - loss: 0.3201 - accuracy: 0.8470 - val_loss: 0.3039 - val_accuracy: 0.8560\nEpoch 38/150\n - 4s - loss: 0.3187 - accuracy: 0.8483 - val_loss: 0.3007 - val_accuracy: 0.8572\nEpoch 39/150\n - 4s - loss: 0.3200 - accuracy: 0.8475 - val_loss: 0.3024 - val_accuracy: 0.8569\nEpoch 40/150\n - 4s - loss: 0.3199 - accuracy: 0.8477 - val_loss: 0.3166 - val_accuracy: 0.8492\nEpoch 41/150\n - 3s - loss: 0.3218 - accuracy: 0.8473 - val_loss: 0.3144 - val_accuracy: 0.8480\nEpoch 42/150\n - 4s - loss: 0.3185 - accuracy: 0.8486 - val_loss: 0.3139 - val_accuracy: 0.8473\nEpoch 43/150\n - 4s - loss: 0.3186 - accuracy: 0.8481 - val_loss: 0.2953 - val_accuracy: 0.8595\nEpoch 44/150\n - 3s - loss: 0.3180 - accuracy: 0.8488 - val_loss: 0.2994 - val_accuracy: 0.8583\nEpoch 45/150\n - 4s - loss: 0.3182 - accuracy: 0.8488 - val_loss: 0.2968 - val_accuracy: 0.8581\nEpoch 46/150\n - 2s - loss: 0.3193 - accuracy: 0.8476 - val_loss: 0.3008 - val_accuracy: 0.8561\nEpoch 47/150\n - 2s - loss: 0.3204 - accuracy: 0.8477 - val_loss: 0.2987 - val_accuracy: 0.8558\nEpoch 48/150\n - 4s - loss: 0.3174 - accuracy: 0.8495 - val_loss: 0.2963 - val_accuracy: 0.8597\nEpoch 49/150\n - 5s - loss: 0.3170 - accuracy: 0.8487 - val_loss: 0.3264 - val_accuracy: 0.8434\nEpoch 50/150\n - 4s - loss: 0.3187 - accuracy: 0.8490 - val_loss: 0.2968 - val_accuracy: 0.8581\nEpoch 51/150\n - 4s - loss: 0.3192 - accuracy: 0.8483 - val_loss: 0.2976 - val_accuracy: 0.8597\nEpoch 52/150\n - 4s - loss: 0.3177 - accuracy: 0.8491 - val_loss: 0.2996 - val_accuracy: 0.8561\nEpoch 53/150\n - 4s - loss: 0.3169 - accuracy: 0.8493 - val_loss: 0.2977 - val_accuracy: 0.8579\nEpoch 54/150\n - 4s - loss: 0.3175 - accuracy: 0.8501 - val_loss: 0.3034 - val_accuracy: 0.8580\nEpoch 55/150\n - 4s - loss: 0.3179 - accuracy: 0.8492 - val_loss: 0.2970 - val_accuracy: 0.8572\nEpoch 56/150\n - 4s - loss: 0.3175 - accuracy: 0.8496 - val_loss: 0.3085 - val_accuracy: 0.8523\nEpoch 57/150\n - 4s - loss: 0.3168 - accuracy: 0.8501 - val_loss: 0.3300 - val_accuracy: 0.8453\nEpoch 58/150\n - 4s - loss: 0.3172 - accuracy: 0.8493 - val_loss: 0.2985 - val_accuracy: 0.8569\nEpoch 59/150\n - 4s - loss: 0.3156 - accuracy: 0.8506 - val_loss: 0.3153 - val_accuracy: 0.8484\nEpoch 60/150\n - 4s - loss: 0.3189 - accuracy: 0.8485 - val_loss: 0.2975 - val_accuracy: 0.8592\nEpoch 61/150\n - 3s - loss: 0.3171 - accuracy: 0.8491 - val_loss: 0.3077 - val_accuracy: 0.8539\nEpoch 62/150\n - 2s - loss: 0.3167 - accuracy: 0.8496 - val_loss: 0.3044 - val_accuracy: 0.8539\nEpoch 63/150\n - 4s - loss: 0.3172 - accuracy: 0.8488 - val_loss: 0.2994 - val_accuracy: 0.8580\nEpoch 64/150\n - 3s - loss: 0.3143 - accuracy: 0.8507 - val_loss: 0.2979 - val_accuracy: 0.8574\nEpoch 65/150\n - 4s - loss: 0.3159 - accuracy: 0.8500 - val_loss: 0.3046 - val_accuracy: 0.8561\nEpoch 66/150\n - 4s - loss: 0.3196 - accuracy: 0.8486 - val_loss: 0.3017 - val_accuracy: 0.8551\nEpoch 67/150\n - 4s - loss: 0.3187 - accuracy: 0.8485 - val_loss: 0.3205 - val_accuracy: 0.8495\nEpoch 68/150\n - 4s - loss: 0.3154 - accuracy: 0.8506 - val_loss: 0.2971 - val_accuracy: 0.8579\nEpoch 69/150\n - 4s - loss: 0.3151 - accuracy: 0.8512 - val_loss: 0.2971 - val_accuracy: 0.8592\nEpoch 70/150\n - 3s - loss: 0.3140 - accuracy: 0.8510 - val_loss: 0.3051 - val_accuracy: 0.8550\nEpoch 71/150\n - 4s - loss: 0.3161 - accuracy: 0.8501 - val_loss: 0.2938 - val_accuracy: 0.8603\nEpoch 72/150\n - 4s - loss: 0.3155 - accuracy: 0.8509 - val_loss: 0.2961 - val_accuracy: 0.8602\nEpoch 73/150\n - 3s - loss: 0.3159 - accuracy: 0.8498 - val_loss: 0.2990 - val_accuracy: 0.8581\nEpoch 74/150\n - 4s - loss: 0.3154 - accuracy: 0.8508 - val_loss: 0.2952 - val_accuracy: 0.8597\nEpoch 75/150\n - 4s - loss: 0.3149 - accuracy: 0.8512 - val_loss: 0.2982 - val_accuracy: 0.8591\nEpoch 76/150\n - 4s - loss: 0.3155 - accuracy: 0.8505 - val_loss: 0.2949 - val_accuracy: 0.8606\nEpoch 77/150\n - 3s - loss: 0.3179 - accuracy: 0.8495 - val_loss: 0.3051 - val_accuracy: 0.8532\nEpoch 78/150\n - 3s - loss: 0.3147 - accuracy: 0.8513 - val_loss: 0.2938 - val_accuracy: 0.8601\nEpoch 79/150\n - 3s - loss: 0.3147 - accuracy: 0.8508 - val_loss: 0.2997 - val_accuracy: 0.8584\nEpoch 80/150\n - 4s - loss: 0.3169 - accuracy: 0.8498 - val_loss: 0.2966 - val_accuracy: 0.8597\nEpoch 81/150\n - 4s - loss: 0.3141 - accuracy: 0.8503 - val_loss: 0.3137 - val_accuracy: 0.8490\nEpoch 82/150\n - 4s - loss: 0.3181 - accuracy: 0.8492 - val_loss: 0.2938 - val_accuracy: 0.8609\nEpoch 83/150\n - 4s - loss: 0.3155 - accuracy: 0.8505 - val_loss: 0.2962 - val_accuracy: 0.8601\nEpoch 84/150\n - 3s - loss: 0.3173 - accuracy: 0.8490 - val_loss: 0.3018 - val_accuracy: 0.8548\nEpoch 85/150\n - 4s - loss: 0.3174 - accuracy: 0.8496 - val_loss: 0.2923 - val_accuracy: 0.8619\nEpoch 86/150\n - 4s - loss: 0.3150 - accuracy: 0.8500 - val_loss: 0.3091 - val_accuracy: 0.8556\nEpoch 87/150\n - 3s - loss: 0.3158 - accuracy: 0.8504 - val_loss: 0.3082 - val_accuracy: 0.8545\nEpoch 88/150\n - 4s - loss: 0.3151 - accuracy: 0.8511 - val_loss: 0.3004 - val_accuracy: 0.8587\nEpoch 89/150\n - 4s - loss: 0.3145 - accuracy: 0.8511 - val_loss: 0.2922 - val_accuracy: 0.8614\nEpoch 90/150\n - 3s - loss: 0.3149 - accuracy: 0.8508 - val_loss: 0.2940 - val_accuracy: 0.8599\nEpoch 91/150\n - 4s - loss: 0.3132 - accuracy: 0.8523 - val_loss: 0.3229 - val_accuracy: 0.8477\nEpoch 92/150\n - 3s - loss: 0.3151 - accuracy: 0.8505 - val_loss: 0.2908 - val_accuracy: 0.8624\nEpoch 93/150\n - 2s - loss: 0.3164 - accuracy: 0.8501 - val_loss: 0.2949 - val_accuracy: 0.8609\nEpoch 94/150\n - 2s - loss: 0.3148 - accuracy: 0.8511 - val_loss: 0.3067 - val_accuracy: 0.8551\nEpoch 95/150\n - 4s - loss: 0.3161 - accuracy: 0.8498 - val_loss: 0.3004 - val_accuracy: 0.8574\nEpoch 96/150\n - 4s - loss: 0.3167 - accuracy: 0.8503 - val_loss: 0.2928 - val_accuracy: 0.8610\nEpoch 97/150\n - 4s - loss: 0.3150 - accuracy: 0.8508 - val_loss: 0.3004 - val_accuracy: 0.8580\nEpoch 98/150\n - 4s - loss: 0.3116 - accuracy: 0.8521 - val_loss: 0.2929 - val_accuracy: 0.8602\nEpoch 99/150\n - 4s - loss: 0.3157 - accuracy: 0.8505 - val_loss: 0.3035 - val_accuracy: 0.8582\nEpoch 100/150\n - 4s - loss: 0.3158 - accuracy: 0.8506 - val_loss: 0.3158 - val_accuracy: 0.8476\nEpoch 101/150\n - 4s - loss: 0.3153 - accuracy: 0.8498 - val_loss: 0.3024 - val_accuracy: 0.8580\nEpoch 102/150\n - 3s - loss: 0.3130 - accuracy: 0.8520 - val_loss: 0.2985 - val_accuracy: 0.8602\nEpoch 103/150\n - 4s - loss: 0.3136 - accuracy: 0.8516 - val_loss: 0.2933 - val_accuracy: 0.8605\nEpoch 104/150\n - 4s - loss: 0.3134 - accuracy: 0.8509 - val_loss: 0.2964 - val_accuracy: 0.8572\nEpoch 105/150\n - 4s - loss: 0.3121 - accuracy: 0.8512 - val_loss: 0.2975 - val_accuracy: 0.8592\nEpoch 106/150\n - 4s - loss: 0.3176 - accuracy: 0.8502 - val_loss: 0.4506 - val_accuracy: 0.8130\nEpoch 107/150\n - 4s - loss: 0.3171 - accuracy: 0.8506 - val_loss: 0.2937 - val_accuracy: 0.8611\nEpoch 108/150\n - 4s - loss: 0.3136 - accuracy: 0.8519 - val_loss: 0.2912 - val_accuracy: 0.8632\nEpoch 109/150\n - 4s - loss: 0.3152 - accuracy: 0.8504 - val_loss: 0.3002 - val_accuracy: 0.8585\nEpoch 110/150\n - 2s - loss: 0.3157 - accuracy: 0.8503 - val_loss: 0.2989 - val_accuracy: 0.8569\nEpoch 111/150\n - 3s - loss: 0.3119 - accuracy: 0.8523 - val_loss: 0.2939 - val_accuracy: 0.8615\nEpoch 112/150\n - 4s - loss: 0.3102 - accuracy: 0.8533 - val_loss: 0.2886 - val_accuracy: 0.8641\nEpoch 113/150\n - 3s - loss: 0.3140 - accuracy: 0.8520 - val_loss: 0.2975 - val_accuracy: 0.8585\nEpoch 114/150\n - 4s - loss: 0.3124 - accuracy: 0.8522 - val_loss: 0.2976 - val_accuracy: 0.8593\nEpoch 115/150\n - 4s - loss: 0.3133 - accuracy: 0.8519 - val_loss: 0.2893 - val_accuracy: 0.8635\nEpoch 116/150\n - 4s - loss: 0.3138 - accuracy: 0.8516 - val_loss: 0.3008 - val_accuracy: 0.8551\nEpoch 117/150\n - 4s - loss: 0.3183 - accuracy: 0.8494 - val_loss: 0.2989 - val_accuracy: 0.8586\nEpoch 118/150\n - 4s - loss: 0.3137 - accuracy: 0.8517 - val_loss: 0.3018 - val_accuracy: 0.8577\nEpoch 119/150\n - 3s - loss: 0.3101 - accuracy: 0.8538 - val_loss: 0.2981 - val_accuracy: 0.8569\nEpoch 120/150\n - 2s - loss: 0.3133 - accuracy: 0.8523 - val_loss: 0.2976 - val_accuracy: 0.8567\nEpoch 121/150\n - 2s - loss: 0.3109 - accuracy: 0.8529 - val_loss: 0.2930 - val_accuracy: 0.8607\nEpoch 122/150\n - 2s - loss: 0.3101 - accuracy: 0.8535 - val_loss: 0.2965 - val_accuracy: 0.8584\nEpoch 123/150\n - 3s - loss: 0.3139 - accuracy: 0.8516 - val_loss: 0.3024 - val_accuracy: 0.8576\nEpoch 124/150\n - 4s - loss: 0.3115 - accuracy: 0.8529 - val_loss: 0.2907 - val_accuracy: 0.8611\nEpoch 125/150\n - 4s - loss: 0.3100 - accuracy: 0.8532 - val_loss: 0.2922 - val_accuracy: 0.8626\nEpoch 126/150\n - 3s - loss: 0.3153 - accuracy: 0.8517 - val_loss: 0.3035 - val_accuracy: 0.8557\nEpoch 127/150\n - 3s - loss: 0.3101 - accuracy: 0.8535 - val_loss: 0.2904 - val_accuracy: 0.8624\nEpoch 128/150\n - 2s - loss: 0.3115 - accuracy: 0.8526 - val_loss: 0.2980 - val_accuracy: 0.8591\nEpoch 129/150\n - 4s - loss: 0.3098 - accuracy: 0.8536 - val_loss: 0.2916 - val_accuracy: 0.8622\nEpoch 130/150\n - 4s - loss: 0.3099 - accuracy: 0.8537 - val_loss: 0.2956 - val_accuracy: 0.8584\nEpoch 131/150\n - 4s - loss: 0.3089 - accuracy: 0.8541 - val_loss: 0.2937 - val_accuracy: 0.8602\nEpoch 132/150\n - 3s - loss: 0.3103 - accuracy: 0.8531 - val_loss: 0.2922 - val_accuracy: 0.8601\nEpoch 133/150\n - 4s - loss: 0.3151 - accuracy: 0.8515 - val_loss: 0.3039 - val_accuracy: 0.8567\nEpoch 134/150\n - 4s - loss: 0.3139 - accuracy: 0.8521 - val_loss: 0.2920 - val_accuracy: 0.8620\nEpoch 135/150\n - 4s - loss: 0.3099 - accuracy: 0.8528 - val_loss: 0.2908 - val_accuracy: 0.8626\nEpoch 136/150\n - 4s - loss: 0.3093 - accuracy: 0.8537 - val_loss: 0.2933 - val_accuracy: 0.8631\nEpoch 137/150\n - 4s - loss: 0.3106 - accuracy: 0.8536 - val_loss: 0.3051 - val_accuracy: 0.8544\nEpoch 138/150\n - 4s - loss: 0.3107 - accuracy: 0.8530 - val_loss: 0.3040 - val_accuracy: 0.8549\nEpoch 139/150\n - 4s - loss: 0.3100 - accuracy: 0.8532 - val_loss: 0.2892 - val_accuracy: 0.8628\nEpoch 140/150\n - 4s - loss: 0.3092 - accuracy: 0.8533 - val_loss: 0.2935 - val_accuracy: 0.8603\nEpoch 141/150\n - 4s - loss: 0.3157 - accuracy: 0.8509 - val_loss: 0.5051 - val_accuracy: 0.7945\nEpoch 142/150\n - 4s - loss: 0.3150 - accuracy: 0.8513 - val_loss: 0.2948 - val_accuracy: 0.8604\nEpoch 143/150\n - 2s - loss: 0.3111 - accuracy: 0.8529 - val_loss: 0.2884 - val_accuracy: 0.8630\nEpoch 144/150\n - 4s - loss: 0.3094 - accuracy: 0.8537 - val_loss: 0.2892 - val_accuracy: 0.8624\nEpoch 145/150\n - 4s - loss: 0.3102 - accuracy: 0.8530 - val_loss: 0.3069 - val_accuracy: 0.8549\nEpoch 146/150\n - 4s - loss: 0.3101 - accuracy: 0.8533 - val_loss: 0.2947 - val_accuracy: 0.8606\nEpoch 147/150\n - 4s - loss: 0.3125 - accuracy: 0.8525 - val_loss: 0.2926 - val_accuracy: 0.8598\nEpoch 148/150\n - 5s - loss: 0.3079 - accuracy: 0.8543 - val_loss: 0.2931 - val_accuracy: 0.8595\nEpoch 149/150\n - 3s - loss: 0.3107 - accuracy: 0.8533 - val_loss: 0.2940 - val_accuracy: 0.8615\nEpoch 150/150\n - 4s - loss: 0.3084 - accuracy: 0.8529 - val_loss: 0.2865 - val_accuracy: 0.8646\n\n\n\nacc = history.history['loss']\nval_acc = history.history['val_loss']\n\nplt.plot(np.arange(len(history.history['accuracy'])), acc, label='Training loss')\nplt.plot(np.arange(len(history.history['accuracy'])), val_acc,label='Validation loss')\nplt.title('FCN - Training and validation accuracy T ='+str(T))\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nIn the ANDI challenge, the rank of task 2 is evaluated with the F1 score. Let’s see how well/bad we did (best value is 1). Recall that this is a toy example, as we are not considergin LW.\n\nfrom sklearn.metrics import f1_score\n\ngroundtruth = np.argmax(Y_e, axis = 1)\npredictions = np.argmax(model.predict(X_e), axis = 1)\n\nf1_score(groundtruth, predictions, average='micro')\n\n0.7168055555555556\n\n\nNot that bad! To analyze a bit more the predictions, we can use the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\nconf = confusion_matrix(groundtruth, predictions)/(predictions.shape[0]/2)\n\n\nfig, ax = plt.subplots(figsize = (7,7))\nax.matshow(conf)\nfor (i, j), z in np.ndenumerate(conf):\n    ax.text(j, i, '{:0.3f}'.format(z), ha='center', va='center', fontsize = 16)\nax.set_xticklabels(['c','ATTM','CTRW','FBM','SBM'], fontsize = 16)\nax.set_yticklabels(['a','ATTM','CTRW','FBM','SBM'], fontsize = 16)\nax.set_xlabel('Predicted class', fontsize = 16)\nax.set_ylabel('Groundtruth', fontsize = 16)\nax.xaxis.set_ticks_position('bottom')\n\n\n\n\nWe see here that the method is not perfect. For instance, it has a very hard time correctly classifying trajectories ATTM trajectories. For CTRW, the job is easier! Take into account that here we are working with trajectories without noise, contrary to what we have in the Challenge.\n\nNow you are ready to use your favourite ML architecture on the true ANDI dataset! Can you do better?"
  },
  {
    "objectID": "tutorials/index_tutorials.html",
    "href": "tutorials/index_tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Here we gather a collection of tutorials for different parts of this library. You can access all the notebooks in our repo."
  },
  {
    "objectID": "tutorials/index_tutorials.html#andi-1-challenge",
    "href": "tutorials/index_tutorials.html#andi-1-challenge",
    "title": "Tutorials",
    "section": "AnDi 1 challenge:",
    "text": "AnDi 1 challenge:\n\nTheory models and AnDi 1: this tutorials introduced the theoretical models contained in this library, as well as the generation of datasets for the AnDi 2020 challenge.\nAnDi 1 submission: from trajectories to predictions: this tutorial shows how to manage the datasets given in the AnDi 2020 challenge and shows how to do predictions both with an statistical approach as with machine learning method."
  },
  {
    "objectID": "tutorials/index_tutorials.html#andi-2-challenge",
    "href": "tutorials/index_tutorials.html#andi-2-challenge",
    "title": "Tutorials",
    "section": "AnDi 2 challenge:",
    "text": "AnDi 2 challenge:\n\nPhenom models and AnDi 2: this tutorial overviews the phenomenological models contained in the library.\nCreating videos tutorial: this tutorial shows how to generate videos using combining the deep-track library together with andi-datasets.\nAnDi 2: from the data to the submission: here we showcase how to read the data of the challenge, create a submission but also how to generate your own data."
  },
  {
    "objectID": "tutorials/challenge_two_datasets.html",
    "href": "tutorials/challenge_two_datasets.html",
    "title": "1. Explore the models",
    "section": "",
    "text": "Welcome to the guide for the ANDI 2 challenge datasets. In this notebook we will explore the new diffusion models considered for the challenge. As opposed to the diffusion models of the previous challenge (check the paper and the previous tutorial ), this time we focus on phenomenological models. That is, models in which diffusion properties arise from the interaction of the moving particle with the environment, or due to its inherent properties. Moreover, we will focus in models in which diffusive changes happened randomly, because of different reasons. Nonetheless, we are still in the ANDI Challenge, so anomalous is still here! All particles will diffuse following fractional Brownian motion, with different anomalous exponents \\(\\alpha\\). We have developed 5 types of diffusion, with which we want to cover most of the phenomena one encounters in real, physical, scenarios. Before reviewing them, let us set some initial conditions which will be considered in all models. These parameters will be similar to the ones considered in the datasets generated for the challenge.\n\nExperimental diffusion conditions\n\nField of view fov and system size L\nWe consider the trajectories to be recorded by a device with field of view \\(128 \\times 128 px^2\\), with a pixel size of 100 nm. In order to avoid boundary effects, we will usually simulate the diffusion in a box of size \\(L =1.5 \\ \\times\\) fov.\n\n\nFrame rate\nIn the same spirit, we consider a tracking device working at a frame rate of \\(0.1 Hz\\). This means that each time step of the particle \\(\\Delta t = 100 \\ ms = 0.1 \\ s\\).\n\n\nDiffusion coefficient D\nTypically, microparticles in biological environments diffuse with coefficients of the order of \\(0.01 \\ \\mu m^2/s\\). In order to get a meaningful input value for the diffusion model generators, we need to consider the following:\n\nAs a working definition of \\(D\\), valid also for anomalous diffusion, we consider it as proportional to the variance of the displacements along one dimension at the shortest time lag, i.e. \\(\\sigma_{\\Delta x}^2= 2 D \\Delta t\\).\nGiven the values of pixel size and frame rate, in adimensional units, \\(D\\) is given by: \\(D= 0.01 \\ \\frac{\\mu m^2}{s} \\ \\frac{0.1 s/ \\Delta t }{ 0.01 \\mu m^2/px^2} = 0.1 \\ px^2/\\Delta t\\).\n\n\n\nLocalization precision sigma_x\nTo replicate experimental conditions, we usually consider the presence of uncorrelated noise in the tracking of the particles. This is taken into account by considering a localization precision of \\(\\sigma_{x} = 12 \\ nm = \\frac{12 \\ nm} {100 \\ nm/px} = 0.12 \\ px\\).\n\n\n\nCreating trajectories from phenomenolgical models\nAll models are gathered in a single class, easily accessed using the cell below. We will also import some usefull libraries to showcase some of the properties of the models. After that, we are ready to explore the various diffusion models available.\n\nfrom andi_datasets.models_phenom import models_phenom\n\n# auxiliaries\nimport stochastic\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nstochastic.random.seed(3)\nnp.random.seed(7)\n\n\nModel 1 - Single state diffusion\nThis is the most boring model of all… It simulates a particle with a single diffusive state, with an associated \\(D\\) and \\(\\alpha\\). This kind of trajectories can also be generated with the class models_theory, check its associated tutorial to know more.\nAll models have similar inputs, with some changing to accomodate the particularities of each model. In general, we input the number of trajectories to be generated N, the number of time steps T, and then length of the box acting as environment L. Let us define these and keep them for the rest of the notebook. We will also define here the diffusion coefficient.\n\n# number of time steps per trajectory (frames)\nT = 500\n# number of trajectories\nN = 100\n# Length of box (pixels)\nL = 1.5*128\n# diffusion coefficient (pixels^2 / frame)\nD = 0.1\n\nTo generate the trajectories, we can choose to either provide a fixed value for \\(D\\) and \\(\\alpha\\), or a list containing a mean and a variance. In the second case, the property is sampled from a normal distribution for every trajectory. Let’s generate some trajectories with fixed anomalous exponent alpha = 0.5, and random diffusion coefficients with a fixed mean. Remember that all trajectories follow FBM!\nThe output of all the phenomenological models is always the same: a tuple containing the trajectories and their labels. Let’s focus on the first now:\n\ntrajs_model1, _ = models_phenom().single_state(N = N, \n                                                L = L,\n                                                T = T,\n                                                Ds = [D, 0.1], # Mean and variance\n                                                alphas = 0.5\n                                                )\n\nThe trajectories will have always the same shape: (time, number of trajectories, number of dimensions). In this challenge, we always consider two dimensions.\n\ntrajs_model1.shape\n\n(500, 100, 2)\n\n\nLet’s take a look at some of the trajectories with a built-in function from the library:\n\nfrom andi_datasets.utils_trajectories import plot_trajs\n\nplot_trajs(trajs_model1, L, N, num_to_plot = 4)\n\n\n\n\n\n\nModel 2 - Multiple state diffusion\nNow we consider the motion of a particle that changes between different diffusive state. We are free to choose the number of states. The probability of jumping from one state to another is given by the transition matrix \\(M\\). Let’s consider a two-state diffusion process. It’s transition matrix is\n\\(M = \\begin{pmatrix} P_{11} & P_{12} \\\\ P_{21} & P_{22} \\end{pmatrix} = \\begin{pmatrix} 0.98 & 0.02 \\\\ 0.02 & 0.98 \\end{pmatrix},\\)\nwhere \\(P_{ij}\\) is the probability of changing from state \\(i\\) to state \\(j\\) at every time step. Each diffusive state can have its own diffusion coefficient and anomalous exponent. For instance, we consider a fast state with \\(D_f = 10 D\\) and \\(\\alpha_f = 1.2\\) and a slow state with \\(D_s = 0.1 D\\) and \\(\\alpha_s = 0.7\\). Let’s create some trajectories!\n\ntrajs_model2, labels_model2 = models_phenom().multi_state(N = N, \n                                                          L = L,\n                                                          T = T,\n                                                          alphas = [1.2, 0.7], # Fixed alpha for each state\n                                                          Ds = [[10*D, 0.1], [0.1*D, 0.0]], # Mean and variance of each state\n                                                          M = [[0.98, 0.02], [0.02, 0.98]]\n                                                            )\n\nSee that now we do care about the trajectory labels. They contain very important information: the value of \\(\\alpha\\), \\(D\\) and the state of the particle at every time step.\n\nlabels_model2.shape\n\n(500, 100, 3)\n\n\nFor instance, let’s check the values of the labels for the first particle:\n\nprint(r'alphas:', np.unique(labels_model2[:, 0, 0]), 'D:', np.unique(labels_model2[:, 0, 1]) )\n\nalphas: [0.7 1.2] D: [0.01       0.77367364]\n\n\nThe state of the particle does not correspond to the actual diffusive state. Even though there are two diffusive states in these trajectories, the label is always 2, which corresponds to a free particle.\n\nnp.unique(labels_model2[..., -1])\n\narray([2.])\n\n\nThe different states are:\n- `0`: immobile\n- `1`: confined\n- `2`: free\n- `3`: directed\n\nmodels_phenom().lab_state\n\n['i', 'c', 'f', 'd']\n\n\nNow, let’s plot some trajectories to see their changes over time. In the third row of the next plot, we see the value of \\(\\alpha\\) and \\(D\\) over time with vertical lines marking the changepoints.\n\nplot_trajs(trajs_model2, L, N, labels = labels_model2, plot_labels = True)\n\n\n\n\nW can see, \\(\\alpha\\) and \\(D\\) change over time randomly, following the transition matrix \\(M\\). We can also work with changepoints and physical properties in the different systems with the following function:\n\nfrom andi_datasets.utils_challenge import label_continuous_to_list\n\nchangepoints, alphas, Ds, state_num = label_continuous_to_list(labels_model2[:, 0, :])\n\nprint('changepoints:', changepoints,\n      '\\nalphas:', alphas,\n      '\\nDs:', Ds)\n\nchangepoints: [ 83 199 218 244 348 385 408 500] \nalphas: [1.2 0.7 1.2 0.7 1.2 0.7 1.2 0.7] \nDs: [0.77367364 0.01       0.77367364 0.01       0.77367364 0.01\n 0.77367364 0.01      ]\n\n\nThis results in \\(C\\) changepoints, which create \\(C+1\\) segments, and hence \\(C+1\\) values of \\(\\alpha\\) and \\(D\\).\n\n\nModel 3 - Dimerization\nThe third model considers a set of trajectories moving all in the same environment. They have all the same properties: \\(D\\), \\(\\alpha\\) and radius \\(r\\). If two particles get closer than a distance \\(2r\\), they bind with probability \\(P_b\\) creating a dimer. Similarly, two dimerized particles may unbind with probability \\(P_u\\). This results in two diffusive states. Just as before, we can define the diffusion coefficients and exponents of both states. Let’s see an example:\n\ntrajs_model3, labels_model3 = models_phenom().dimerization(N = N, \n                                                           L = L,\n                                                           T = T,\n                                                           alphas = [1.2, 0.7], # Fixed alpha for each state\n                                                           Ds = [10*D, 0.1*D], # Fixed D for each state\n                                                           r = 1, # Radius of the particles\n                                                           Pb = 1, # Binding probability\n                                                           Pu = 0 # Unbinding probability\n                                                           )\n\nNow, let’s plot the trajectories to see how particles dimerize.\n\nplot_trajs(trajs_model3, L, N, labels = labels_model3, plot_labels = True)\n\n\n\n\nIn the third row of the plot, we can see how the diffusve state changes when the particles dimerize! Since we have set \\(P_u=0\\), once the particles dimerize, they remain bound for the rest of the experiment.\n\n\nModel 4 - Immobile traps\nThe fourth model considers the presence of fixed traps of radius \\(r\\) which completely immobilize the particles. Similar to the dimerization model, the particles have a probability \\(P_b\\) of getting trapped and \\(P_u\\) of getting released whenever they are within the area of the trap. The generator of this trajectories allows us (optional) to set the position of the traps beforehand. Let’s throw them randomly over the box:\n\nnumber_traps = 100\ntraps_positions = np.random.rand(number_traps, 2)*L\n\nNow, let’s put some particles in the system with the usual diffusion coefficient \\(D\\) and an \\(\\alpha = 1.6\\).\n\ntrajs_model4, labels_model4 = models_phenom().immobile_traps(N = N,\n                                                             T = T,                \n                                                             L = L,\n                                                             r = 2, # Radius of the traps\n                                                             Pu = 0.01, # Unbinding probability\n                                                             Pb = 1, # Binding probability\n                                                             Ds = D, # Diffusion coefficients of moving state\n                                                             alphas = 1.6, # Anomalous exponents of moving state\n                                                             Nt = number_traps, # Number of traps\n                                                             traps_pos = traps_positions\n                                                             )\n\nIn this case, besides \\(\\alpha\\) and \\(D\\), the labels also tell us when the particle is trapped with the label 0.\n\nnp.unique(labels_model4[..., -1])\n\narray([0., 2.])\n\n\nLet’s see some trajectory examples.\n\nplot_trajs(trajs_model4, L, N, \n           labels = labels_model4, plot_labels = True,\n           traps_positions = traps_positions)\n\n\n\n\nThe particles superdiffuse until they hit a trap, moment in which they get immobilized. In some cases, the particles may avoid the traps entirely and never be bound! (The size of the dot is not the size of the trap)\n\n\nModel 5 - Confinement\nLast but not least, we have one of the most relevant models of the challenge: the presence of compartments, whose boundaries can prevent particles from escaping them. Note that we consider here the case of osmotic boundaries, which means that particles will always enter the compartment. Once inside, the boundaries have a certain transmittance trans, i.e., the probability of the particle exiting the compartment. Just as the case of immobile traps, we can define (optionally) the compartments’ distribution a priori. Note that we always consider here circular compartments of fixed radius r. We can use the following built-in function to distribute cercles over the environment without overlap. If the algorithm does not manage to place them all, it raises a warning, and only the circles that fit are placed.\n\nnumber_compartments = 50\nradius_compartments = 10\ncompartments_center = models_phenom._distribute_circular_compartments(Nc = number_compartments, \n                                                                      r = radius_compartments,\n                                                                      L = L # size of the environment\n                                                                      )\n\nLet’s check the distribution of the compartments. The environment is quite dense!\n\nfig, ax = plt.subplots(figsize = (4,4))\nfor c in compartments_center:\n    circle = plt.Circle((c[0], c[1]), radius_compartments, facecolor = 'None', edgecolor = 'C1', zorder = 10)\n    ax.add_patch(circle) \nplt.setp(ax, xlim = (0, L), ylim = (0, L));\n\n\n\n\nNow, let’s introduce some diffusive particles in this environment. We will consider a rather extreme case to nicely show the effect of the compartments. Outside the comparments, the particles will move very, very fast, at \\(D_f = 1500D\\). Replicating what we see in many biological scenarios, the particles will move much slower, \\(D_s = 50 D\\), inside the compartments. Let’s keep the same \\(\\alpha = 1\\) for both diffusive states.\nComment on default values: the default values for alpha are always 1 for all methods. Hence, if no exponents are given, all diffusive states are normally diffusing. Most of the arguments of the functions presented in this tutorial have default values similar to the ones used in this notebook. If you want to know more, you can check their source code.\nLet’s consider a transmittance trans = 0.2 for the compartment boundaries:\n\ntrajs_model5, labels_model5 = models_phenom().confinement(N = N,\n                                                          L = L,\n                                                          Ds = [1500*D, 50*D],\n                                                          comp_center = compartments_center,\n                                                          r = radius_compartments,\n                                                          trans = 0.2 # Boundary transmittance\n                                                           )\n\nLet’s see the resulting trajectories, superimposed to the cercles we created just before.\n\nplot_trajs(trajs_model5, L, N, \n           comp_center = compartments_center,\n           r_cercle = radius_compartments,\n           plot_labels = True, labels = labels_model5\n           )\n\n\n\n\nWith these extreme diffusion coefficients, the particles move very fast from one compartment to the other. Once inside, the low transmittance of the boundaries make it such that they stay for some time before they exit!\n\n\n\nCreating datasets with multiple models\nThe library allows to simultaneously generate trajectories from all the previous models, and gather them in a single dataset. To do so, we have created the the class datasets_phenom. Let’s import it and see some of its properties:\n\nfrom andi_datasets.datasets_phenom import datasets_phenom\n\nOne of the first things we can check is the models that we can access with this class. Spoiler: they are the same we just reviewed above!\n\ndatasets_phenom().avail_models_name\n\n['single_state',\n 'multi_state',\n 'immobile_traps',\n 'dimerization',\n 'confinement']\n\n\nAs we have seen previously, each model has its own parameters. Most of them have some sensible default values, so we can generally skip them when generating trajectories. Nonetheless, we can always easily see what are the parameters for a given model:\n\nmodel = 'confinement'\ndatasets_phenom()._get_inputs_models(model)\n\n['N',\n 'T',\n 'L',\n 'Ds',\n 'alphas',\n 'gamma_d',\n 'epsilon_a',\n 'r',\n 'comp_center',\n 'Nc',\n 'trans',\n 'deltaT']\n\n\nOf course, we encourage you to also check the documentation and inspect all the details about the models.\nIn order to generate trajectories, we first need to set: 1. the models we want to generate trajectories from 2. the properties of each of the models.\nTo do so, we need to create a dictionary that stores all the needed information for each model. Hence, for multiple models, we will need to input a list of dictionaries. For example, let’s create a dataset with trajectories from the dimerization (model 3) and confinement (model 5) models. We will make use of the default values of the previous models, and only input the binding/unbinding probability for the former and the transmittance for the latter.\n\ndict_model3 = {'model': 'dimerization', \n               'L': L,\n               'Pu': 0.1, 'Pb': 1}\n\ndict_model5 = {'model': 'confinement', \n               'L': L,\n               'trans': 0.2}\n\ndict_all = [dict_model3, dict_model5]\n\ntrajs, labels = datasets_phenom().create_dataset(N_model = N, # number of trajectories per model\n                                                 T = T,\n                                                 dics = dict_all,\n                                                )\n\nThe trajectories arising from the previous function are ordered following the order of the input list of dictionaries. We requested N_model = N = 100 trajectories per model, hence the dataset will have a total of 200 trajectories. The first 100 will come from the dimerization model and the last 100 from the confinement model.\n\nfrom andi_datasets.utils_trajectories import plot_trajs\n\nplot_trajs(trajs, L, N, \n           plot_labels = True, labels = labels\n           )\n\n\n\n\n\nSaving and loading datasets\nJust as with the datasets_theory class (check here the details), you can save and load datasets, so that you avoid creating trajectories every time you need them. The same function presented above has such options:\n\ntrajs, labels = datasets_phenom().create_dataset(N_model = N, # number of trajectories per model\n                                                 T = T,\n                                                 dics = dict_all,\n                                                 save = True, path = 'datasets_folder/'\n                                                )\n\nSaving creates two files per model:\n\na .csv file, containing the details of the dataset generated. This file stores the values of the parameters of the saved dataset. This file helps organizing all the saved datasets and is used later to know, given some parameters, from where to load the trajectories.\na .npy file, containing the trajectories and labels of the generated trajectories. Each file will have an integer number at end, which is used, together with the csv file, in order to know the properties of the dataset.\n\nAfter saving, we can just load the dataset with the same function:\n\ntrajs, labels = datasets_phenom().create_dataset(N_model = N, # number of trajectories per model\n                                                 T = T,\n                                                 dics = dict_all,\n                                                 load = True, path = 'datasets_folder/'\n                                                )"
  },
  {
    "objectID": "tutorials/challenge_one_datasets.html",
    "href": "tutorials/challenge_one_datasets.html",
    "title": "1. Explore the models",
    "section": "",
    "text": "How to use this notebook:\nIf you are interested in quick overview on how to create datasets for the ANDI challenge, please go to Section 1. For further details on the creation of datasets of theoretical trajectories, please go to Section 2. To learn how to access the various theoretical diffusion models contained in the andi_datasets, please go to Section 3."
  },
  {
    "objectID": "tutorials/challenge_one_datasets.html#creating-the-andi-1-challenge-dataset",
    "href": "tutorials/challenge_one_datasets.html#creating-the-andi-1-challenge-dataset",
    "title": "1. Explore the models",
    "section": "Creating the ANDI 1 challenge dataset",
    "text": "Creating the ANDI 1 challenge dataset\nBefore starting, make sure that you know the details of the competition. You can learn about every in the associated webpage. A quick overview: there are three tasks: 1) inference of the anomalous diffusion exponent, 2) classification of the diffusion model and 3) segmentation of trajectories. For each task, there are three subtasks, one per dimension. To create a dataset for every task and dimension, each e.g. of \\(N=10\\) trajectories, you just need to run\n\nfrom andi_datasets.datasets_challenge import challenge_theory_dataset\n\n\nX1, Y1, X2, Y2, X3, Y3 = challenge_theory_dataset(N = 10)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\nGenerating dataset for dimension 1.\nGenerating dataset for dimension 2.\nGenerating dataset for dimension 3.\n\n\nX1, X2 and X3 correspond to the trajectories of Task 1, 2 and 3, respectively, while Y1, Y2 and Y_3 correspond to the labels. Each of these list contains three list, each one containing the trajectories for each dimension, in ascending order. For example, X2[2] contains the trajectories for Task 2 for dimension 3. If you only want to generate trajectories for a specific Task and dimensions, you can specify it as\n\nX1, Y1, X2, Y2, X3, Y3 = challenge_theory_dataset(N = 10, tasks = 1, dimensions = 3)\n\nCreating a dataset for task(s) 1 and dimension(s) 3.\nGenerating dataset for dimension 3.\n\n\nIn this case all the lists but the ones specified will be empty. For example:\n\nprint('Task 3, dimension 1 is empty: len(X3[0]) = '+ str(len(X3[0])))\nprint('But Task 1, dimension 3 has our 10 trajectories: len(X1[2]) = '+str(len(X1[2])))\n\nTask 3, dimension 1 is empty: len(X3[0]) = 0\nBut Task 1, dimension 3 has our 10 trajectories: len(X1[2]) = 10\n\n\nTo avoiding having to create a dataset every time we want to use it, you can use the option save_dataset, which will save a file for every dataset of trajectories (named task1.txt, task2.txt and task3.txt) and their corresponding labels (named ref1.txt, ref2.txt and ref3.txt). The first column of each file corresponds to the dimension of the trajectory.\n\nX1, Y1, X2, Y2, X3, Y3 = challenge_theory_dataset(N = 10, save_dataset = True)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\nGenerating dataset for dimension 1.\nGenerating dataset for dimension 2.\nGenerating dataset for dimension 3.\n\n\nNow that we saved the dataset, we can load it with the option load_dataset:\n\nX1, Y1, X2, Y2, X3, Y3 = challenge_theory_dataset(N = 10, load_dataset = True)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\n\n\nTake into account that if you saved \\(N=10\\) trajectories, you will only be able to load this 10, even if you ask for \\(N=20\\).\nBy default, the trajectories in this dataset will have a maximum length of 1000 steps and a minimum length of 10. To change that, you can use the options max_T and min_T. Take into account that you will need trajectories of 200 steps for task 3. Shorter trajectories will raise an error.\n\nX1, Y1, X2, Y2, X3, Y3 = challenge_theory_dataset(N = 10, min_T = 15, max_T = 200)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\nGenerating dataset for dimension 1.\nGenerating dataset for dimension 2.\nGenerating dataset for dimension 3.\n\n\nThe rests of the optional inputs of the function (load_trajectories, save_trajectories, N_save and T_save) refer to the saving/loading of the trajectories used to generate the datasets, in numpy format. See Section 2 for more details on this."
  },
  {
    "objectID": "tutorials/challenge_one_datasets.html#creating-trajectories-from-theoretical-models",
    "href": "tutorials/challenge_one_datasets.html#creating-trajectories-from-theoretical-models",
    "title": "1. Explore the models",
    "section": "Creating trajectories from theoretical models",
    "text": "Creating trajectories from theoretical models\nFor this, we will the class andi_datasets.datasets_theory class. First, lets import some auxiliary packages:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nFirst, let’s define the class. One thing we can do is to access the available diffusion models:\n\nfrom andi_datasets.datasets_theory import datasets_theory\n\nAD = datasets_theory()\nAD.avail_models_name\n\n['attm', 'ctrw', 'fbm', 'lw', 'sbm']\n\n\nThe label asigned to each model is given by its position in the previous list. Now, we will generate a dataset containing \\(N=2\\) trajectories per model and exponent. We will consider anomalous exponents \\(\\alpha = [0.7, 0.9]\\) and lenghts \\(T=5\\) from the models attm and fbm.\n\ndataset = AD.create_dataset(T = 4, N_models = 2, exponents = [0.7, 0.9], models = [0, 2])\n\nprint(np.round(dataset,2))\n\n[[ 0.    0.7   0.    0.4  -0.15  0.04]\n [ 0.    0.7   0.   -0.49 -1.02 -0.87]\n [ 0.    0.9   0.    0.04  0.85  0.64]\n [ 0.    0.9   0.    1.26  2.33  2.38]\n [ 2.    0.7   0.    0.08  2.09  1.03]\n [ 2.    0.7   0.    0.18  0.61  1.12]\n [ 2.    0.9   0.    0.12  1.12  0.33]\n [ 2.    0.9   0.    0.08 -0.59 -0.45]]\n\n\nEach row corresponds to a trajectory. The first column is the model label, while the second column labels the exponent. The rest of the row is the trajectory. The default dimension for the trajectories is one. We will see later how to create trajectories with more dimensions.\nTo create datasets of trajectories with higher dimensions, you can use the input dimensions. The trajectories will be outputed as a single row of the dataset matrix. This means that if you create a dataset of trajectories of length \\(T = 10\\) and dimensions 2, the number of columns of the dataset will be 2 (the labels) + \\(2\\cdot 10\\) (the trajectory). Here is an example:\n\ndataset = AD.create_dataset(T = 10, N_models = 1, exponents = [0.7], models = [2], dimension = 2)\nprint(np.round(dataset[0], 2))\n\n[ 2.    0.7   0.   -1.14 -1.17 -1.36 -1.62 -0.88 -0.85 -1.05 -1.23 -0.57\n  0.   -0.37 -0.33 -0.71 -0.25  0.39  0.7   0.05 -0.18 -0.24]\n\n\n\nplt.plot(dataset[0,2:12], dataset[0, 12:], '-')\nplt.scatter(dataset[0,2:12], dataset[0,12:], c=np.arange(10), s = 250)\nplt.colorbar().set_label('Time'); plt.xlabel('X'); plt.ylabel('Y');\n\n\n\n\n\nSaving and loading datasets\ndatasets_theory allows also to save and load datasets, avoiding the need of generate new trajectories each time we want to create a dataset. There exist two optional inputs to the function create_datasets: save_trajectories and load_trajectories, which if True, save and load datasets, respectively. A dataset for each exponent and model considered are saved in a .h5py file whose name is the model considered. Each file contains datasets for each exponent.\nIn these cases, two important variables take also into play, T_save and N_save. These set the lenght and number of trajectories to save for each exponent and model. They are set as default to T_save\\(=10^3\\) and N_save\\(=10^4\\) as this allows to create any other combinantion of dataset. See that in the ANDI challenge we will never consider trajectories longer than \\(T=10^3\\).\nThe name of the datasets inside the .h5py file is then (exponent)_(T_save)_(N_save).\nUsing the default values for this two variables, let’s save datasets for the previous examples:\n\ndataset = AD.create_dataset(T = 4, N_models = 2, exponents = [0.7, 0.9], models = [0, 2], \n                              save_trajectories = True, path = 'datasets/')\n\nprint(np.round(dataset,2))\n\n100%|█████████████████████████████████████████| 1000/1000 [00:02<00:00, 365.01it/s, exponent=0.7, model=attm, saving=1]\n100%|█████████████████████████████████████████| 1000/1000 [00:06<00:00, 156.58it/s, exponent=0.9, model=attm, saving=1]\n100%|█████████████████████████████████████████| 1000/1000 [00:00<00:00, 2349.36it/s, exponent=0.7, model=fbm, saving=1]\n100%|█████████████████████████████████████████| 1000/1000 [00:00<00:00, 2394.90it/s, exponent=0.9, model=fbm, saving=1]\n\n\n[[ 0.    0.7   0.   -0.16 -1.69 -0.53]\n [ 0.    0.7   0.   -0.45 -0.9  -0.39]\n [ 0.    0.9   0.    0.08  0.69  0.15]\n [ 0.    0.9   0.   -1.65  0.29  2.47]\n [ 2.    0.7   0.   -0.02  0.11  0.15]\n [ 2.    0.7   0.    0.01  0.02  0.05]\n [ 2.    0.9   0.    0.   -0.04 -0.06]\n [ 2.    0.9   0.   -0.03  0.05  0.03]]\n\n\n\n\n\nIf the datasets were already saved, we can load them instead:\n\ndataset = AD.create_dataset(T = 4, N_models = 2, exponents = [0.7, 0.9], models = [0, 2], \n                              load_trajectories = True, path = 'datasets/')\n\nprint(np.round(dataset,2))\n\n[[ 0.    0.7   0.    1.02  0.9   0.64]\n [ 0.    0.7   0.   -0.03  0.07 -0.96]\n [ 0.    0.9   0.   -0.68  0.31  1.78]\n [ 0.    0.9   0.   -0.45  1.02  2.24]\n [ 2.    0.7   0.   -0.02 -0.05  0.06]\n [ 2.    0.7   0.   -0.05 -0.03 -0.02]\n [ 2.    0.9   0.   -0.03  0.06  0.07]\n [ 2.    0.9   0.    0.03  0.02  0.02]]\n\n\n\n\nCreating noisy datasets\nThere are two ways of adding noise to the trajectories: either by varying their diffusion coefficient or by adding noise to each of the trahectory’s points. You can do it with the functions create_noisy_diffusion_dataset or create_noisy_localization_dataset, respectively. These functions allow to either create a new dataset to which we will add the noise or give as input the dataset. We will use now the dataset we created in previous cells.\nNoisy diffusion coefficients The function create_noisy_diffusion_dataset changes the diffusion coefficient of a given trajectory. This function works similarly to create_dataset, but has as optional input diffusion_coefficients. You can use this variable to change the diffusion coefficient of the input trajectories at your please. If you don’t give an input to it, they will be drawn randomly from a Gaussian distribution with variance 1. See that, as \\(D\\) may be negative, the trajectories can be reversed.\n\ndataset_noise_D = AD.create_noisy_diffusion_dataset(dataset.copy(), T = 4)\n\n\nplt.plot(dataset[1,2:],'o-', label = 'Original trajectory')\nplt.plot(dataset_noise_D[1,2:],'o-', label = r'Trajectory with changed $D$')\nplt.xlabel('Time'); plt.ylabel('Position')\nplt.legend();\n\n\n\n\nLocalization noise The class also allows to create noisy trajectories via create_noisy_localization_dataset. This functions works as create_dataset but has three extra optional input parameters: noise_func, sigma and mu. If noise_func is False, the function generates a dataset with Gaussian noise with variance sigma and mean mu. Keep in mind that the function will never save noisy trajectories. The saved/loaded trajectories will never have noise, which will be added a posteriori. Here is an example with Gaussian noise:\n\n# The default value of noise_func is False, so we don't need to input it. \n# Default value of sigma is 1 and mu is 0.\ndataset_noisy = AD.create_noisy_localization_dataset(dataset.copy(), T = 4)\n\nprint(np.round(dataset_noisy,2))\n\n[[ 0.    0.7   1.22  2.04  0.48 -0.89]\n [ 0.    0.7   0.59  1.37 -0.36 -0.77]\n [ 0.    0.9  -0.7  -1.18  0.87  0.94]\n [ 0.    0.9   0.17  0.32  1.95  1.64]\n [ 2.    0.7  -0.31 -1.24  0.17  0.76]\n [ 2.    0.7  -1.35  1.16  2.18 -1.73]\n [ 2.    0.9   2.14  0.    1.32 -0.69]\n [ 2.    0.9  -0.64  0.39 -0.22  0.41]]\n\n\n\nplt.plot(dataset[1,2:],'o-', label = 'Original trajectory')\nplt.plot(dataset_noisy[1,2:],'o-', label = 'Trajectory + Gaussian noise')\nplt.legend();\n\n\n\n\nYou can also define your own noise function. The noise created by this functions will be added to each of the trajectories of the dataset. The input must be the trajectories and the output has to be of equal size of the trajectories. The output of this matrix will be added to the trajectories.\n\n# Uniformly distributed noise\ndef uniform_noise(trajs):\n    N, M = trajs.shape\n    return np.random.rand(N, M)*(2*np.random.randint(2, size = (N, M))-1) \n\ndataset_uniform = AD.create_noisy_localization_dataset(dataset.copy(), T = 4, \n                                                       noise_func=uniform_noise)\n\nprint(np.round(dataset_uniform,2))\n\n[[ 0.    0.7  -0.37  0.53  1.29  0.97]\n [ 0.    0.7   0.72 -0.51  0.08 -0.64]\n [ 0.    0.9  -0.86 -0.91 -0.22  2.52]\n [ 0.    0.9  -0.94 -0.38  0.92  3.15]\n [ 2.    0.7  -0.76  0.49  0.61 -0.12]\n [ 2.    0.7   0.91  0.42  0.62 -0.75]\n [ 2.    0.9   0.77 -0.98 -0.51  0.32]\n [ 2.    0.9   0.14  0.35  0.85 -0.69]]\n\n\n\nplt.plot(dataset[1,2:],'o-', label = 'Original trajectory')\nplt.plot(dataset_noisy[1,2:],'o-', label = 'Trajectory + Gaussian noise')\nplt.plot(dataset_uniform[1,2:],'o-', label = 'Trajectory + uniform noise')\nplt.legend();\n\n\n\n\n\n\nCreating segmented datasets\nThe class datasets_theory also allows to create trajectories which are made of two different diffusion models or anomalous exponent, at a given changing point. Such function has the name create_segmented_dataset. Following the ANDI challenge guidelines, the output trajectories will have length 200. However, you can choose an arbitrary length with the variable final_length. The input datasets must have at least the same size as final_length. There is also the option to randomly shuffle the input datasets from which the segmented output dataset will be generate, throught the variable random_shuffle.\n\n# First we define two datasets. Let's take same anomalous exponent but various models\ndataset1 = AD.create_dataset(T = 200, N_models = 2, exponents = [0.7], models = [0, 1, 2, 4])\ndataset2 = AD.create_dataset(T = 200, N_models = 2, exponents = [0.7], models = [0, 1, 2, 4])\n# Now we give them to the desired function\nseg = AD.create_segmented_dataset(dataset1, dataset2)\n\nThe first 5 elements of every row of the variable seg label the trajectory. First point is the changing time, second and third, and fourth and fifthe are the labels of the model and exponent of the input trajectories, respectevely.\n\nseg[:, :5]\n\narray([[145. ,   0. ,   0.7,   0. ,   0.7],\n       [ 91. ,   0. ,   0.7,   0. ,   0.7],\n       [ 16. ,   1. ,   0.7,   1. ,   0.7],\n       [ 88. ,   1. ,   0.7,   1. ,   0.7],\n       [100. ,   2. ,   0.7,   2. ,   0.7],\n       [ 82. ,   2. ,   0.7,   2. ,   0.7],\n       [175. ,   4. ,   0.7,   4. ,   0.7],\n       [189. ,   4. ,   0.7,   4. ,   0.7]])\n\n\nAs we did not shuffle data and dataset1 and dataset2 have the trajectories ordered by model, there are no mixes between different models. This can be done thanks to the random_shuffle variable:\n\nseg = AD.create_segmented_dataset(dataset1, dataset2, random_shuffle = True)\nseg[:, :5]\n\narray([[ 33. ,   0. ,   0.7,   2. ,   0.7],\n       [ 86. ,   2. ,   0.7,   1. ,   0.7],\n       [ 74. ,   0. ,   0.7,   0. ,   0.7],\n       [132. ,   1. ,   0.7,   2. ,   0.7],\n       [140. ,   1. ,   0.7,   4. ,   0.7],\n       [ 75. ,   4. ,   0.7,   0. ,   0.7],\n       [ 12. ,   2. ,   0.7,   4. ,   0.7],\n       [163. ,   4. ,   0.7,   1. ,   0.7]])\n\n\n\nplt.plot(seg[0,5:], label = 'Model 1 = '+AD.avail_models_name[int(seg[0, 1])]+', Model 2 = '+AD.avail_models_name[int(seg[0, 3])])\nplt.axvline(seg[0,0], c = 'C1', ls = '--', label = 'Changing point')\nplt.legend();"
  },
  {
    "objectID": "tutorials/challenge_one_datasets.html#accesing-the-diffusion-models-from-models_theory",
    "href": "tutorials/challenge_one_datasets.html#accesing-the-diffusion-models-from-models_theory",
    "title": "1. Explore the models",
    "section": "Accesing the diffusion models from models_theory",
    "text": "Accesing the diffusion models from models_theory\nAll the theoretical diffusion models available in andi are collected in the class models_theory(). We can access them with a simple import :\n\nfrom andi_datasets.models_theory import models_theory\n# We will also need the following libraries\nimport inspect\n\nThere exist three subclasses, one for each dimensions. To see the available models for each dimension, you can use the following:\n\nfor dimensions in range(3):\n    if dimensions == 0:\n        models = models_theory()._oneD()\n    elif dimensions == 1:\n        models = models_theory()._twoD()\n    elif dimensions == 2:\n        models = models_theory()._threeD()\n        \n    available_models = inspect.getmembers(models, inspect.ismethod)\n        \n    print('\\nThe availailabe models for dimension '+str(dimensions+1)+' are:')\n    [print('- '+x[0]) for x in available_models]\n\n\nThe availailabe models for dimension 1 are:\n- attm\n- ctrw\n- fbm\n- lw\n- sbm\n\nThe availailabe models for dimension 2 are:\n- attm\n- ctrw\n- fbm\n- lw\n- sbm\n\nThe availailabe models for dimension 3 are:\n- attm\n- ctrw\n- fbm\n- lw\n- sbm\n\n\nOne dimensional trajectories\nFor this example, we will create a trajectory of a 1D continuous time random walk (ctrw). All the model functions must have as inputs the length of the trajectories and the anomalous exponent \\(\\alpha\\):\n\noneD = models_theory()._oneD()\n\ntraj = oneD.ctrw(T = 100, alpha = 0.8)\nplt.plot(traj)\n\n\n\n\nSome diffusion models are constructed from a collection of positions at different sampling times. This means that their output array does not have \\(T\\) points, but rather number \\(m\\) of sampled posisition. In order to acces such kind of information, some model generators have the optional parameter regulare_time, which if False, makes the function to return an array with two rows and \\(m\\) columns. The first row are the sampling times and the second are the positions. CTRW is an example of these models:\n\ntraj_nonR = oneD.ctrw(T = 100, alpha = 0.8, regular_time = False)\ntraj_nonR.shape\n\n(2, 13)\n\n\nIf regulare_time = True (default value), the trajectory is fed to the function regularize from utils_trajectories, which transforms the trajectory to regular sampling times. Below is an example with the previously defined trajectory. The offset is due to the need of rounding the times coming from the variable traj_nonR[1].\n\nfrom andi_datasets.utils_trajectories import regularize\n\ntraj_R = regularize(positions = traj_nonR[1,:], times = traj_nonR[0, :], T = 100)\n\nfig, ax = plt.subplots()\nax.scatter(traj_nonR[0,:], traj_nonR[1, :], c = 'C0', label = 'Irregular sampling')\nax.plot(traj_R, c = 'C1', label = 'Regularized sampling')\nax.set_xlabel('Positions'); ax.set_ylabel('Time'); ax.axvline(100,ls = '--', c = 'C2')\nax.legend();\n\n\n\n\nTwo dimensional trajectories\nWe will proceed with an example of a 2D trajectory coming from the ATTM model. 2D trajectories are output as an array of length \\(2\\times T\\).\n\nT = 20\ntraj = models_theory()._twoD().attm(T = T, alpha = 0.8)\nprint(traj.shape)\n\n(40,)\n\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize = (6,5))\nax = fig.gca(projection='3d')\nax.plot(traj[:T], traj[T:], np.arange(T))\nscat = ax.scatter3D(traj[:T], traj[T:], np.arange(T), c=np.arange(T), s = 250);\nax.set_xlabel('Position X')\nax.set_ylabel('Position Y')\nax.set_zlabel('Time')\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\nplt.tight_layout();\n\n\n\n\nEnsemble averages of diffusion_models trajectories\nIn the ANDI challenge, one of the parameters to be extracted is the anomalous exponent \\(\\alpha\\). This exponents corresponds to the proportionality relation between the mean squared displacement averaged over \\(N\\) trajectories (eMSD) and time: \\(\\left< x^2(t) \\right>\\sim t^\\alpha\\). Here are a series of examples of eMSD calculated for various models:\n(If your viewer does not allow you to see the following images, you can see them in the folder figures/)\nSubdiffusive trajectories:\n\nSuperdiffusive trajectories:"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html",
    "href": "tutorials/creating_videos_phenom.html",
    "title": "2. Create videos",
    "section": "",
    "text": "This tutorial demonstrates how to generate fluorescence microscopy videos of the AnDi trajectories using the function transform_to_video."
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#setup",
    "href": "tutorials/creating_videos_phenom.html#setup",
    "title": "2. Create videos",
    "section": "1. Setup",
    "text": "1. Setup\nImporting the dependencies needed to run this tutorial.\n\nimport numpy as np\nimport random\nimport imageio\nimport matplotlib.pyplot as plt\nimport deeptrack as dt\nfrom andi_datasets.models_phenom import models_phenom\n\n/opt/miniconda3/envs/handi/lib/python3.10/site-packages/deeptrack/backend/_config.py:11: UserWarning: cupy not installed. GPU-accelerated simulations will not be possible\n  warnings.warn(\n/opt/miniconda3/envs/handi/lib/python3.10/site-packages/deeptrack/backend/_config.py:25: UserWarning: cupy not installed, CPU acceleration not enabled\n  warnings.warn(\"cupy not installed, CPU acceleration not enabled\")\n2023-06-22 13:47:17.661504: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#defining-example-diffusion-model",
    "href": "tutorials/creating_videos_phenom.html#defining-example-diffusion-model",
    "title": "2. Create videos",
    "section": "2. Defining example diffusion model",
    "text": "2. Defining example diffusion model\nAs an example, We generate the trajectories of dimerization model from models_phenom.\n\n2.1. Dimerization\nDefining simulation parameters.\n\nT = 100 # number of time steps (frames)\nN = 50 # number of particles (trajectories)\nL = 1.5 * 128 # length of the box (pixels) -> extending fov by 1.5 times\nD = 0.1 # diffusion coefficient (pixels^2/frame)\n\n\ntrajs, labels = models_phenom().dimerization(\n    N=N,\n    L=L,\n    T=T,\n    alphas=[1.2, 0.7],\n    Ds=[10 * D, 0.1 * D],\n    r=1,  # radius of the particles\n    Pb=1,  # binding probability\n    Pu=0,  # unbinding probability\n)\n\nPlotting trajectories.\n\nfor traj in np.moveaxis(trajs, 0, 1):\n    plt.plot(traj[:,0], traj[:,1])\nplt.show()"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#generating-videos",
    "href": "tutorials/creating_videos_phenom.html#generating-videos",
    "title": "2. Create videos",
    "section": "3. Generating videos",
    "text": "3. Generating videos\n\n3.1. Import functions\nFor generating videos we import transform_to_video function from andi_datasets package. Additionally we import play_video function to display the videos within the jupyter notebook.\n\nfrom andi_datasets.utils_videos import transform_to_video, play_video\n\n\n\n3.2. Usage\nThe trajectory data generated can be directly passed through transform_to_video to generate fluorescence videos of the particles.\n\n3.2.1. Generating a sample video\n\nvideo = transform_to_video(\n    trajs,\n)\n\n\nplay_video(video)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nBy default, transform_to_video function generates video like above. However, the features of the video can be easily controlled with the help of the dictionaries: particle_props, optics_props, and background_props. For detailed information about the dictionaries and the inputs they can take, please refer to the documentation of transform_to_video.\nWe will some example use cases of dictionaries below.\n\n\n3.2.2. Controlling the noise\nThe noise in the videos can be controlled by adjusting the particle intensities in particle_props, and background intensity in background_props. We generate two videos in the following cell with low noise and high noise by controlling the particle intensity, for a fixed background intensity.\n\n# low noise video\nlow_noise_video = transform_to_video(\n    trajs,\n    particle_props={\n        \"particle_intensity\": [1000, 0]\n    },\n    background_props={\n        \"background_mean\": 100\n    },\n)\n\n# high noise video\nhigh_noise_video = transform_to_video(\n    trajs,\n    particle_props={\n        \"particle_intensity\": [300, 0]\n    },\n    background_props={\n        \"background_mean\": 100\n    },\n)\n\n\nfig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\nax0.imshow(low_noise_video[0], cmap=\"gray\")\nax0.set_title(\"Low noise\")\nax1.imshow(high_noise_video[0], cmap=\"gray\")\nax1.set_title(\"High noise\")\nplt.show()\n\n\n\n\n\n\n3.2.3. Controlling the output region\nIf you notice, the size of the video is always restricted to 128 x 128 px by default, while the trajectories are spread out in a larger area as per our definition of the length of the box above (Defined by simulation parameter, L). In this case, L was defined to be 1.5 * 128.\nThe output region of the video can be controlled in the optics_props. In the following cell, we generate the same low noise video as above but with a different region of interest (ROI). We shift the origin from [0, 0] to [30, 30], while maintaining the width and height to 128 px as before.\n\nvideo_ROI = transform_to_video(\n    trajs,\n    particle_props={\n        \"particle_intensity\": [1000, 0]\n    },\n    background_props={\n        \"background_mean\": 100\n    },\n    optics_props={\n        \"output_region\" : [30, 30, 30 + 128, 30 + 128] # [x, y, x + width, y + height]\n    }\n)\n\n\nfig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\nax0.imshow(low_noise_video[0], cmap=\"gray\")\nax0.set_title(\"Origin set to [0, 0]\")\nax1.imshow(video_ROI[0], cmap=\"gray\")\nax1.set_title(\"Origin set to [30, 30]\")\nplt.show()"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#generating-particle-masks",
    "href": "tutorials/creating_videos_phenom.html#generating-particle-masks",
    "title": "2. Create videos",
    "section": "4. Generating particle masks",
    "text": "4. Generating particle masks\nThe transform_to_video function can generate the particle masks along with the videos.\n\n4.1. All particle masks\nBy setting the parameter with_masks = True, one can generate videos and masks simultaneously. The pixel values in the masks indicate the particle numbers.\n\nvideo, masks = transform_to_video(\n    trajs,\n    with_masks=True\n)\n\n\nfig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\nax0.imshow(video[0], cmap=\"gray\")\nax0.set_title(\"Frame\")\nax1.imshow(masks[0], cmap=\"gray\")\nax1.set_title(\"Mask\")\nplt.show()\n\n\n\n\n\n\n4.2. VIP particle masks\nBy giving a list of particle numbers to get_vip_particles, one can restrict the masks to contain the only the vip particles.\n\nvip_particles = random.sample(range(10, 50), 10)\nvideo, masks = transform_to_video(\n    trajs,\n    optics_props= {\n        \"output_region\": [0, 0, 200, 200]\n    },\n    get_vip_particles=vip_particles,\n    with_masks=True\n)\n\n\nfig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\nax0.imshow(video[0], cmap=\"gray\")\nax0.set_title(\"Frame\")\nax1.imshow(masks[0], cmap=\"gray\")\nax1.set_title(\"VIP particle masks\")\nplt.show()\n\n\n\n\n\n\n4.3. Special case\nIf get_vip_particles is a non-empty list, and with_masks is set to False (Default), then the output will still be a video, However, the first frame of the video now contains the masks of vip particles in the first frame.\n\nvip_particles = random.sample(range(10, 50), 10)\nvideo = transform_to_video(\n    trajs,\n    optics_props= {\n        \"output_region\": [0, 0, 200, 200]\n    },\n    get_vip_particles=vip_particles,\n    with_masks=False # Default\n)\n\n\nfig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\nax0.imshow(video[0], cmap=\"gray\")\nax0.set_title(\"VIP particle masks (Frame 1)\")\nax1.imshow(video[1], cmap=\"gray\")\nax1.set_title(\"Regular video frame (Frame 2)\")\nplt.show()"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#saving-the-videos",
    "href": "tutorials/creating_videos_phenom.html#saving-the-videos",
    "title": "2. Create videos",
    "section": "5. Saving the videos",
    "text": "5. Saving the videos\nVideos can be saved by setting save_video to True, and providing a path. Alternatively, the videos can be directly saved as numpy arrays using np.save(...).\n\nvideo = transform_to_video(\n    trajs,\n    save_video=True,\n    path=\"../Test_video.tiff\"\n)"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#generating-videos-with-motion-blur",
    "href": "tutorials/creating_videos_phenom.html#generating-videos-with-motion-blur",
    "title": "2. Create videos",
    "section": "6. Generating videos with motion blur",
    "text": "6. Generating videos with motion blur\nWe can generate videos with motion blur by creating a instance of motion_blur class and passing it to transform_to_video function as a parameter.\n\nfrom andi_datasets.utils_trajectories import motion_blur\n\nGenerate oversampled trajectories\n\noutput_length = 50\noversamp_factor = 10\nexposure_time = 0.2\n\nT = output_length * oversamp_factor # number of time steps (frames)\nN = 50 # number of particles (trajectories)\nL = 1* 128 # length of the box (pixels) -> exteneding fov by 1.5 times\nD = 0.1 # diffusion coefficient (pixels^2/frame)\n\n\ntrajs_test, labels = models_phenom().dimerization(\n    N=N,\n    L=L,\n    T=T,\n    alphas=[1.2, 0.7],\n    Ds=[1 * D, 0.1 * D],\n    r=1,  # radius of the particles\n    Pb=1,  # binding probability\n    Pu=0,  # unbinding probability\n)\n\nprint(trajs_test.shape)\n\n(500, 50, 2)\n\n\nInitiate motion blur class with defined parameters\n\nMB = motion_blur(output_length = output_length, oversamp_factor = oversamp_factor, exposure_time = exposure_time)\n\nPass the trajectory data and motion blur instance to transform_to_video function\n\nvideo = transform_to_video(\n    trajs_test,\n    motion_blur_generator=MB,\n)\n\n\nplay_video(video)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "figures/figure_helper.html",
    "href": "figures/figure_helper.html",
    "title": "ANDI",
    "section": "",
    "text": "from andi_datasets.datasets_theory import datasets_theory\nfrom andi_datasets.utils_trajectories import normalize\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nAD = datasets_theory()\n\n\ntrajs = AD.create_dataset(exponents = 1, models=np.arange(5), N_models = 1, T = 500, dimension=2)[:,2:]\n# trajs = trajs.reshape(5, 500, 2)\n\n\ntrajs.shape\n\n(5, 1000)\n\n\n\nimport matplotlib.colors as clr\ncmap = clr.LinearSegmentedColormap.from_list('custom blue', ['#2790DB','#DB403D'], N=256)\ncolors = cmap(np.linspace(0,1, trajs.shape[0]))\n\nfig, axs = plt.subplots(3,3)\nfor t, c, ax in zip(trajs, colors, axs.flatten()):\n\n    ax.plot(normalize(t[:500]), normalize(t[500:]), c = c)\n    ax.set_axis_off()\nfig.savefig('theory_models.svg')\n\n\n\n\n\ntrajs = AD.create_dataset(exponents = 0.2, models=1, N_models = 10000, T = 500)[:,2:]\n\n\nmsd = (trajs**2).mean(axis = 0)\n\n\nplt.loglog(msd)\nplt.savefig('msd.svg')\n\n\n\n\n\nL = 200; T = 100\nNs = [20,10,10]\nalphas = [1,1.5]\nD = 1   \n\ntrajs, labels = models_phenom().multi_state(N = 500, L = L, T = T)\n\n\nfov_origin = [10,10]; fov_length = L*0.4\ntrajs_fov, labels_fov = inside_fov_dataset(trajs, labels, fov_origin, fov_length)\n\nfov_origin2 = [105,90]; fov_length = L*0.4\ntrajs_fov2, labels_fov = inside_fov_dataset(trajs, labels, fov_origin2, fov_length)\n\n\nfig, ax = plt.subplots(figsize = (7,7))\ncmap = clr.LinearSegmentedColormap.from_list('custom blue', ['#2790DB','#DB403D'], N=256)\ncolors = cmap(np.linspace(0, 1, len(trajs_fov)))\n\nfor idx, og_traj in enumerate(trajs[:, :, :].transpose(1,0,2)):\n    ax.plot(og_traj[:, 0], og_traj[:, 1], c = 'k', alpha = 0.1, lw = 0.8)\n\nfor t, c in zip(trajs_fov, colors[::-1, :]):\n    ax.plot(t[0], t[1], c= c)    \n    \ncolors2 = cmap(np.linspace(0, 1, len(trajs_fov2)))\nfor t, c in zip(trajs_fov2, colors2[::-1, :]):\n    ax.plot(t[0], t[1], c= c)\n\n# FOV\nfov_min_x, fov_min_y = fov_origin\nfov_max_x, fov_max_y = np.array(fov_origin)+fov_length\n# currentAxis = ax.gca()\nax.add_patch(Rectangle((fov_min_x, fov_min_y), fov_length, fov_length, fill=None, alpha=1, lw = 2, label = 'FOV', zorder = 20))\n\nfov_min_x, fov_min_y = fov_origin2\nfov_max_x, fov_max_y = np.array(fov_origin2)+fov_length\n# currentAxis = ax.gca()\nax.add_patch(Rectangle((fov_min_x, fov_min_y), fov_length, fov_length, fill=None, alpha=1, lw = 2, label = 'FOV', zorder = 20))\n\n\n# Boundary\n# ax.axhline(0,  alpha = 0.5, ls = '--', c = 'k', label = 'boundary')\n# ax.axhline(L,  alpha = 0.5, ls = '--', c = 'k')\n# ax.axvline(0,  alpha = 0.5, ls = '--', c = 'k')\n# ax.axvline(L,  alpha = 0.5, ls = '--', c = 'k')\n\n\nax.set_axis_off()\n# FOV origin\n# ax.scatter(fov_origin[0], fov_origin[1], label = 'FOV origin', s = 40, zorder = 10)\n\n# # legend = ax.legend()\n# legend.get_frame().set_alpha(None)\n# plt.setp(ax, xlabel = 'X (px)', ylabel = 'Y (px)')\n# plt.setp(ax2, xlim = (0,L), ylim = (0,L));\nplt.savefig('fov.svg')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The anomalous diffusion library",
    "section": "",
    "text": "Get started | Documentation | Tutorials | Cite us\nThis library has been created in the framework of the Anomalous Diffusion (AnDi) Challenge and allows to create trajectories and datasets from various anomalous diffusion models. You can install the package using:\nYou can then import the package in a Python3 environment using:"
  },
  {
    "objectID": "index.html#library-organization",
    "href": "index.html#library-organization",
    "title": "The anomalous diffusion library",
    "section": "Library organization",
    "text": "Library organization\nThe andi_datasets class allows to generate, transform, analyse, save and load diffusion trajectories from a plethora of diffusion models and experimental generated with various diffusion models. The library is structured in two main blocks, containing either theoretical or phenomenological models. Here is a scheme of the library’s content:\n\n\nTheoretical models\nThe library allows to generate trajectories from various anomalous diffusion models: continuous-time random walk (CTRW), fractional Brownian motion (FBM), Lévy walks (LW), annealed transit time model (ATTM) and scaled Brownian motion (SBM). You can generate trajectories with the desired anomalous exponent in either one, two or three dimensions.\nExamples of their use and properties can be found in this tutorial.\n\n\nPhenomenological models\nWe have also included models specifically developed to simulate realistic physical systems, in which random events alter the diffusion behaviour of the particle. The sources of these changes can be very broad, from the presence of heterogeneities either in space or time, the possibility of creating dimers and condensates or the presence of immobile traps in the environment.\nExamples of their use and properties can be found in this tutorial."
  },
  {
    "objectID": "index.html#the-andi-challenges",
    "href": "index.html#the-andi-challenges",
    "title": "The anomalous diffusion library",
    "section": "The AnDi Challenges",
    "text": "The AnDi Challenges\n\n1st AnDi Challenge (2020)\n\nThe first AnDi challenge was held between March and November 2020 and focused on the characterization of trajectories arising from different theoretical diffusion models under various experimental conditions. The results of the challenge are published in this article: Muñoz-Gil et al., Nat Commun 12, 6253 (2021).\nIf you want to reproduce the datasets used during the challenge, please check this tutorial. You can then test your predictions and compare them with the those of challenge participants in this online interactive tool.\n\n\n2nd AnDi Challenge (2023 / 2024)\nThe second AnDi challenge is LIVE. Follow the previous link to keep updated on all news. If you want to learn more about the data we will use, you can check this tutorial."
  },
  {
    "objectID": "index.html#version-control",
    "href": "index.html#version-control",
    "title": "The anomalous diffusion library",
    "section": "Version control",
    "text": "Version control\nDetails on each release are presented here."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "The anomalous diffusion library",
    "section": "Contributing",
    "text": "Contributing\nThe AnDi challenge is a community effort, hence any contribution to this library is more than welcome. If you think we should include a new model to the library, you can contact us in this mail: andi.challenge@gmail.com. You can also perform pull-requests and open issues with any feedback or comments you may have."
  },
  {
    "objectID": "index.html#cite-us",
    "href": "index.html#cite-us",
    "title": "The anomalous diffusion library",
    "section": "Cite us",
    "text": "Cite us\nIf you found this package useful and used it in your projects, you can use the following to directly cite the package:\n\nMuñoz-Gil, G., Requena B., Volpe G., Garcia-March M.A. and Manzo C.\nAnDiChallenge/ANDI_datasets: Challenge 2020 release (v.1.0). Zenodo (2021). \nhttps://doi.org/10.5281/zenodo.4775311\n\nOr you can cite the paper this package was developed for:\n- AnDi Challenge 1\n\nG. Muñoz-Gil, G. Volpe ... C. Manzo \nObjective comparison of methods to decode anomalous diffusion. \nNat Commun 12, 6253 (2021). \nhttps://doi.org/10.1038/s41467-021-26320-w\n\n- AnDi Challenge 2\n\nG. Muñoz-Gil, H. Bachimanchi ...  C. Manzo\nIn-principle accepted at Nature Communications (Registered Report Phase 1)\narXiv:2311.18100\nhttps://doi.org/10.48550/arXiv.2311.18100"
  },
  {
    "objectID": "lib_nbs/datasets_theory.html",
    "href": "lib_nbs/datasets_theory.html",
    "title": "datasets_theory",
    "section": "",
    "text": "Main class for generating datasets of theoretical trajectories. For details on this class, please check this tutorial.\n\nsource\n\ndatasets_theory\n\n datasets_theory ()\n\nThis class generates, saves and loads datasets of theoretical trajectories simulated from various diffusion models (available at andi_datasets.models_theory).\n\nsource\n\n\ncreate_dataset\n\n create_dataset (T, N_models, exponents, models, dimension=1,\n                 save_trajectories=False, load_trajectories=False,\n                 path='datasets/', N_save=1000, T_save=1000)\n\nCreates a dataset of trajectories via the theoretical models defined in .models_theory. Check our tutorials for use cases of this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nint\n\nLength of the trajectories.\n\n\nN_models\nint, numpy.array\n\n- if int, number of trajectories per class (i.e. exponent and model) in the dataset.- if numpy.array, number of trajectories per classes: size (number of models)x(number of classes)\n\n\nexponents\nfloat, array\n\nAnomalous exponents to include in the dataset. Allows for two digit precision.\n\n\nmodels\nbool, int, list\n\nLabels of the models to include in the dataset. Correspodance between models and labels is given by self.label_correspodance, defined at init.If int/list, choose the given models. If False, choose all of them.\n\n\ndimension\nint\n1\n\n\n\nsave_trajectories\nbool\nFalse\nIf True, the module saves a .h5 file for each model considered, with N_save trajectories and T = T_save.\n\n\nload_trajectories\nbool\nFalse\nIf True, the module loads the trajectories of an .h5 file.\n\n\npath\nstr\ndatasets/\nPath to the folder where to save/load the trajectories dataset.\n\n\nN_save\nint\n1000\nNumber of trajectories to save for each exponents/model. Advise: save at the beggining a big dataset (t_save ~ 1e3 and N_save ~ 1e4) which then allows you to load any other combiantion of T and N_models.\n\n\nT_save\nint\n1000\n\n\n\nReturns\nnumpy.array\n\n- Dataset of trajectories of lenght Nx(T+2), with the following structure: o First column: model label  o Second column: value of the anomalous exponent o 2:T columns: trajectories\n\n\n\n\nsource\n\n\ncreate_segmented_dataset\n\n create_segmented_dataset (dataset1, dataset2, dimension=1,\n                           final_length=200, random_shuffle=False)\n\nCreates a dataset with trajectories which change diffusive feature (either model or anomalous exponent) after a time ‘t_change’.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset1\nnumpy.array\n\nArray of size Nx(t+2), where the first columns values correspondto the labels of the model and anomalous exponent. The rest correspond to the trajectories of length t.\n\n\ndataset2\nnumpy.array\n\nSame as dataset1\n\n\ndimension\nint\n1\nDimensions of the generated trajectories. Three possible values: 1, 2 and 3.\n\n\nfinal_length\nint\n200\nLength of the output trajectories.\n\n\nrandom_shuffle\nbool\nFalse\nIf True, shuffles the first axis of dataset1 and dataset2.\n\n\nReturns\nnumpy.array\n\nArray of size Nx(t+5) whose columns represent: o Column 0: changing time o Column 1,2: labels first part of the trajectory (model, exponent) o Column 3,4: labels second part of the trajectory (model, exponent) o Column 5:(t+5): trajectories of lenght t.\n\n\n\n\nsource\n\n\ncreate_noisy_diffusion_dataset\n\n create_noisy_diffusion_dataset (dataset=False, T=False, N=False,\n                                 exponents=False, models=False,\n                                 dimension=1,\n                                 diffusion_coefficients=False,\n                                 save_trajectories=False,\n                                 load_trajectories=False,\n                                 path='datasets/', N_save=1000,\n                                 t_save=1000)\n\nCreate a dataset of noisy trajectories. This function creates trajectories with _create_trajectories and then adds given noise to them.\nAll arguments are the same as _create_trajectories but dataset and diffusion_coefficients.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\nbool\nFalse\n- If False, creates a dataset with the given parameters. - If numpy array, dataset to which the function applies the noise.\n\n\nT\nbool\nFalse\n\n\n\nN\nbool\nFalse\n\n\n\nexponents\nbool\nFalse\n\n\n\nmodels\nbool\nFalse\n\n\n\ndimension\nint\n1\n\n\n\ndiffusion_coefficients\nbool\nFalse\n\n\n\nsave_trajectories\nbool\nFalse\n\n\n\nload_trajectories\nbool\nFalse\n\n\n\npath\nstr\ndatasets/\n\n\n\nN_save\nint\n1000\n\n\n\nt_save\nint\n1000\n\n\n\nReturns\nnumpy.array\n\nDataset of trajectories of lenght Nx(T+2), with the following structure: o First column: model label  o Second column: value of the anomalous exponent o 2:T columns: trajectories\n\n\n\n\nsource\n\n\ncreate_noisy_localization_dataset\n\n create_noisy_localization_dataset (dataset=False, T=False, N=False,\n                                    exponents=False, models=False,\n                                    dimension=1, noise_func=False,\n                                    sigma=1, mu=0,\n                                    save_trajectories=False,\n                                    load_trajectories=False,\n                                    path='datasets/', N_save=1000,\n                                    t_save=1000)\n\nCreate a dataset of noisy trajectories. This function creates trajectories with _create_trajectories and then adds given noise to them.\nAll parameters are the same as _create_trajectories but noise_func.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\nbool\nFalse\nIf False, creates a dataset with the given parameters.If numpy array, dataset to which the function applies the noise.\n\n\nT\nbool\nFalse\n\n\n\nN\nbool\nFalse\n\n\n\nexponents\nbool\nFalse\n\n\n\nmodels\nbool\nFalse\n\n\n\ndimension\nint\n1\n\n\n\nnoise_func\nbool\nFalse\nIf False, the noise added to the trajectories will be Gaussian distributed, with variance sigma and mean value mu.If function, uses the given function to generate noise to be added to the trajectory.The function must have as input two ints, N and M and the output must be a matrix of size NxM.\n\n\nsigma\nint\n1\n\n\n\nmu\nint\n0\n\n\n\nsave_trajectories\nbool\nFalse\n\n\n\nload_trajectories\nbool\nFalse\n\n\n\npath\nstr\ndatasets/\n\n\n\nN_save\nint\n1000\n\n\n\nt_save\nint\n1000\n\n\n\nReturns\nnumpy.array\n\nDataset of trajectories of lenght Nx(T+2), with the following structure: o First column: model label  o Second column: value of the anomalous exponent o 2:T columns: trajectories"
  },
  {
    "objectID": "lib_nbs/datasets_challenge.html",
    "href": "lib_nbs/datasets_challenge.html",
    "title": "datasets_challenge",
    "section": "",
    "text": "source\n\n\n\n challenge_theory_dataset (N:numpy.ndarray|int=1000, max_T:int=1000,\n                           min_T:int=10, tasks:list|int=[1, 2, 3],\n                           dimensions:list|int=[1, 2, 3],\n                           load_dataset:{'True','False'}=False,\n                           save_dataset:{'True','False'}=False,\n                           path_datasets:str='',\n                           load_labels:{'True','False'}=True,\n                           load_trajectories:{'True','False'}=False,\n                           save_trajectories:{'True','False'}=False,\n                           path_trajectories:str='datasets/',\n                           N_save:int=1000, t_save:int=1000,\n                           return_noise:{'True','False'}=False)\n\nCreates a dataset similar to the one given by in the ANDI 1 challenge. Check the webpage of the challenge for more details. The default values are similar to the ones used to generate the available dataset.\nThe function returns 6 variables, three variables for the trajectories and three for the corresponding labels. Each variable is a list of three lists. Each of the three lists corresponds to a given dimension, in ascending order. If one of the tasks/dimensions was not calculated, the given list will be empty.\nSee the tutorials in our Github repository to learn about this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nnumpy.ndarray | int\n1000\nNumber of trajectories per class (i.e.size # models x # classes). If int, all classes have same number.\n\n\nmax_T\nint\n1000\nMaximum length of the trajectories in the dataset.\n\n\nmin_T\nint\n10\nMinimum length of the trajectories in the dataset.\n\n\ntasks\nlist | int\n[1, 2, 3]\nTask(s) of the ANDI challenge I for which datasets will be generated.\n\n\ndimensions\nlist | int\n[1, 2, 3]\nDimension(s) for which trajectories will be generated. Three possible values: 1, 2 and 3.\n\n\nload_dataset\n{‘True’, ‘False’}\nFalse\nIf True, the module loads existing datasets from the files task{}.txt and ref{}.txt.\n\n\nsave_dataset\n{‘True’, ‘False’}\nFalse\nIf True, the module saves the datasets in a .txt following the competition format.\n\n\npath_datasets\nstr\n\nPath from where to load the dataset.\n\n\nload_labels\n{‘True’, ‘False’}\nTrue\nIf False, only loads trajectories and avoids the files refX.txt.\n\n\nload_trajectories\n{‘True’, ‘False’}\nFalse\nIf True, the module loads the trajectories of an .h5 file.\n\n\nsave_trajectories\n{‘True’, ‘False’}\nFalse\nIf True, the module saves a .h5 file for each model considered, with N_save trajectories and T = T_save.\n\n\npath_trajectories\nstr\ndatasets/\nPath from where to load trajectories.\n\n\nN_save\nint\n1000\nNumber of trajectories to save for each exponents/model. Advise: save at the beggining a big dataset (i.e. with default t_save N_save) which allows you to load any  other combiantionof T and N.\n\n\nt_save\nint\n1000\nLength of the trajectories to be saved. See comments on N_save.\n\n\nreturn_noise\n{‘True’, ‘False’}\nFalse\nIf True, returns the amplitudes of the noises added to the trajectories.\n\n\nReturns\nmultiple\n\nXn (lists): trajectoriesYn (lists): labelsloc_noise_tn (lists): localization noise amplitudesdiff_noise_tn (lists): variance of the diffusion noise"
  },
  {
    "objectID": "lib_nbs/datasets_challenge.html#examples",
    "href": "lib_nbs/datasets_challenge.html#examples",
    "title": "datasets_challenge",
    "section": "Examples",
    "text": "Examples\nWe generate a dataset of trajectories from 5 different experiments. As we are not stating the opposite, each experiment will correspond to one of the 5 diffusion models considered in ANDI 2 (phenom).\n\nnum_experiments, num_fovs = 2, 1\n\ndics = []\nfor i in range(num_experiments):    \n    dic = _get_dic_andi2(i+1)    \n    if i == 0: dic.update({'dim':2})\n    dics.append(dic)\n    \n    \ndf_list, _, _, _ = challenge_phenom_dataset(experiments = num_experiments, \n                                            num_fovs = num_fovs, \n                                            dics = dics,\n                                            return_timestep_labs = False, \n                                            get_video = True, num_vip = 3,\n                                            save_data = True, path = 'dataset/',                                 \n                                            files_reorg = True, path_reorg = 'reorg/', save_labels_reorg = False, delete_raw = True)\n\n\nDistributions parameters\nWe first check how distributed are the diffusion parameters of the generated trajectories.\n\nfig, axs = plt.subplots(2, len(df_list), figsize = (len(df_list)*2, 2*2), tight_layout = True)\n\nfor df, ax, dic in zip(df_list, axs.transpose(), dics):\n    alphas = df['alpha']\n    Ds = df['D']\n    states = df['state']\n    for u in np.unique(states):\n        ax[0].hist(alphas[states == u], density = 1)\n        ax[1].hist(Ds[states == u], density = 1)\n    \n    ax[0].set_title(dic['model'])\nplt.setp(axs[:,0], ylabel = 'Frequency')\nplt.setp(axs[0,:], xlabel = r'$\\alpha$')\nplt.setp(axs[1,:], xlabel = r'$D$')\n;\n\n''\n\n\n\n\n\n\n\nFOVs\nWe can also check that generating multiple FOVS from every experiments actually choses random FOVs in the desired space.\n\nnum_fovs = 3; num_experiments = 4\ndf_fov, _ , lab_e = challenge_phenom_dataset(experiments = [1,2,3,4,5],\n                                               num_fovs =num_fovs, \n                                               return_timestep_labs = True\n                                               )\n\n\n\n\nCreating dataset for Exp_0 (single_state).\nCreating dataset for Exp_1 (multi_state).\nCreating dataset for Exp_2 (immobile_traps).\nCreating dataset for Exp_3 (dimerization).\nCreating dataset for Exp_4 (confinement)."
  },
  {
    "objectID": "lib_nbs/utils_trajectories.html",
    "href": "lib_nbs/utils_trajectories.html",
    "title": "utils_trajectories",
    "section": "",
    "text": "source\n\n\n\n\n pert (params:list, size:int=1, lamb=4)\n\nSamples from a Pert distribution of given parameters\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nlist\n\nPert parameters a, b, c\n\n\nsize\nint\n1\nnumber of samples to get\n\n\nlamb\nint\n4\nlambda pert parameters\n\n\nReturns\narray\n\nsamples from the given Pert distribution\n\n\n\n\n\n\n\nsource\n\n\n\n\n gaussian (params:list|int, size=1, bound=None)\n\nSamples from a Gaussian distribution of given parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nlist | int\n\nIf list, mu and sigma of the gaussian. If int, we consider sigma = 0\n\n\nsize\nint\n1\nNumber of samples to get.\n\n\nbound\nNoneType\nNone\nBound of the Gaussian, if any.\n\n\nReturns\narray\n\nSamples from the given Gaussian distribution\n\n\n\n\nk = gaussian([2, 0.1], size = int(1e5), bound = [0, 2])\nplt.hist(k, bins = 200);\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n sample_sphere (N:int, R:int|list)\n\nSamples random number that lay in the surface of a 3D sphere centered in zero and with radius R.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nN\nint\nNumber of points to generate.\n\n\nR\nint | list\nRadius of the sphere. If int, all points havethe same radius, if numpy.array, each number has different radius.\n\n\nReturns\narray\nSampled numbers"
  },
  {
    "objectID": "lib_nbs/utils_trajectories.html#regularize-trajectory-with-irregular-sampling-times",
    "href": "lib_nbs/utils_trajectories.html#regularize-trajectory-with-irregular-sampling-times",
    "title": "utils_trajectories",
    "section": "Regularize trajectory with irregular sampling times",
    "text": "Regularize trajectory with irregular sampling times\n\nsource\n\nregularize\n\n regularize (positions:<built-infunctionarray>, times:<built-\n             infunctionarray>, T:int)\n\nRegularizes a trajectory with irregular sampling times.\n\n\n\n\nType\nDetails\n\n\n\n\npositions\narray\nPositions of the trajectory to regularize\n\n\ntimes\narray\nTimes at which previous positions were recorded\n\n\nT\nint\nLength of the output trajectory\n\n\nReturns\narray\nRegularized trajectory."
  },
  {
    "objectID": "lib_nbs/utils_trajectories.html#normalize-displacements-of-a-trajectory",
    "href": "lib_nbs/utils_trajectories.html#normalize-displacements-of-a-trajectory",
    "title": "utils_trajectories",
    "section": "Normalize displacements of a trajectory",
    "text": "Normalize displacements of a trajectory\n\nsource\n\nnormalize\n\n normalize (trajs)\n\nNormalizes trajectories by substracting average and dividing by SQRT of their standard deviation.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntrajs\nnp.array\nArray of length N x T or just T containing the ensemble or single trajectory to normalize.\n\n\n\n\nsource\n\n\nnormalize_fGN\n\n normalize_fGN (disp, alpha, D, T:int, deltaT:int=1)\n\nNormalizes fractional Gaussian Noise created with stochastic library.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndisp\nArray-like of shape N x T or just T containing the displacements to normalize.\n\n\n\n\nalpha\nfloat in [0,2] or array-like of length N x 1\n\nAnomalous exponent\n\n\nD\nfloat or array-like of shape N x 1\n\nDiffusion coefficient\n\n\nT\nint\n\nNumber of timesteps the displacements were generated with\n\n\ndeltaT\nint\n1\nSampling time\n\n\nReturns\nArray-like containing T displacements of given parameters\n\n\n\n\n\n\nN = 10; T = 10\ntrajs = 3*np.random.randn(N*T*2).reshape(N,T,2)\ntrajs = np.cumsum(trajs, axis = 1)\n\nnorm_trajs = normalize(trajs)\n\nidx = 0; plt.figure(figsize = (3,3))\nplt.plot(trajs[idx,:,0]-trajs[idx,0,0], '-o', label = 'Original trajectory')\nplt.plot(norm_trajs[idx,:,0], '-o', label = 'Normalized')\nplt.legend(); plt.xlabel('Time'); plt.ylabel('Position')\n\nText(0, 0.5, 'Position')"
  },
  {
    "objectID": "lib_nbs/models_theory.html",
    "href": "lib_nbs/models_theory.html",
    "title": "models_theory",
    "section": "",
    "text": "This class contains a recopilatory of diffusion models used in ANDI. The class is organized in three subclasses, each corresponding to a different dimensionality of the output trajectory.\nCurrently the library containts the following models:\n\n\n\n\n\n\n\n\nFunction\nDimensions\nDescription\n\n\n\n\nbm\n(1D)\nBrownian motion\n\n\nfbm\n(1D/2D/3D)\nFractional browian motion, simulated via the stochastic Python library\n\n\nctrw\n(1D/2D/3D)\nContinuous time random walks\n\n\nlw\n(1D/2D/3D)\nLevy walks\n\n\nattm\n(1D/2D/3D)\nAnnealed transit time\n\n\nsbm\n(1D/2D/3D)\nScaled brownian motion\n\n\n\nThe class is organized as follows:\n\nFor every model in the previous list, there exists a function models_theory.name_walk which generates diffusion following that model. All functions have the same inputs and outputs:\nInputs\n\nT (int): lenght of the trajectory. Gets transformed to int if input is float.\nalpha (float): anomalous exponent\nD ({1,2,3}): dimension of the walk.\n\nOutputs\n\nnumpy.array of lenght DxT.\n\nThis is the recommended way of generating trajectories.\nFor every dimension, there exists a subclass models_theory._dimensionD, which gives access to the walk generators in each dimensions. Every walk is a function of these subclasses. This allows to access more features of the diffusion models:\n\nctrw\n\nregular_time (bool): if true, regularizes the trajectory so that every element in the output corresponds to a time step. If false, returns the positions and times at which steps were done.\n\nattm:\n\nregime ({1,2,3}): allows to chose the regime of ATTM model. See the original paper Phys. Rev. Lett. 112, 150603 (2014) for further details.\n\nsbm:\n\nsigma (float): variance of the noise generator (similar to a local diffusion coefficient).\n\n\n\nTo learn more on the use of this class, please visit this tutorial."
  },
  {
    "objectID": "lib_nbs/index_docs.html",
    "href": "lib_nbs/index_docs.html",
    "title": "Documentation",
    "section": "",
    "text": "andi_datasets is a modular library that allows you to create diffusion trajectories in a variety of experimentally revelant conditions. It is organized in four main blocks:\n\nTheory datasets: motivated by our first AnDi Challenge, we gather here different theoretical anomalous diffusion models, as e.g. fractional Brownian motion or continuous time random walk.\nPhenomenological datasets: to closer simulate experimental trajectories, we consider also diffusion when interactions between particles and the environmnet are present. For instance, the appearance of comparments, trapping, dimerization but also changes in diffusion properties.\nAnDi Challenge datasets: this blocks manages the generation of datasets for the various AnDi Challenges. It allows to replicate the same datasets that will be given in the different editions of the challenge.\nTrajectory analysis: here we offer different statistical approaches to anomalous diffusion characterization. This will be a growing set of tools that will help the user to correctly characterize their trajectories with minimal coding.\n\nHere is a schematic representation of the library contents:\n\n\n\nA smooth introduction to each component of this library is done in the Tutorials sections."
  },
  {
    "objectID": "lib_nbs/models_phenom.html",
    "href": "lib_nbs/models_phenom.html",
    "title": "models_phenom",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#single-trajectory-generator",
    "href": "lib_nbs/models_phenom.html#single-trajectory-generator",
    "title": "models_phenom",
    "section": "Single trajectory generator",
    "text": "Single trajectory generator\n\nsource\n\n_single_state_traj\n\n _single_state_traj (T:int=200, D:float=1, alpha:float=1, L:float=None,\n                     deltaT:int=1, dim:int=2)\n\nGenerates a single state trajectory with given parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nint\n200\nLength of the trajectory\n\n\nD\nfloat\n1\nDiffusion coefficient\n\n\nalpha\nfloat\n1\nAnomalous exponent\n\n\nL\nfloat\nNone\nLength of the box acting as the environment\n\n\ndeltaT\nint\n1\nSampling time\n\n\ndim\nint\n2\nDimension of the walk (can be 2 or 3)\n\n\nReturns\ntuple\n\n- pos: position of the particle- labels: anomalous exponent, D and state at each timestep. State is always free here.\n\n\n\nExample in 2D\n\ntraj, labels = models_phenom._single_state_traj(D = 0.05,  alpha = 1.5, \n                                                T =1000, L = 10, dim = 2)\n\nfig, ax = plt.subplots(1, 3, figsize = (10, 2), tight_layout = True)\nax[0].plot(traj[:, 0], traj[:, 1], alpha = 0.5)\nplt.setp(ax[0], xlabel = 'X', ylabel = 'Y')\n\nax[1].plot(traj[:, 0], '.', label = 'X')\nax[1].plot(traj[:, 1], '.', label = 'Y', )\nplt.setp(ax[1], ylabel = 'Position', xlabel = 'Time')\nax[1].legend()\n\nax[2].plot(labels[:, 0], '.', label = r'$\\alpha$')\nax[2].plot(labels[:, 1], '.', label = r'$D$' )\nplt.setp(ax[2], ylabel = 'Label', xlabel = 'Time')\nax[2].legend()\n\nfor b in [0,10]:\n    ax[0].axhline(b, ls = '--', alpha = 0.3, c = 'k')\n    ax[0].axvline(b, ls = '--', alpha = 0.3, c = 'k')\n    ax[1].axhline(b, ls = '--', alpha = 0.3, c = 'k')\n\n\n\n\nExample in 3D\n\ntraj, labels = models_phenom._single_state_traj(D = 0.05,  alpha = 1.5, \n                                                T =1000, L = 10, dim = 3)\n\nfig = plt.figure(figsize = (6,5))\nax = fig.add_subplot(projection = '3d')\n\nax.plot(traj[:,0], traj[:,1], traj[:,2], c = 'k')\nax.scatter3D(traj[:,0], traj[:,1], traj[:,2], c = np.arange(traj.shape[0]), s = 20);\nplt.setp(ax, xlabel = 'Position X', ylabel = 'Position Y', zlabel = 'Position Z',\n         xticklabels = [], yticklabels = [], zticklabels = [])\nax.zaxis.labelpad=-8"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#dataset-generation",
    "href": "lib_nbs/models_phenom.html#dataset-generation",
    "title": "models_phenom",
    "section": "Dataset generation",
    "text": "Dataset generation\n\nsource\n\nsingle_state\n\n single_state (N:int=10, T:int=200, Ds:list=[1, 0], alphas:list=[1, 0],\n               L:float=None, dim:int=2)\n\nGenerates a dataset made of single state trajectories with given parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n10\nNumber of trajectories in the dataset\n\n\nT\nint\n200\nLength of the trajectory\n\n\nDs\nlist\n[1, 0]\nIf list, mean and variance from which to sample the diffusion coefficient. If float, we consider variance = 0.\n\n\nalphas\nlist\n[1, 0]\nIf list, mean and variance from which to sample the anomalous exponent. If float, we consider variance = 0.\n\n\nL\nfloat\nNone\nLength of the box acting as the environment\n\n\ndim\nint\n2\nDimension of the walk (can be 2 or 3)\n\n\nReturns\ntuple\n\n- positions: position of the N trajectories.- labels: anomalous exponent, D and state at each timestep. State is always free here. \n\n\n\nExample in 2D\n\nN = 500; L = 5; T = 100;\nalpha = [0.8, 0.1]; D = [0.1,0.1]\n\ntrajs, labels = models_phenom().single_state(N = N,\n                                           L = L,\n                                           T = T, \n                                           alphas = alpha,\n                                           Ds = D)\nfig, ax = plt.subplots(1, 2, figsize = (6, 3), tight_layout = True)\nax[0].hist(labels[:,:,0].flatten(), bins = 50)\nplt.setp(ax[0], title = r'Distribution of $\\alpha$', xlabel = r'$\\alpha$', ylabel = 'Frequency');\n\nax[1].hist(labels[:,:,1].flatten(), bins = 50)\nplt.setp(ax[1], title = r'Distribution of $D$', xlabel = r'$D$', ylabel = 'Frequency');\n\n\n\n\n\nplot_trajs(trajs, L, N)\n\n\n\n\nExample in 3D\n\nN = 4; L = 5; T = 100;\nalpha = [0.8, 0.1]; D = [0.1,0.1]\n\ntrajs, labels = models_phenom().single_state(N = N,\n                                           L = L,\n                                           T = T, \n                                           alphas = alpha,\n                                           Ds = D,\n                                            dim = 3)\n\n\nfig = plt.figure(figsize = (10,5))\n\naxs = []\nfor idx, traj in enumerate(trajs.transpose(1,0,2)):\n    ax = fig.add_subplot(1,4,idx+1, projection = '3d')\n\n    ax.plot(traj[:,0], traj[:,1], traj[:,2], c = 'k')\n    ax.scatter3D(traj[:,0], traj[:,1], traj[:,2], c = np.arange(traj.shape[0]), s = 20);\n    axs.append(ax)\n    \nplt.setp(axs, xticklabels = [], yticklabels = [], zticklabels = []);"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#single-trajectory-generator-1",
    "href": "lib_nbs/models_phenom.html#single-trajectory-generator-1",
    "title": "models_phenom",
    "section": "Single trajectory generator",
    "text": "Single trajectory generator\n\nsource\n\n_multiple_state_traj\n\n _multiple_state_traj (T=200, M=[[0.95, 0.05], [0.05, 0.95]], Ds=[1, 0.1],\n                       alphas=[1, 1], L=None, deltaT=1,\n                       return_state_num=False, init_state=None)\n\nGenerates a 2D multi state trajectory with given parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nint\n200\nLength of the trajectory\n\n\nM\nlist\n[[0.95, 0.05], [0.05, 0.95]]\nTransition matrix between diffusive states.\n\n\nDs\nlist\n[1, 0.1]\nDiffusion coefficients of the diffusive states. Must have as many Ds as states defined by M.\n\n\nalphas\nlist\n[1, 1]\nAnomalous exponents of the diffusive states. Must have as many alphas as states defined by M.\n\n\nL\nNoneType\nNone\nLength of the box acting as the environment\n\n\ndeltaT\nint\n1\nSampling time\n\n\nreturn_state_num\nbool\nFalse\nIf True, returns as label the number assigned to the state at each time step.\n\n\ninit_state\nNoneType\nNone\nIf True, the particle starts in state 0. If not, sample initial state.\n\n\nReturns\ntuple\n\n- pos: position of the particle- alphas_t: anomalous exponent at each step- Ds_t: diffusion coefficient at each step. - label_diff_state: particle’s state (can be either free or directed for alpha ~ 2) at each step.- state (optional): state label at each step. \n\n\n\n\nT = 1000; L = 100\ntraj, labels = models_phenom._multiple_state_traj(T = T,\n                                                  L = L,\n                                                  alphas = [0.2, 0.7], \n                                                  Ds = [1.5, 2],                                                  \n                                                  return_state_num=True)\n\nfig, ax = plt.subplots(1, 3, figsize = (9, 3), tight_layout = True)\nax[0].plot(traj[:, 0], traj[:, 1], alpha = 0.5)\nplt.setp(ax[0], xlabel = 'X', ylabel = 'Y')\n\nax[1].plot(traj[:, 0], label = 'X')\nax[1].plot(traj[:, 1], label = 'Y', )\nplt.setp(ax[1], ylabel = 'Position', xlabel = 'Time')\nax[1].legend()\n\nax[2].plot(labels[:, 0], '.', label = r'$\\alpha$')\nax[2].plot(labels[:, 1], '.', label = r'$D$' )\nax[2].plot(labels[:, 3], '.', label = r'$state \\#$', alpha = 0.3 )\nplt.setp(ax[2], ylabel = 'Label', xlabel = 'Time')\nax[2].legend()\n\nfor b in [0,L]:\n    ax[0].axhline(b, ls = '--', alpha = 0.3, c = 'k')\n    ax[0].axvline(b, ls = '--', alpha = 0.3, c = 'k')\n    ax[1].axhline(b, ls = '--', alpha = 0.3, c = 'k')"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#dataset-generation-1",
    "href": "lib_nbs/models_phenom.html#dataset-generation-1",
    "title": "models_phenom",
    "section": "Dataset generation",
    "text": "Dataset generation\n\nsource\n\nmulti_state\n\n multi_state (N=10, T=200, M:<built-infunctionarray>=[[0.9, 0.1], [0.1,\n              0.9]], Ds:<built-infunctionarray>=[[1, 0], [0.1, 0]],\n              alphas:<built-infunctionarray>=[[1, 0], [1, 0]],\n              gamma_d=None, epsilon_a=None, L=None,\n              return_state_num=False, init_state=None)\n\nGenerates a dataset of 2D multi state trajectory with given parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n10\nNumber of trajectories\n\n\nT\nint\n200\nLength of the trajectory\n\n\nM\narray\n[[0.9, 0.1], [0.1, 0.9]]\nTransition matrix between diffusive states\n\n\nDs\narray\n[[1, 0], [0.1, 0]]\nList of means and variances from which to sample the diffusion coefficient of each state. If element size is one, we consider variance = 0.\n\n\nalphas\narray\n[[1, 0], [1, 0]]\nList of means and variances from which to sample the anomalous exponent of each state. If element size is one, we consider variance = 0.\n\n\ngamma_d\nNoneType\nNone\nMinimum factor between D of diffusive states (see ._sampling_diff_parameters)\n\n\nepsilon_a\nNoneType\nNone\nDistance between alpha of diffusive states (see ._sampling_diff_parameters)\n\n\nL\nNoneType\nNone\nLength of the box acting as the environment\n\n\nreturn_state_num\nbool\nFalse\nIf True, returns as label the number assigned to the state at each time step.\n\n\ninit_state\nNoneType\nNone\nIf True, the particle starts in state 0. If not, sample initial state.\n\n\nReturns\ntuple\n\n- trajs (array TxNx2): particles’ position- labels (array TxNx2): particles’ labels (see ._multi_state for details on labels) \n\n\n\n\n2-states\n\nN = 100; L = 50; T = 100;\n\ntrajs, labels = models_phenom().multi_state(N = N, T = T, L = L,\n                                            M = np.array([[0.95 , 0.05],[0.05, 0.95]]),\n                                            Ds = np.array([[1, 0], [1, 0.5]]), \n                                            alphas = np.array([[1, 0.01], [0.5, 0.02]]),\n                                            epsilon_a=[0.4], gamma_d = [0.75],\n                                            return_state_num=True)\n\nWe can first check the parameter distributions:\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3), tight_layout = True)\n\n# Diffusion coefficients\nax[0].hist(labels[labels[:,:,3] == 0, 1].flatten(), label = f'State 1 - % = {(labels[:,:,3] == 0).sum()/np.prod(labels.shape[:2])}')\nax[0].hist(labels[labels[:,:,3] == 1, 1].flatten(), label = f'State 2 - % = {(labels[:,:,3] == 1).sum()/np.prod(labels.shape[:2])}')\nplt.setp(ax[0], title = r'Distribution of $D$', xlabel = r'$D$', ylabel = 'Frequency');\n\n# Anomalous exponents\nax[1].hist(labels[labels[:,:,3] == 0, 0].flatten(), label = f'State 1 - # = {(labels[:,:,3] == 0).sum()}')\nax[1].hist(labels[labels[:,:,3] == 1, 0].flatten(), label = f'State 2 - # = {(labels[:,:,3] == 1).sum()}')\nplt.setp(ax[1], title = r'Distribution of $\\alpha$', xlabel = r'$\\alpha$', ylabel = 'Frequency');\nax[0].legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nAnd then see some examples of trajectories:\n\nplot_trajs(trajs, L, N, labels = labels, plot_labels = True)\n\n\n\n\n\n\n3-states\n\nN = 100; L = 50; T = 100;\n\ntrajs, labels = models_phenom().multi_state(N = N, T = T, L = L,\n                                            M = np.array([[0.98 , 0.01, 0.01],[0.01, 0.98, 0.01], [0.01, 0.01, 0.98]]),\n                                            Ds = np.array([[1, 0], [1, 0.5], [0.1, 0]]), \n                                            alphas = np.array([[1, 0.01], [0.5, 0.02], [1.8, 0.1]]),\n                                            # epsilon_a=[0.4], gamma_d = [0.75],\n                                            return_state_num=True)\n\nWe can first check the parameter distributions:\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3), tight_layout = True)\n\n# Diffusion coefficients\nax[0].hist(labels[labels[:,:,3] == 0, 1].flatten(), label = f'State 1 - % = {(labels[:,:,3] == 0).sum()/np.prod(labels.shape[:2])}')\nax[0].hist(labels[labels[:,:,3] == 1, 1].flatten(), label = f'State 2 - % = {(labels[:,:,3] == 1).sum()/np.prod(labels.shape[:2])}')\nax[0].hist(labels[labels[:,:,3] == 2, 1].flatten(), label = f'State 3 - % = {(labels[:,:,3] == 2).sum()/np.prod(labels.shape[:2])}')\nplt.setp(ax[0], title = r'Distribution of $D$', xlabel = r'$D$', ylabel = 'Frequency');\n\n# Anomalous exponents\nax[1].hist(labels[labels[:,:,3] == 0, 0].flatten(), label = f'State 1 - # = {(labels[:,:,3] == 0).sum()}')\nax[1].hist(labels[labels[:,:,3] == 1, 0].flatten(), label = f'State 2 - # = {(labels[:,:,3] == 1).sum()}')\nax[1].hist(labels[labels[:,:,3] == 2, 0].flatten(), label = f'State 3 - # = {(labels[:,:,3] == 2).sum()}')\nplt.setp(ax[1], title = r'Distribution of $\\alpha$', xlabel = r'$\\alpha$', ylabel = 'Frequency');\nax[0].legend()\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#auxiliary-functions",
    "href": "lib_nbs/models_phenom.html#auxiliary-functions",
    "title": "models_phenom",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\nDistance calculator\n\nsource\n\n_get_distance\n\n _get_distance (x)\n\nGiven a matrix of size Nx2, calculates the distance between the N particles.\n\n\n\n\nType\nDetails\n\n\n\n\nx\narray\nParticles’ positions\n\n\nReturns\narray\nDistance between particles \n\n\n\nEscaping dynamics\n\nsource\n\n\n_make_escape\n\n _make_escape (Pu, label, diff_state)\n\nGiven an unbinding probablity (Pu), the current labeling of particles (label) and the current state of particle (diff_state, either bound, 1, or unbound, 0), simulate an stochastic binding mechanism.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nPu\nfloat\nUnbinding probablity\n\n\nlabel\narray\nCurrent labeling of the particles (i.e. to which condensate they belong)\n\n\ndiff_state\narray\nCurrent state of the particles\n\n\nReturns\ntuple\nNew labeling and diffusive state of the particles \n\n\n\nClustering dynamics\n\nsource\n\n\n_make_condensates\n\n _make_condensates (Pb, label, diff_state, r, distance, max_label)\n\nGiven a binding probability Pb, the current label of particles (label), their current diffusive state (diff_state), the particle size (r), their distances (distance) and the label from which binding is not possible (max_label), simulates a binding mechanism.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nPb\nfloat\nBinding probablity.\n\n\nlabel\narray\nCurrent labeling of the particles (i.e. to which condensate they belong)\n\n\ndiff_state\narray\nCurrent state of the particles\n\n\nr\nfloat\nParticle size.\n\n\ndistance\narray\nDistance between particles\n\n\nmax_label\nint\nMaximum label from which particles will not be considered for binding\n\n\nReturns\ntuple\nNew labeling and diffusive state of the particles\n\n\n\nHere is a test in which some particles, distributed randomly through a bounded space, first bind and then unbind using the previous defined functions:\n\n# Binding and unbinding probabilities\nPb = 0.8; Pu = 0.5\n# Generate the particles\nN = 200; L = 10; r = 1; max_n = 2; Ds = np.ones(100)\npos = np.random.rand(N, 2)*L    \n# Put random labels (label = which condensate you belong). diff_state is zero because all are unbound)\nlabel = np.arange(N)#np.random.choice(range(500), N, replace = False)\ndiff_state = np.zeros(N).astype(int)\n# Define max_label bigger than max of label so everybody binds\nmax_label = max(label)+2\n# Calculate distance between particles\ndistance = models_phenom._get_distance(pos)\n\nprint('# of free particles:')\nprint(f'Before binding: {len(label)}')\n\n# First make particle bind:\nlab, ds = models_phenom._make_condensates(Pb, label, diff_state, r, distance, max_label)\nprint(f'After binding: {np.unique(lab[np.argwhere(ds == 0)], return_counts=True)[0].shape[0]}')\n\n# Then we do unbinding:\nlab, ds = models_phenom._make_escape(Pu, lab, ds)\nprint(f'After unbinding: {np.unique(lab[np.argwhere(ds == 0)], return_counts=True)[0].shape[0]}')\n\n# of free particles:\nBefore binding: 200\nAfter binding: 4\nAfter unbinding: 106\n\n\nStokes drag\n\nsource\n\n\n_stokes\n\n _stokes (D)\n\nApplies a Stokes-Einstein-like transformation to two diffusion coefficients.\n\n\n\n\nType\nDetails\n\n\n\n\nD\ntuple\nDiffusion coefficients of the two binding particles.\n\n\nReturns\nfloat\nResulting diffusion coefficient."
  },
  {
    "objectID": "lib_nbs/models_phenom.html#time-evolution",
    "href": "lib_nbs/models_phenom.html#time-evolution",
    "title": "models_phenom",
    "section": "Time evolution",
    "text": "Time evolution\n\nsource\n\ndimerization\n\n dimerization (N=10, T=200, L=100, r=1, Pu=0.1, Pb=0.01, Ds:<built-\n               infunctionarray>=[[1, 0], [0.1, 0]], alphas:<built-\n               infunctionarray>=[[1, 0], [1, 0]], epsilon_a=0,\n               stokes=False, return_state_num=False, deltaT=1)\n\nGenerates a dataset of 2D trajectories of particles perfoming stochastic dimerization.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n10\nNumber of trajectories\n\n\nT\nint\n200\nLength of the trajectory\n\n\nL\nint\n100\nLength of the box acting as the environment\n\n\nr\nint\n1\nRadius of particles.\n\n\nPu\nfloat\n0.1\nUnbinding probability.\n\n\nPb\nfloat\n0.01\nBinding probability.\n\n\nDs\narray\n[[1, 0], [0.1, 0]]\nList of means and variances from which to sample the diffusion coefficient of each state. If element size is one, we consider variance = 0.\n\n\nalphas\narray\n[[1, 0], [1, 0]]\nList of means and variances from which to sample the anomalous exponent of each state. If element size is one, we consider variance = 0.\n\n\nepsilon_a\nint\n0\nDistance between alpha of diffusive states (see ._sampling_diff_parameters)\n\n\nstokes\nbool\nFalse\nIf True, applies a Stokes-Einstein like coefficient to calculate the diffusion coefficient of dimerized particles. If False, we use as D resulting from the dimerization the D assigned to the dimerized state of one of the two particles.\n\n\nreturn_state_num\nbool\nFalse\nIf True, returns as label the number assigned to the state at each time step.\n\n\ndeltaT\nint\n1\nSampling time\n\n\nReturns\ntuple\n\n- trajs (array TxNx2): particles’ position- labels (array TxNx2): particles’ labels (see ._multi_state for details on labels)\n\n\n\n\nN = 500; L = 50; r = 1; T = 100\nPu = 0.1 # Unbinding probability\nPb = 1 # Binding probability\n# Diffusion coefficients of two states\nstokes = True\nDs = np.array([[2, 0.01], [1e-5, 0]]) # because stokes = True, we don't care about the second state\n# Anomalous exponents for two states\nalphas = np.array([[1, 0], [1, 0.2]]) \n\ntrajs, labels = models_phenom().dimerization(N = N,\n                                            L = L,\n                                            r = r,\n                                            T = T,\n                                            Pu = Pu, # Unbinding probability\n                                            Pb = Pb, # Binding probability\n                                            Ds = Ds, # Diffusion coefficients of two states\n                                            alphas = alphas, # Anomalous exponents for two states,\n                                            return_state_num = True,\n                                            stokes = True, epsilon_a=0.2\n                                            )\n\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3), tight_layout = True)\n\n# Diffusion coefficients\nax[0].hist(labels[labels[:,:,3] == 0, 1].flatten(), label = f'State 1 - % = {(labels[:,:,3] == 0).sum()/np.prod(labels.shape[:2])}')\nax[0].hist(labels[labels[:,:,3] == 1, 1].flatten(), label = f'State 2 - % = {(labels[:,:,3] == 1).sum()/np.prod(labels.shape[:2])}')\nplt.setp(ax[0], title = r'Distribution of $D$', xlabel = r'$D$', ylabel = 'Frequency');\n\n# Anomalous exponents\nax[1].hist(labels[labels[:,:,3] == 0, 0].flatten(), label = f'State 1 - # = {(labels[:,:,3] == 0).sum()}')\nax[1].hist(labels[labels[:,:,3] == 1, 0].flatten(), label = f'State 2 - # = {(labels[:,:,3] == 1).sum()}')\nplt.setp(ax[1], title = r'Distribution of $\\alpha$', xlabel = r'$\\alpha$', ylabel = 'Frequency');\nax[0].legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\nplot_trajs(trajs, L, N, labels = labels, plot_labels = True)"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#auxiliary-functions-1",
    "href": "lib_nbs/models_phenom.html#auxiliary-functions-1",
    "title": "models_phenom",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\nDistribute compartments\n\nsource\n\n_distribute_circular_compartments\n\n _distribute_circular_compartments (Nc, r, L)\n\nDistributes circular compartments over an environment without overlapping. Raises a warning and stops when no more compartments can be inserted.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nNc\nfloat\nNumber of compartments\n\n\nr\nfloat\nSize of the compartments\n\n\nL\nfloat\nSide length of the squared environment.\n\n\nReturns\narray\nPosition of the centers of the compartments\n\n\n\n\nfig, ax = plt.subplots(figsize = (5,5))\n\nNc = 60; r = 10; L = 256;\ncomp_center = models_phenom._distribute_circular_compartments(Nc, r, L)\n\nfor c in comp_center:\n    circle = plt.Circle((c[0], c[1]), r)\n    ax.add_patch(circle)\nax.set_xlim(0,L)\nax.set_ylim(0,L)\n\n(0.0, 256.0)\n\n\n\n\n\nReflection inside circles\n\nsource\n\n\n_reflected_position\n\n _reflected_position (circle_center, circle_radius, beg, end,\n                      precision_boundary=0.0001)\n\nGiven the begining and end of a segment crossing the boundary of a circle, calculates the new position considering that boundaries are fully reflective.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncircle_center\nfloat\n\nCenter of the circle\n\n\ncircle_radius\nfloat\n\nRadius of the circle\n\n\nbeg\ntuple\n\nPosition (in 2D) of the begining of the segment\n\n\nend\ntuple\n\nPosition (in 2D) of the begining of the segment\n\n\nprecision_boundary\nfloat\n0.0001\nSmall area around the real boundary which is also considered as boundary. For numerical stability\n\n\nReturns\ntuple\n\n- Reflected position- Intersection point\n\n\n\n\ncircle_radius = 2;\ncircle_center = [0,0]\nbeg = np.array([0.8, 0])+circle_center\nend = np.array([2.5, -0.8])+circle_center\n\nfinal_point, intersect = models_phenom._reflected_position(circle_center, circle_radius, beg, end)\n\nfig, ax = plt.subplots(figsize = (3, 3))\n\ncircle = plt.Circle(circle_center, circle_radius, facecolor = 'w', ec = 'C0', label = 'Compartment', zorder = -1)\nax.add_patch(circle)\nax.plot([beg[0],end[0]], [beg[1], end[1]], '-o', c = 'C1', label = 'Displacement segment')\nax.plot([circle_center[0], intersect[0]], [circle_center[1], intersect[1]], c = 'C2')\nax.plot([intersect[0], final_point[0]], [intersect[1], final_point[1]], '-o', c = 'C4', label = 'Resulting reflection')\nax.set_ylim(circle_center[1]-circle_radius*1.5, circle_center[1]+circle_radius*1.5)\nax.set_xlim(circle_center[0]-circle_radius*1.5, circle_center[0]+circle_radius*1.5)\nax.legend(fontsize = 8)\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#single-trajectory-generator-2",
    "href": "lib_nbs/models_phenom.html#single-trajectory-generator-2",
    "title": "models_phenom",
    "section": "Single trajectory generator",
    "text": "Single trajectory generator\n\nsource\n\n_confinement_traj\n\n _confinement_traj (T=200, L=100, Ds=[1, 0.1], alphas=[1, 1], r=1,\n                    comp_center=None, Nc=10, trans=0.1, deltaT=1)\n\nGenerates a 2D trajectory of particles diffusing in an environment with partially transmitting circular compartments.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nint\n200\nLength of the trajectory\n\n\nL\nint\n100\nLength of the box acting as the environment\n\n\nDs\nlist\n[1, 0.1]\nDiffusion coefficients of the two diffusive states (first free, the confined). Size must be 2.\n\n\nalphas\nlist\n[1, 1]\nAnomalous exponents of the two diffusive states (first free, the confined). Size must be 2.\n\n\nr\nint\n1\nRadius of the compartments.\n\n\ncomp_center\nNoneType\nNone\nIf given, center of the compartments. If None, centers are uniformly sampled.\n\n\nNc\nint\n10\nNumber of compartments\n\n\ntrans\nfloat\n0.1\nTransmittance of the boundaries\n\n\ndeltaT\nint\n1\nSampling time.\n\n\nReturns\ntuple\n\n- pos (array Tx2): particle’s position- labels (array Tx2): particle’s labels (see ._multi_state for details on labels)\n\n\n\n\nN = 50;  L = 20\nNc = 15; r = 1; L = 20\nDs = [1, 0.1]\nr , L, Nc = (20, 256, 20)\n\ncomp_center = models_phenom._distribute_circular_compartments(Nc = Nc, r = r, L = L)\ntrajs, labels = models_phenom()._confinement_traj(trans = 0.1, Nc = Nc, r = r, L = L, T =200, comp_center=comp_center, Ds = Ds)\n\nfig, axs = plt.subplots(1,2, figsize = (6,3), tight_layout = True)\n\nax = axs[0]\nfor c in comp_center:\n    circle = plt.Circle((c[0], c[1]), r, facecolor = 'None', edgecolor = 'C0')\n    ax.add_patch(circle)    \nax.scatter(trajs[:,0], trajs[:,1], c = plt.cm.cividis(labels[:,-1]/2), zorder = -1, s = 2)   \n\nplt.setp(axs[0], xlim = (0,L), ylim = (0,L), xlabel = 'X', ylabel = 'Y')\n\naxs[1].plot(trajs[:,0], label = 'x');\naxs[1].plot(trajs[:,1], label = 'y');\naxs[1].legend()\nplt.setp(axs[1], xlabel = 'Position', ylabel = 'Time')\n\n[Text(0.5, 0, 'Position'), Text(0, 0.5, 'Time')]"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#dataset-generation-2",
    "href": "lib_nbs/models_phenom.html#dataset-generation-2",
    "title": "models_phenom",
    "section": "Dataset generation",
    "text": "Dataset generation\n\nsource\n\nconfinement\n\n confinement (N=10, T=200, L=100, Ds=[[1, 0], [0.1, 0]], alphas=[[1, 0],\n              [1, 0]], gamma_d=[1], epsilon_a=[0], r=1, comp_center=None,\n              Nc=10, trans=0.1, deltaT=1)\n\nGenerates a dataset of 2D trajectories of particles diffusing in an environment with partially transmitting circular compartments.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n10\nNumber of trajectories\n\n\nT\nint\n200\nLength of the trajectory\n\n\nL\nint\n100\nLength of the box acting as the environment\n\n\nDs\nlist\n[[1, 0], [0.1, 0]]\nList of means and variances from which to sample the diffusion coefficient of each state. If element size is one, we consider variance = 0.\n\n\nalphas\nlist\n[[1, 0], [1, 0]]\nList of means and variances from which to sample the anomalous exponent of each state. If element size is one, we consider variance = 0.\n\n\ngamma_d\nlist\n[1]\nMinimum factor between D of diffusive states (see ._sampling_diff_parameters). Size is number of states -1 (in this case size 1)\n\n\nepsilon_a\nlist\n[0]\nDistance between alpha of diffusive states (see ._sampling_diff_parameters). Size is number of states -1 (in this case size 1)\n\n\nr\nint\n1\nRadius of the compartments.\n\n\ncomp_center\nNoneType\nNone\nIf given, center of the compartments. If None, centers are uniformly sampled.\n\n\nNc\nint\n10\nNumber of compartments\n\n\ntrans\nfloat\n0.1\nTransmittance of the boundaries\n\n\ndeltaT\nint\n1\nSampling time.\n\n\nReturns\ntuple\n\n- pos (array Tx2): particle’s position- labels (array Tx2): particle’s labels (see ._multi_state for details on labels)\n\n\n\n\nN = 50;  L = 20\nNc = 15; r = 1; L = 20\nDs = [[1,0], [150,0.1]]\nalphas = [[1, 0], [1.5,0.1]]\nr , L, Nc = (20, 256, 15)\ncomp_center = models_phenom._distribute_circular_compartments(Nc = Nc, r = r, L = L)\ntrajs, labels = models_phenom().confinement(N = N, L = L, comp_center = comp_center, trans = 0.1, Ds = Ds, \n                                            r = r, alphas = alphas, epsilon_a = [0])\n\n\nplot_trajs(trajs, L, N, labels = labels, plot_labels = True, comp_center = comp_center, r_cercle=r)"
  },
  {
    "objectID": "lib_nbs/datasets_phenom.html",
    "href": "lib_nbs/datasets_phenom.html",
    "title": "datasets_phenom",
    "section": "",
    "text": "source\n\ndatasets_phenom\n\n datasets_phenom (models_class=<andi_datasets.models_phenom.models_phenom\n                  object at 0x7f95753da050>)\n\nThis class generates, saves and loads datasets of trajectories simulated from various phenomenological diffusion models (available at andi_datasets.models_phenom).\n\nsource\n\n\ncreate_dataset\n\n create_dataset (dics:list|dict|bool=None, T:None|int=None,\n                 N_model:None|int=None, path:str='', save:bool=False,\n                 load:bool=False)\n\nGiven a list of dictionaries, generates trajectories of the demanded properties. The only compulsory input for every dictionary is model, i.e. the model from which trajectories must be generated. The rest of inputs are optional. You can see the input parameters of the different models in andi_datasets.models_phenom, This function checks and handles the input dataset and the manages both the creation, loading and saving of trajectories.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndics\nlist | dict | bool\nNone\n- if list or dictionary: the function generates trajectories with the properties stated in each dictionary.- if bool: the function generates trajectories with default parameters set for the ANDI 2 challenge (phenom) for every available diffusion model.\n\n\nT\nNone | int\nNone\n- if int: overrides the values of trajectory length in the dictionaries.- if None: uses the trajectory length values in the dictionaries. Caution: the minim T of all dictionaries will be considered!\n\n\nN_model\nNone | int\nNone\n- if int: overrides the values of number of trajectories in the dictionaries.- if None: uses the number of trajectories in the dictionaries\n\n\npath\nstr\n\nPath from where to save or load the dataset.\n\n\nsave\nbool\nFalse\nIf True, saves the generated dataset (see self._save_trajectories).\n\n\nload\nbool\nFalse\nIf True, loads a dataset from path (see self._load_trajectories).\n\n\nReturns\ntuple\n\n- trajs (array TxNx2): particles’ position. N considers here the sum of all trajectories generated from the input dictionaries. Note: if the dimensions of all trajectories are not equal, then trajs is a list.- labels (array TxNx2): particles’ labels (see ._multi_state for details on labels) \n\n\n\nIn the example below we create two dictionaries and generate a dataset with it. See the corresponding tutorial for more details.\n\nL = 50\ndict_model3 = {'model': 'dimerization', \n               'L': L,\n               'Pu': 0.1, 'Pb': 1}\ndict_model5 = {'model': 'confinement',\n               'L': L, \n               'trans': 0.2}\n\ndict_all = [dict_model3, dict_model5]\n\ntrajs, labels = datasets_phenom().create_dataset(N_model = 10, # number of trajectories per model\n                                                 T = 200,\n                                                 dics = dict_all\n                                                )\nplot_trajs(trajs, L , N = 10, \n           num_to_plot = 3,\n           labels = labels,\n           plot_labels = True\n          )\n\nFalse\nFalse\n\n\n\n\n\n\n\nCreating, saving and loading trajectories\nThese auxiliary functions used in create_trajectories that allow for manipulate trajectories in various forms.\n\nsource\n\n_create_trajectories\n\n _create_trajectories ()\n\nGiven a list of dictionaries, generates trajectories of the demanded properties. First checks in the .csv of each demanded model if a dataset of similar properties exists. If it does, it loads it from the corresponding file.\n\nL = 20\ndict_1 = {'model': 'single_state', \n          'L': L}\ndict_2 = {'model': 'immobile_traps', \n               'L': L}\ndict_all = [dict_1, dict_2]\n\nDP = datasets_phenom()\ntrajs, labels = DP.create_dataset(N_model = 13, # number of trajectories per model\n                                 T = 20,\n                                 dics = dict_all                                            \n                                )\nplot_trajs(trajs, L , N = 10, \n           num_to_plot = 3,\n           labels = labels,\n           plot_labels = True\n          )\n\n\n\n\n\nsource\n\n\n_save_trajectories\n\n _save_trajectories (trajs, labels, dic, df, dataset_idx, path)\n\nGiven a set of trajectories and labels, saves two things:\n- In the .csv corresponding to the demanded model, all the input parameters of the generated dataset. This allows to keed that of what was created before. - In a .npy file, the trajectories and labels generated.\n\ntrajs, labels = DP.create_dataset(N_model = 10, # number of trajectories per model\n                                     T = 20,\n                                     dics = dict_all,\n                                     save = True, path = 'datasets_folder/'\n                                    )\nplot_trajs(trajs, L , N = 3)\n\n\n\n\n\nsource\n\n\n_load_trajectories\n\n _load_trajectories (model_name, dataset_idx, path)\n\nGiven the path for a dataset, loads the trajectories and labels\n\n# You must run to cells above for this one to work. Check that this are the \n# exact same trajectories.\ntrajs, labels = DP.create_dataset(N_model = 10, # number of trajectories per model\n                                                 T = 20,\n                                                 dics = dict_all[0],\n                                                 load = True, path = 'datasets_folder/'\n                                                )\nplot_trajs(trajs, L , N = 3 )\n\n\n\n\n\n\n\nManaging parameters and dictionaries\n\ndictm = {'model': 'immobile_traps', \n               'L': 10}\n\n\nDP = datasets_phenom()\nDP.N_model = 10\nDP.T = 20\nDP.load = True\nDP.save = False\nDP.path = 'datasets_folder/'\n\n\ntry:\n    DP._inspect_dic(copy.deepcopy(dictm))\nexcept Exception as e: \n    print(e)\n\nThe dataset you want to load does not exist.\n\n\n/home/gorka/miniconda3/envs/andi/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n----------- in \nChecks the information of the input dictionaries so that they fulfil the constraints of the program , completes missing information\nwith default values and then decides about loading/saving depending on parameters....\n  else: warn(msg)\n\nsource\n\n_inspect_dic\n\n _inspect_dic (dic)\n\nChecks the information of the input dictionaries so that they fulfil the constraints of the program , completes missing information with default values and then decides about loading/saving depending on parameters.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndic\ndict\nDictionary with the information of the trajectories we want to generate\n\n\nReturns\ntuple\ndf: dataframe collecting the information of the dataset to load.dataset_idx: location in the previous dataframe of the particular dataset we want to generate.\n\n\n\n\nsource\n\n\n_get_args\n\n _get_args (model, return_defaults=False)\n\nGiven the name of a diffusion model, return its inputs arguments.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nstr\n\nName of the diffusion model (see self.available_models_name)\n\n\nreturn_defaults\nbool\nFalse\nIf True, the function will also return the default values of each input argument.\n\n\nReturns\ntuple\n\nargs (list): list of input arguments.defaults (optional, list): list of default value for the input arguments."
  },
  {
    "objectID": "lib_nbs/utils_challenge.html",
    "href": "lib_nbs/utils_challenge.html",
    "title": "utils_challenge",
    "section": "",
    "text": "These functions are used to smooth a given vector of labels of heterogeneous processes by means of majority filter. It allows to define a minimum segment length.\n\nsource\n\n\n\n label_filter (label, window_size=5, min_seg=3)\n\nGiven a vector of changing labels, applies a majority filter such that the minimum segment of a particular label is bigger than the minimum set segment.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlabel\nlist\n\nlabel vector to filter.\n\n\nwindow_size\nint\n5\nSize of the window in which the majority filter is applied.\n\n\nmin_seg\nint\n3\nMinimum segment size after filtering.\n\n\nReturns\nnp.array\n\nFiltered label vector\n\n\n\n\nsource\n\n\n\n\n majority_filter (seq, width)\n\nGiven a vector, applies a majority filter of given width.\n\n\n\n\nType\nDetails\n\n\n\n\nseq\nlist\nVector to filter.\n\n\nwidth\nint\nSize of the window in which the filter is applied.\n\n\nReturns\nlist\nFiltered vector\n\n\n\n\nfig, axs = plt.subplots(3, 3, figsize = (9, 5), tight_layout = True)\nwindow_size = 5\n\nfor ax in axs.flatten():    \n    traj, labs = models_phenom()._multiple_state_traj(alphas = [0.7, 0.8], Ds = [0.01, 0.1])\n    filtered_d = label_filter(labs[:,1],\n                              window_size = window_size)\n    \n    ax.plot(labs[:, 1], '.', label = 'True label')\n    ax.plot(filtered_d, label = r'Filtered label')\n    \naxs[0,0].set_title(f'Majority filter with window size = {window_size}')\naxs[0,0].legend()\nplt.setp(axs, xticklabels = [], yticklabels = []);\n\n\n\n\n\n\n\nNote that smoothing the signal will have an effect on the actual proportion of time a particle spends in each state. This will be taken into account in the challenge. Here we showcase this effect:\n\nT = 100\ntraj, labs = models_phenom().multi_state(N = 500, alphas = [[0.7, 1],[0.4,2]], Ds = [[0, 1], [1, 0]], T = T)\n\n\nres_t = np.array([])\nres_ft = np.array([])\nfor label in tqdm(labs.transpose(1,0,2)[:,:,0]):\n    \n    # raw labels\n    CP = np.argwhere(label[1:] != label[:-1]).flatten()\n    if CP[-1] != 199: CP = np.append(CP, T-1)\n    CP = np.append(0, CP)\n\n    res_t = np.append(res_t, CP[1:] - CP[:-1])\n    \n    \n    # filtered labels\n    filt = label_filter(label)\n    \n    CP_f = np.argwhere(filt[1:] != filt[:-1]).flatten()\n    if CP_f[-1] != 199: CP_f = np.append(CP_f, T-1)\n    CP_f = np.append(0, CP_f)\n\n    res_ft = np.append(res_ft, CP_f[1:] - CP_f[:-1])\n\n\n\n\nWe show now the new transition rates (e.g. 1 over the residence time of a given state). Because we are minimum segment length of 3, we can actually approximate the filtered transition rate as the original times 2/3:\n\nprint(f' True transition rate: {1/np.mean(res_t)}\\n',\n      f'Filtered transition rate: {1/np.mean(res_ft)}\\n',\n      f'True rate x 2/3: {1/np.mean(res_t)*(2/3)}')\n\n True transition rate: 0.10947474747474747\n Filtered transition rate: 0.07402020202020201\n True rate x 2/3: 0.07298316498316498\n\n\n\n\n\n\nThe labels in the challenge will be the list of \\(n\\) changepoints as well as the \\(n+1\\) diffusion properties (\\(D\\) and \\(\\alpha\\)) for each segment. This function transforms the stepwise labels into three lists: CPs, \\(\\alpha\\)s and \\(D\\)s.\n\nsource\n\n\n\n label_continuous_to_list (labs)\n\nGiven an array of T x 2 labels containing the anomalous exponent and diffusion coefficient at each timestep, returns 3 arrays, each containing the changepoints, exponents and coefficient, respectively. If labs is size T x 3, then we consider that diffusive states are given and also return those.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nlabs\narray\nT x 2 or T x 3 labels containing the anomalous exponent, diffusion and diffusive state.\n\n\nReturns\ntuple\n- First element is the list of change points- The rest are corresponding segment properties (order: alpha, Ds and states) \n\n\n\n\n# Generate the trajectory\ntrajs, labels = models_phenom().multi_state(N = 1, T = 50)\n\n# Transform the labels:\nCP, alphas, Ds, _ = label_continuous_to_list(labels[:,-1,:])\n\nplt.figure(figsize=(5, 3))\nplt.plot(labels[:, -1, 1], 'o', alpha = 0.4, label = 'Continuous label')\nplt.scatter(CP-1, Ds, c = 'C1', label = 'CP-1 and value of previous segment')\nplt.legend(); plt.xlabel('T'); plt.ylabel(r'$\\alpha$')\n\nText(0, 0.5, '$\\\\alpha$')\n\n\n\n\n\n\n\n\n\nThis function does the opposite from than label_continuous_to_list. From a list of properties as the one used in ANDI 2 challenge, creates continuous labels.\n\nsource\n\n\n\n label_list_to_continuous (CP, label)\n\nGiven a list of change points and the labels of the diffusion properties of the resulting segments, generates and array of continuous labels. The last change point indicates the array length.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nCP\narray, list\nlist of change points. Last change point indicates label length.\n\n\nlabel\narray, list\nlist of segment properties\n\n\nReturns\narray\nContinuous label created from the given change points and segment properties\n\n\n\n\nCP = [3,24,34]\nlabel = [0.5, 0.4, 1]\ncont = label_list_to_continuous(CP, label)\nplt.figure(figsize = (3,1))\nplt.plot(cont, c = 'C1')\n[plt.axvline(c, c = 'k', ls = '--') for c in CP[:-1]];\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n array_to_df (trajs, labels, min_length=10, fov_origin=[0, 0],\n              fov_length=100.0, cutoff_length=10)\n\nGiven arrays for the position and labels of trajectories, creates a dataframe with that data. The function also applies the demanded FOV. If you don’t want a field of view, chose a FOV length bigger (smaller) that your maximum (minimum) trajectory position.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrajs\narray\n\nTrajectories to store in the df (dimension: T x N x 3)\n\n\nlabels\narray\n\nLabels to store in the df (dimension: T x N x 3)\n\n\nmin_length\nint\n10\n\n\n\nfov_origin\nlist\n[0, 0]\nBottom left point of the square defining the FOV.\n\n\nfov_length\nfloat\n100.0\nSize of the box defining the FOV.\n\n\ncutoff_length\nint\n10\nMinimum length of a trajectory inside the FOV to be considered in the output dataset.\n\n\nReturns\ntuple\n\n- df_in (dataframe): dataframe with trajectories- df_out (datafram): dataframe with labels \n\n\n\n\n#trajs, labels = models_phenom().multi_state(T = 200, N = 10, alphas=[0.5, 1], Ds = [1,1], L = 100)\ntrajs, labels = models_phenom().single_state(T = 200, N = 10)\n\n# Changing dimensions\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_out = array_to_df(trajs, labels)\n\n\ndf_out.head()\n\n\n\n\n\n  \n    \n      \n      traj_idx\n      Ds\n      alphas\n      states\n      changepoints\n    \n  \n  \n    \n      0\n      0\n      [1.0]\n      [1.0]\n      [2.0]\n      [115]\n    \n    \n      1\n      1\n      [1.0]\n      [1.0]\n      [2.0]\n      [22]\n    \n    \n      2\n      2\n      [1.0]\n      [1.0]\n      [2.0]\n      [10]\n    \n    \n      3\n      3\n      [1.0]\n      [1.0]\n      [2.0]\n      [39]\n    \n    \n      4\n      4\n      [1.0]\n      [1.0]\n      [2.0]\n      [28]\n    \n  \n\n\n\n\n\n\n\n\n\nsource\n\n\n\n df_to_array (df, pad=-1)\n\nTransform a dataframe as the ones given in the ANDI 2 challenge (i.e. 4 columns: traj_idx, frame, x, y) into a numpy array. To deal with irregular temporal supports, we pad the array whenever the trajectory is not present. The output array has the typical shape of ANDI datasets: TxNx2\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ndataframe\n\nDataframe with four columns ‘traj_idx’: the trajectory index, ‘frame’ the time frame and ‘x’ and ‘y’ the positions of the particle.\n\n\npad\nint\n-1\nNumber to use as padding.\n\n\nReturns\narray\n\nArray containing the trajectories from the dataframe, with usual ANDI shape (TxNx2).\n\n\n\n\n\n\n\nThe outputs of datasets_challenge.challenge_phenom_dataset are not in the appropriate form if one considers the case of non-overlapping FOVS. The latter means that instead of taking n_fovs from the same experiment, we repeat the same experiment n_fovs times. This functions rearranges the folders to get the proper structure proposed in the paper.\n\nsource\n\n\n\n file_nonOverlap_reOrg (raw_folder, target_folder, experiments, num_fovs,\n                        tracks=[1, 2], save_labels=False, task=['single',\n                        'ensemble'], print_percentage=True)\n\nThis considers that you have n_fovs*n_experiments ‘fake’ experiments and organize them based on the challenge instructions\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_folder\n\n\noriginal folder with data produced by datasets_challenge.challenge_phenom_dataset\n\n\ntarget_folder\n\n\n\n\n\nexperiments\n\n\n\n\n\nnum_fovs\n\n\n\n\n\ntracks\nlist\n[1, 2]\n\n\n\nsave_labels\nbool\nFalse\nIf True, moves all data (also labels,.. etc). Do True only if saving reference / groundtruth data\n\n\ntask\nlist\n[‘single’, ‘ensemble’]\n\n\n\nprint_percentage\nbool\nTrue"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#setting-maximum-erros-for-different-metrics",
    "href": "lib_nbs/utils_challenge.html#setting-maximum-erros-for-different-metrics",
    "title": "utils_challenge",
    "section": "Setting maximum erros for different metrics",
    "text": "Setting maximum erros for different metrics"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#changepoint-pairing",
    "href": "lib_nbs/utils_challenge.html#changepoint-pairing",
    "title": "utils_challenge",
    "section": "Changepoint pairing",
    "text": "Changepoint pairing\nWe use an assignment algorithm to pair predicted and groundtruth changepoints. From there, we will calculate the various metrics of the challenge.\n\nsource\n\nchangepoint_assignment\n\n changepoint_assignment (GT, preds)\n\nGiven a list of groundtruth and predicted changepoints, solves the assignment problem via the Munkres algorithm (aka Hungarian algorithm) and returns two arrays containing the index of the paired groundtruth and predicted changepoints, respectively.\nThe distance between change point is the Euclidean distance.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nGT\nlist\nList of groundtruth change points.\n\n\npreds\nlist\nList of predicted change points.\n\n\nReturns\ntuple\n- tuple of two arrays, each corresponding to the assigned GT and pred changepoints- Cost matrix\n\n\n\n\nngts = 10; npreds = 6; T = 100\nGT = np.sort(np.random.choice(np.arange(1,T), ngts, replace = False))\npreds = np.sort(np.random.choice(np.arange(1,T)*0.5, npreds, replace = False)).astype(int)\nprint('GT:', GT)\nprint('Pred:', preds)\nchangepoint_assignment(GT, preds)[0]\n\nGT: [ 2  8 24 33 34 54 55 64 73 85]\nPred: [ 8 11 16 30 36 47]\n\n\n(array([0, 1, 2, 3, 4, 5], dtype=int64),\n array([1, 0, 2, 3, 4, 5], dtype=int64))\n\n\n\nsource\n\n\nchangepoint_alpha_beta\n\n changepoint_alpha_beta (GT, preds, threshold=10)\n\nCalculate the alpha and beta measure of paired changepoints. Inspired from Supplemantary Note 3 in https://www.nature.com/articles/nmeth.2808\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT\nlist\n\nList of groundtruth change points.\n\n\npreds\nlist\n\nList of predicted change points.\n\n\nthreshold\nint\n10\nDistance from which predictions are considered to have failed. They are then assigned this number.\n\n\nReturns\ntuple\n\nalpha, beta\n\n\n\n\nlabels = [r'Random Guess + $N_p>N_{gt}$',\n          r'Random Guess + $N_p<N_{gt}$',\n          r'GT + rand $\\in [-3, 3]$',\n          r'GT + rand $\\in [-1, 1]$']\n\nfig, ax = plt.subplots(figsize = (4,3))\nalpha = 0.2\n\nT = 200; ngts = 15; \n\nfor case, (label, color) in enumerate(zip(labels, ['C0', 'C1', 'C2', 'C3'])):\n\n    alphas, betas = [], []\n    for _ in range(100):\n        \n        GT = np.sort(np.random.choice(np.arange(1,T), ngts, replace = False))\n        if case == 0:\n            npreds = np.random.randint(low = ngts, high = ngts*2)\n            preds = np.sort(np.random.choice(np.arange(1,T), npreds, replace = False)) \n        elif case == 1:\n            npreds = np.random.randint(low = 1, high = ngts)\n            preds = np.sort(np.random.choice(np.arange(1,T), npreds, replace = False))     \n        elif case == 2:\n            preds = GT + np.random.randint(-3, 3, ngts)\n        elif case == 3:\n            preds = GT + np.random.randint(-1, 1, ngts)\n            \n        alpha, beta = changepoint_alpha_beta(GT, preds)\n        \n        alphas.append(alpha)\n        betas.append(beta)\n     \n    \n    ax.scatter(alphas, betas, c = color, alpha = alpha)\n    ax.scatter(np.mean(alphas), np.mean(betas), c = color, label = label, s = 50, marker = 's', edgecolors = 'k')\nplt.setp(ax, xlabel = r'$\\alpha$', ylabel = r'$\\beta$')\nax.legend(loc = (1.01,0.4))\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\nsource\n\n\njaccard_index\n\n jaccard_index (TP:int, FP:int, FN:int)\n\nGiven the true positive, false positive and false negative rates, calculates the Jaccard Index\n\n\n\n\nType\nDetails\n\n\n\n\nTP\nint\ntrue positive\n\n\nFP\nint\nfalse positive\n\n\nFN\nint\nfalse negative\n\n\nReturns\nfloat\nJaccard Index\n\n\n\n\nsource\n\n\nsingle_changepoint_error\n\n single_changepoint_error (GT, preds, threshold=5)\n\nGiven the groundtruth and predicted changepoints for a single trajectory, first solves the assignment problem between changepoints, then calculates the RMSE of the true positive pairs and the Jaccard index.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT\nlist\n\nList of groundtruth change points.\n\n\npreds\nlist\n\nList of predicted change points.\n\n\nthreshold\nint\n5\nDistance from which predictions are considered to have failed. They are then assigned this number.\n\n\nReturns\ntuple\n\n- TP_rmse: root mean square error of the true positive change points.- Jaccard Index of the ensemble predictions \n\n\n\n\nsource\n\n\nensemble_changepoint_error\n\n ensemble_changepoint_error (GT_ensemble, pred_ensemble, threshold=5)\n\nGiven an ensemble of groundtruth and predicted change points, iterates over each trajectory’s changepoints. For each, it solves the assignment problem between changepoints. Then, calculates the RMSE of the true positive pairs and the Jaccard index over the ensemble of changepoints (i.e. not the mean of them w.r.t. to the trajectories)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT_ensemble\nlist, array\n\nEnsemble of groutruth change points.\n\n\npred_ensemble\nlist\n\nEnsemble of predicted change points.\n\n\nthreshold\nint\n5\nDistance from which predictions are considered to have failed. They are then assigned this number.\n\n\nReturns\ntuple\n\n- TP_rmse: root mean square error of the true positive change points.- Jaccard Index of the ensemble predictions\n\n\n\n\nlabels = ['Random Guess + Incorrect number',\n          r'GT + rand $\\in [-3, 3]$',\n          r'GT + rand $\\in [-1, 1]$']\n\nfig, ax = plt.subplots(figsize = (3,3))\nalpha = 0.2\n\nT = 200; ngts = 10; npreds = 8\n\nfor case, (label, color) in enumerate(zip(labels, ['C0', 'C1', 'C2'])):\n    \n    rmse, ji = [], []\n    GT, preds = [], []\n    for _ in range(100):\n\n        GT.append(np.sort(np.random.choice(np.arange(1,T), ngts, replace = False)))\n        if case == 0:\n            preds.append(np.sort(np.random.choice(np.arange(1,T), npreds, replace = False)))                  \n        elif case == 1:\n            preds.append(GT[-1] + np.random.randint(-3, 3, ngts))\n        elif case == 2:\n            preds.append(GT[-1] + np.random.randint(-1, 1, ngts))\n\n        assignment, _ = changepoint_assignment(GT[-1], preds[-1])\n        assignment = np.array(assignment)\n\n        RMSE, JI = single_changepoint_error(GT[-1], preds[-1], threshold = 5)     \n        \n        rmse.append(RMSE)\n        ji.append(JI)\n\n    rmse_e, ji_e = ensemble_changepoint_error(GT, preds, threshold = 5)\n    \n    ax.scatter(rmse, ji, c = color, alpha = alpha)\n    ax.scatter(rmse_e, ji_e, c = color, label = label, s = 50, marker = 's', edgecolors = 'k')\nplt.setp(ax, xlabel = 'TP RMSE', ylabel = 'Jaccard')\nax.legend(loc = (0.91,0.4))\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#segments-pairing",
    "href": "lib_nbs/utils_challenge.html#segments-pairing",
    "title": "utils_challenge",
    "section": "Segments pairing",
    "text": "Segments pairing\nHere we focus on pairing the segments arising from a list of changepoints. We will use this to latter compare the predicted physical properties for each segment\n\nsource\n\ncreate_binary_segment\n\n create_binary_segment (CP:list, T:int)\n\nGiven a set of changepoints and the lenght of the trajectory, create segments which are equal to one if the segment takes place at that position and zero otherwise.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nCP\nlist\nlist of changepoints\n\n\nT\nint\nlength of the trajectory\n\n\nReturns\nlist\nlist of arrays with value 1 in the temporal support of the current segment.\n\n\n\n\nT= 50\nGT = np.sort(np.random.choice(np.arange(1,T), 10, replace = False))\nplt.figure(figsize = (4,3))\nfor idx, x in enumerate(create_binary_segment(GT, T)):\n    plt.plot(x*idx, 'o')\n\n\n\n\n\nsource\n\n\njaccard_between_segments\n\n jaccard_between_segments (gt, pred)\n\nGiven two segments, calculates the Jaccard index between them by considering TP as correct labeling, FN as missed events and FP leftover predictions.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ngt\narray\ngroundtruth segment, equal to one in the temporal support of the given segment, zero otherwise.\n\n\npred\narray\npredicted segment, equal to one in the temporal support of the given segment, zero otherwise.\n\n\nReturns\nfloat\nJaccard index between the given segments.\n\n\n\n\nsource\n\n\nsegment_assignment\n\n segment_assignment (GT, preds, T:int=None)\n\nGiven a list of groundtruth and predicted changepoints, generates a set of segments. Then constructs a cost matrix by calculting the Jaccard Index between segments. From this cost matrix, we solve the assignment problem via the Munkres algorithm (aka Hungarian algorithm) and returns two arrays containing the index of the groundtruth and predicted segments, respectively.\nIf T = None, then we consider that GT and preds may have different lenghts. In that case, the end of the segments is the the last CP of each set of CPs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT\nlist\n\nList of groundtruth change points.\n\n\npreds\nlist\n\nList of predicted change points.\n\n\nT\nint\nNone\nLength of the trajectory. If None, considers different GT and preds length.\n\n\nReturns\ntuple\n\n- tuple of two arrays, each corresponding to the assigned GT and pred changepoints- Cost matrix calculated via JI of segments \n\n\n\n\nExamples\nPredictions close to groundtruth\n\nT = 200; \nngts = 10; \nGT = np.sort(np.random.choice(np.arange(1,T), ngts, replace = False))\npreds = np.sort(GT + np.random.randint(-5, 5, 1) )\n\nseg_GT = create_binary_segment(GT, T)\nseg_preds = create_binary_segment(preds, T)   \n\n[row_ind, col_ind], cost_matrix = segment_assignment(GT, preds, T)\n\nfig, axs = plt.subplots(2, 5, figsize = (15, 6))\nfor r, c, ax in zip(row_ind, col_ind, axs.flatten()):\n    ax.set_title(f'1 - JI = {np.round(cost_matrix[r, c], 2)}')\n    ax.plot(seg_GT[r], label = 'Groundtruth')\n    ax.plot(seg_preds[c], label = 'Prediction')\naxs[0,0].legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nDifferent size between predictions and trues\n\nT1 = 200; T2 = 100\nngts = 10; \nGT = np.sort(np.random.choice(np.arange(1,T1), ngts, replace = False))\npreds = np.sort(np.random.choice(np.arange(1,T2), 5, replace = False))\n\nseg_GT = create_binary_segment(GT, T1)\nseg_preds = create_binary_segment(preds, T2)   \n\n[row_ind, col_ind], cost_matrix = segment_assignment(GT, preds)\n\nfig, axs = plt.subplots(1, 5, figsize = (15, 3))\nfor r, c, ax in zip(row_ind, col_ind, axs.flatten()):\n    ax.set_title(f'1 - JI = {np.round(cost_matrix[r, c], 2)}')\n    ax.plot(seg_GT[r], label = 'Groundtruth')\n    ax.plot(seg_preds[c], label = 'Prediction')\naxs[0].legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nPredictions very different to groundtruth\n\nT = 200;\nngts = 5; npreds = 5;\nGT = np.sort(np.random.choice(np.arange(1,T), ngts, replace = False))\npreds = np.sort(np.random.choice(np.arange(1,T), npreds, replace = False))  \n\nseg_GT = create_binary_segment(GT, T)\nseg_preds = create_binary_segment(preds, T)\n\n[row_ind, col_ind], cost_matrix = segment_assignment(GT, preds, T)\n\nfig, axs = plt.subplots(1, 5, figsize = (15, 3))\nfor r, c, ax in zip(row_ind, col_ind, axs.flatten()):\n    ax.set_title(f'1 - JI = {np.round(cost_matrix[r, c], 2)}')\n    ax.plot(seg_GT[r], label = 'Groundtruth')\n    ax.plot(seg_preds[c], label = 'Prediction')\naxs[0].legend()\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#segment-properties-comparison",
    "href": "lib_nbs/utils_challenge.html#segment-properties-comparison",
    "title": "utils_challenge",
    "section": "Segment properties comparison",
    "text": "Segment properties comparison\nWe use the segment pairing functions that we have defined above to compute various metrics between the properties of predicted and groundtruth segments.\n\nMetrics of segment properties\n\nsource\n\n\nmetric_diffusive_state\n\n metric_diffusive_state (gt=None, pred=None)\n\nCompute the F1 score between diffusive states.\n\nsource\n\n\nmetric_diffusion_coefficient\n\n metric_diffusion_coefficient (gt=None, pred=None, threshold_min=1e-12,\n                               max_error=190.86835960820298)\n\nCompute the mean squared log error (msle) between diffusion coefficients. Checks the current bounds of diffusion from models_phenom to calculate the maximum error.\n\nsource\n\n\nmetric_anomalous_exponent\n\n metric_anomalous_exponent (gt=None, pred=None, max_error=1.999)\n\nCompute the mean absolute error (mae) between anomalous exponents. Checks the current bounds of anomalous exponents from models_phenom to calculate the maximum error.\n\nx = np.random.rand(100)\ny = np.random.rand(100)\n\n\nmetric_diffusion_coefficient(x+2,y+2, threshold_min=-2)\n\n0.014261449910975834\n\n\n\n\nPairing and metrics calculation\n\nsource\n\n\ncheck_no_changepoints\n\n check_no_changepoints (GT_cp, GT_alpha, GT_D, GT_s, preds_cp,\n                        preds_alpha, preds_D, preds_s, T:bool|int=None)\n\nGiven predicionts over changepoints and variables, checks if in both GT and preds there is an absence of change point. If so, takes that into account to pair variables.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT_cp\nlist, int, float\n\nGroundtruth change points\n\n\nGT_alpha\nlist, float\n\nGroundtruth anomalous exponent\n\n\nGT_D\nlist, float\n\nGroundtruth diffusion coefficient\n\n\nGT_s\nlist, float\n\nGroundtruth diffusive state\n\n\npreds_cp\nlist, int, float\n\nPredicted change points\n\n\npreds_alpha\nlist, float\n\nPredicted anomalous exponent\n\n\npreds_D\nlist, float\n\nPredicted diffusion coefficient\n\n\npreds_s\nlist, float\n\nPredicted diffusive state\n\n\nT\nbool | int\nNone\n(optional) Length of the trajectories. If none, last change point is length.\n\n\nReturns\ntuple\n\n- False if there are change points. True if there were missing change points.- Next three are either all Nones if change points were detected, or paired exponents, coefficient and states if some change points were missing.\n\n\n\n\nsource\n\n\nsegment_property_errors\n\n segment_property_errors (GT_cp, GT_alpha, GT_D, GT_s, preds_cp,\n                          preds_alpha, preds_D, preds_s,\n                          return_pairs=False, T=None)\n\nGiven predicionts over change points and the value of diffusion parameters in the generated segments, computes the defined metrics.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT_cp\nlist, int, float\n\nGroundtruth change points\n\n\nGT_alpha\nlist, float\n\nGroundtruth anomalous exponent\n\n\nGT_D\nlist, float\n\nGroundtruth diffusion coefficient\n\n\nGT_s\nlist, float\n\nGroundtruth diffusive state\n\n\npreds_cp\nlist, int, float\n\nPredicted change points\n\n\npreds_alpha\nlist, float\n\nPredicted anomalous exponent\n\n\npreds_D\nlist, float\n\nPredicted diffusion coefficient\n\n\npreds_s\nlist, float\n\nPredicted diffusive state\n\n\nreturn_pairs\nbool\nFalse\nIf True, returns the assigment pairs for each diffusive property.\n\n\nT\nNoneType\nNone\n(optional) Length of the trajectories. If none, last change point is length.\n\n\nReturns\ntuple\n\n- if return_pairs = True, returns the assigned pairs of diffusive properties- if return_pairs = False, returns the errors for each diffusive property\n\n\n\nWe generate some random predictions to check how the metrics behave. We consider errors also in the change point predictions, hence there will be some segment mismatchings, which will affect the diffusive properties predictions:\n\nT = 200; \nngts = 10; \nerrors_alpha = np.linspace(0, 1, ngts)\nerrors_d = np.linspace(0, 10, ngts)\nerrors_s = np.linspace(0, 1, ngts)\n\nmetric_a, metric_d, metric_s = [], [], []\nfor error_a, error_d, error_s in zip(errors_alpha, errors_d, errors_s):\n    la, ld, ls = [], [], []\n    for _ in range(100):\n\n        GT_cp = np.sort(np.random.choice(np.arange(1,T-1), ngts, replace = False))\n        preds_cp = np.sort(np.random.choice(np.arange(1,T-1), ngts, replace = False)) \n\n        GT_alpha = np.random.rand(GT_cp.shape[0]+1)\n        preds_alpha = GT_alpha + np.random.randn(preds_cp.shape[0]+1)*error_a\n\n        GT_D = np.abs(np.random.randn(GT_cp.shape[0]+1)*10)\n        preds_D = GT_D + np.abs(np.random.randn(preds_cp.shape[0]+1))*error_d\n        \n        GT_s = np.random.randint(0, 5, GT_cp.shape[0]+1)\n        coin = np.random.rand(len(GT_s))\n        preds_s = GT_s.copy()\n        preds_s[coin < error_s] = np.random.randint(0, 5, len(coin[coin < error_s]))\n\n        m_a, m_d, m_s = segment_property_errors(GT_cp, GT_alpha, GT_D, GT_s, preds_cp, preds_alpha, preds_D, preds_s, T = T)\n        \n        la.append(m_a); ld.append(m_d); ls.append(m_s)\n    \n    metric_a.append(np.mean(la))\n    metric_d.append(np.mean(ld))    \n    metric_s.append(np.mean(ls))\n\nWith no error in the changepoint predicitions:\n\nfig, ax = plt.subplots(1, 3, figsize = (9, 3), tight_layout = True)\n\nax[0].plot(np.arange(ngts), errors_alpha, c = 'C0', ls = '--', label = 'Expected with no assigment error')\nax[0].plot(np.arange(ngts), metric_a, c = 'C0')\nax[0].set_title(r'Error in $\\alpha$ (MAE)')\n\n#ax[1].plot(np.arange(ngts), errors_d, c = 'C1', ls = '--')\nax[1].plot(np.arange(ngts), metric_d, c = 'C1')\nax[1].set_title(r'Error in $D$ (MSLE)')\n\nax[2].plot(np.arange(ngts), metric_s, c = 'C1')\nax[2].set_title(r'Error in states (JI)')\n\nplt.setp(ax, xlabel = 'Error magnitude')\n\n[Text(0.5, 0, 'Error magnitude'),\n Text(0.5, 0, 'Error magnitude'),\n Text(0.5, 0, 'Error magnitude')]\n\n\n\n\n\nWith error in the changepoint predicitions:\n\nfig, ax = plt.subplots(1, 3, figsize = (9, 3), tight_layout = True)\n\nax[0].plot(np.arange(ngts), errors_alpha, c = 'C0', ls = '--', label = 'Expected with no assigment error')\nax[0].plot(np.arange(ngts), metric_a, c = 'C0')\nax[0].set_title(r'Error in $\\alpha$ (MAE)')\n\n#ax[1].plot(np.arange(ngts), errors_d, c = 'C1', ls = '--')\nax[1].plot(np.arange(ngts), metric_d, c = 'C1')\nax[1].set_title(r'Error in $D$ (MSLE)')\n\nax[2].plot(np.arange(ngts), metric_s, c = 'C1')\nax[2].set_title(r'Error in states (JI)')\n\nplt.setp(ax, xlabel = 'Error magnitude')\n\n[Text(0.5, 0, 'Error magnitude'),\n Text(0.5, 0, 'Error magnitude'),\n Text(0.5, 0, 'Error magnitude')]"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#ensemble-metrics",
    "href": "lib_nbs/utils_challenge.html#ensemble-metrics",
    "title": "utils_challenge",
    "section": "Ensemble metrics",
    "text": "Ensemble metrics\n\nGet ensemble information\n\nsource\n\n\nextract_ensemble\n\n extract_ensemble (state_label, dic)\n\nGiven an array of the diffusive state and a dictionary with the diffusion information, returns a summary of the ensemble properties for the current dataset.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nstate_label\narray\nArray containing the diffusive state of the particles in the dataset.For multi-state and dimerization, this must be the number associated to thestate (for dimerization, 0 is free, 1 is dimerized). For the rest, we followthe numeration of models_phenom().lab_state.\n\n\ndic\ndict\nDictionary containing the information of the input dataset.\n\n\nReturns\narray\nMatrix containing the ensemble information of the input dataset. It has the following shape: |mu_alpha1 mu_alpha2 … | |sigma_alpha1 sigma_alpha2 … | |mu_D1 mu_D1 … |  |sigma_D1 sigma_D2 … | |counts_state1 counts_state2 … |\n\n\n\n\n\nGenerate distribution and distances\n\nsource\n\n\nmultimode_dist\n\n multimode_dist (params, weights, bound, x, normalized=False,\n                 min_var=1e-09)\n\nGenerates a multimodal distribution with given parameters. Also accounts for single mode if weight is float or int.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nlist\n\nMean and variances of every mode.\n\n\nweights\nlist, float\n\nWeight of every mode. If float, we consider a single mode.\n\n\nbound\ntuple\n\nBounds (min, max) of the functions support.\n\n\nx\narray\n\nSupport upon which the distribution is created.\n\n\nnormalized\nbool\nFalse\n\n\n\nmin_var\nfloat\n1e-09\n\n\n\n\n\n# True distribution\nx = np.logspace(np.log10(models_phenom().bound_D[0]), \n                      np.log10(models_phenom().bound_D[1]), 100)\nweights = [0.0005,0.9]\nparams_true = [[0.0,0],[1.5,0.5]]\ntrue = multimode_dist(params_true, weights, bound = models_phenom().bound_D, x = x, normalized = False, min_var=1e-9)\nplt.semilogx(x, true)\n\n\n\n\n\nsource\n\n\ndistribution_distance\n\n distribution_distance (p:<built-infunctionarray>, q:<built-\n                        infunctionarray>, x:<built-infunctionarray>=None,\n                        metric='wasserstein')\n\nCalculates distance between two distributions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\narray\n\ndistribution 1\n\n\nq\narray\n\ndistribution 2\n\n\nx\narray\nNone\nsupport of the distributions (not needed for MAE)\n\n\nmetric\nstr\nwasserstein\ndistance metric (either ‘wasserstein’ or ‘mae’)\n\n\nReturns\nfloat\n\ndistance between distributions\n\n\n\n\nTests distance\n\nNormal scenario\n\nmeans = np.linspace(0, 2, 30)\nnormalize = False\nfig = plt.figure(figsize=(15, 4))\ngs = fig.add_gridspec(2, 10)\n\n# True distribution\nx = np.arange(0, 3, 0.01)\nparams = [[1.7,0.01]]\nweights = [1]\ntrue = multimode_dist(params, weights, bound = [0, 3], x = x, normalized = normalize)\n\nrange_x = (1,2)\nidx_range = np.argwhere((x>range_x[0]) & (x<range_x[1])).flatten()\n\n\nMSE = []\nwass = []\nfor idx, mean in enumerate(means):\n    params = [[mean, 0.01]]\n    weights = [1]\n    pred = multimode_dist(params, weights, bound = [0, 3], x = x, normalized = normalize)  \n    MSE.append(distribution_distance(true, pred, metric = 'mae'))  \n    wass.append(distribution_distance(true, pred, x))  \n    \n    if idx % 3 == 0:\n        \n        ax = fig.add_subplot(gs[0, int(idx/3)])\n        ax.plot(x, true, label = 'True')\n        ax.plot(x, pred, label = 'Predicted')        \n        plt.setp(ax, yticks = []);\n        \n      \n    if idx == 0:\n        ax.legend()\n        \n    ax.axvline(range_x[0])\n    ax.axvline(range_x[1])\n    \nax_dist = fig.add_subplot(gs[1, :])\nax_dist.plot(MSE, '-o', label = 'MAE')\nax_dist.plot(wass, '-o', label = 'wasserstein')\nax_dist.legend()\nplt.setp(ax_dist, ylabel = 'MAE')\nax_dist.grid()\n\n\n\n\n\n\nHaving a variance = 0 (as in immobile)\nWasserstein:\n\nmeans = np.linspace(0, 2, 30)\nnormalize = False\n\nfig = plt.figure(figsize=(15, 7))\ngs = fig.add_gridspec(4, 10)\n\n# True distribution\nx = np.arange(0, 3, 0.01)\nweights = [0.3, 0.9]\n\nparams_var0 = [[0,0.0],[1,0.1]]\ntrue_var0 = multimode_dist(params_var0, weights, bound = [0, 3], x = x, normalized = normalize)\n\nparams = [[0,0.1],[1,0.1]]\ntrue = multimode_dist(params, weights, bound = [0, 3], x = x, normalized = normalize)\n\nwass_var0 = []\nwass = []\nmae_var0 = []\nmae = []\nfor idx, mean in enumerate(means):\n    params = [[mean, 0.01]]\n    weights = [1]\n    pred = multimode_dist(params, weights, bound = [0, 3], x = x, normalized = normalize)  \n    wass_var0.append(distribution_distance(true_var0, pred, x))  \n    wass.append(distribution_distance(true, pred, x))  \n    \n    mae_var0.append(distribution_distance(true_var0, pred, metric = 'mae'))  \n    mae.append(distribution_distance(true, pred, metric = 'mae'))  \n    \n    if idx % 3 == 0:\n        \n        ax0 = fig.add_subplot(gs[0, int(idx/3)])\n        ax0.plot(x, np.log(true_var0), label = 'log(True)', c = 'C0')\n        ax0.plot(x, pred, label = 'Predicted', c = 'k')        \n        plt.setp(ax0, yticks = [], ylim = (-5, 5));\n        \n        ax = fig.add_subplot(gs[1, int(idx/3)])\n        ax.plot(x, true, label = 'True', c = 'C1')\n        ax.plot(x, pred, label = 'Predicted', c = 'k')        \n        plt.setp(ax, yticks = []);\n        \n      \n    if idx == 0:\n        ax0.legend()\n        ax.legend()\n\nax_wass = fig.add_subplot(gs[2, :])\nax_wass.plot(wass_var0, '-o', label = 'Var_0 = 0')\nax_wass.plot(wass, '-o', label = r'Var_0 $\\neq$ 0')\nax_wass.legend()\nax_wass.set_ylabel('wass dist')\n\nax_mse = fig.add_subplot(gs[3, :])\nax_mse.plot(mae_var0, '-o', label = 'Var_0 = 0')\nax_mse.plot(mae, '-o', label = r'Var_0 $\\neq$ 0')\nax_mse.set_yscale('log')\nax_mse.set_ylabel('mse dist')\n\n# ax_dist.grid()\n\nText(0, 0.5, 'mse dist')\n\n\n\n\n\n\n\nChecking how variance of predicted affects Wasserstein distance:\n\nvariances = np.logspace(-12, -1,300)\n# True distribution\nx = np.logspace(-12, 1, 10000)\nweights = [1]\nparams_true = [[0.0,0]]\ntrue = multimode_dist(params_true, weights, bound = [1e-9, 3], x = x, normalized = normalize, min_var=1e-7)\n\ndist = []\nfor idx, var in enumerate(variances):\n    params = [[0.5, var]]\n    weights = [1]\n    pred = multimode_dist(params, weights, bound = [1e-9, 3], x = x, normalized = normalize, min_var=1e-7)  \n    dist.append(distribution_distance(true, pred, x=x))  \n    \nplt.plot(variances, np.array(dist)+1, 'o')\nplt.axvline(params_true[0][1], c = 'k', label = 'True variance')\nplt.legend()\nplt.xscale('log')\nplt.yscale('log')\n\nplt.xlabel('Variance prediction')\nplt.ylabel('Wasserstein distance')\n\nText(0, 0.5, 'Wasserstein distance')\n\n\n\n\n\nChecking if we are considering a peak at 0\n\n# True distribution\nx = np.logspace(np.log10(models_phenom().bound_D[0]), \n                      np.log10(models_phenom().bound_D[1]), 100)\nweights = [0.0005,0.9]\nparams_true = [[0.0,0],[1.5,0.5]]\ntrue = multimode_dist(params_true, weights, bound = models_phenom().bound_D, x = x, normalized = normalize)\n\n\nplt.plot(x, true)\nplt.xscale('log')\n\n\n\n\nTesting maximum value of Wasserstein distance for considered \\(\\alpha\\) and \\(D\\) ranges\n\nfrom andi_datasets.utils_challenge import multimode_dist, distribution_distance\n\n\nmin_a, max_a = models_phenom().bound_alpha[0], models_phenom().bound_alpha[1]\nx = np.arange(min_a, max_a, 0.01)\nnormalize = False\n\n\ndistmax = multimode_dist([[max_a,0.0001]], [1], bound = [min_a, max_a], x = x, normalized = normalize)\ndistmin = multimode_dist([[min_a,0.0001]], [1], bound = [min_a, max_a], x = x, normalized = normalize)\n\n\ndistribution_distance(distmax, distmin, x)\n\n1.982486622823773\n\n\n\nmin_d, max_d = models_phenom().bound_D[0], models_phenom().bound_D[1]\nx = np.logspace(np.log10(models_phenom().bound_D[0]), \n                      np.log10(models_phenom().bound_D[1]), 100)\n\n\ndistmax = multimode_dist([[max_d,0.1]], [1], bound = [min_d, max_d], x = x, normalized = normalize)\ndistmin = multimode_dist([[min_d,0.01]], [1], bound = [min_d, max_d], x = x, normalized = normalize)\n\n\ndistribution_distance(distmax, distmin, x)\n\n-0.004896474885754287\n\n\n\n\n\n\nCalculate ensemble metric\n\nsource\n\n\nerror_Ensemble_dataset\n\n error_Ensemble_dataset (true_data, pred_data, size_support=1000,\n                         metric='wasserstein', return_distributions=False)\n\nCalculates the ensemble metrics for the ANDI 2 challenge. The input are matrices of shape:\n\n\n\ncol1 (state 1)\ncol2 (state 2)\ncol3 (state 3)\n…\n\n\n\n\n\\(\\mu_a^1\\)\n\\(\\mu_a^2\\)\n\\(\\mu_a^3\\)\n…\n\n\n\\(\\sigma_a^1\\)\n\\(\\sigma_a^2\\)\n\\(\\sigma_a^3\\)\n…\n\n\n\\(\\mu_D^1\\)\n\\(\\mu_D^2\\)\n\\(\\mu_D^3\\)\n…\n\n\n\\(\\sigma_D^1\\)\n\\(\\sigma_D^2\\)\n\\(\\sigma_D^3\\)\n…\n\n\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n…\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrue_data\narray\n\nMatrix containing the groundtruth data.\n\n\npred_data\narray\n\nMatrix containing the predicted data.\n\n\nsize_support\nint\n1000\nsize of the support of the distributions\n\n\nmetric\nstr\nwasserstein\nmetric used to calculate distance between distributions\n\n\nreturn_distributions\nbool\nFalse\nIf True, the function also outputs the generated distributions.\n\n\nReturns\ntuple\n\n- distance_alpha: distance between anomalous exponents- distance_D: distance between diffusion coefficients- dists (if asked): distributions of both groundtruth and predicted data. Order: true_a, true_D, pred_a, pred_D \n\n\n\n\ntrack = 1\n# Choose the paths\nPATH_PRED = f'../../testing/data/fourth_round/pred_carlo/Track{track}/'\nPATH_TRUE = '../../testing/data/fourth_round/true/fourth_round/'\n\n\n\ndistance_D, distance_alpha = [], []\nfor exp in [2]:\n    \n    true = np.loadtxt(PATH_TRUE+f'exp_{exp}/ensemble_labels.txt', skiprows = 1, delimiter = ';')\n    pred = np.loadtxt(PATH_PRED+f'exp_{exp}/ensemble_pred.txt', skiprows = 1, delimiter = ';')\n    \n    df_true = pandas.DataFrame(data = true.reshape(1,5) if exp == 1 else true.transpose(), \n                           columns = [r'mean $\\alpha$', r'var $\\alpha$', r'mean $D$', r'var $D$', '% residence time'])\n    df_pred = pandas.DataFrame(data = pred.reshape(1,5) if exp == 1 else pred.transpose(), \n                           columns = [r'mean $\\alpha$', r'var $\\alpha$', r'mean $D$', r'var $D$', '% residence time'])\n\n#     print(f'Experiment {exp}: \\nGroundtruth:')\n#     display(df_true)\n#     print('Prediction:')\n#     display(df_pred)\n#     print('------ \\n')\n\n    distance_D.append(distance_D_exp)\n    distance_alpha.append(distance_a_exp)\n    \n    distance_a_exp, distance_D_exp, dists = error_Ensemble_dataset(true, pred, return_distributions = True)\n\n\nprint(f'Distance distribution D = {np.mean(distance_D)}')\nprint(fr'Distance distribution $\\alpha$ = {np.mean(distance_alpha)}')\n\nDistance distribution D = 0.06045239400247898\nDistance distribution $\\alpha$ = 0.10273932574583697"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#single-trajectory-metrics",
    "href": "lib_nbs/utils_challenge.html#single-trajectory-metrics",
    "title": "utils_challenge",
    "section": "Single trajectory metrics",
    "text": "Single trajectory metrics\nThe participants will have to output predictions in a .txt file were each line corresponds to the predictions of a trajectory. The latter have to be ordered as:\n0, d\\(_0\\), a\\(_0\\), s\\(_0\\), t\\(_1\\), d\\(_1\\), a\\(_1\\), s\\(_1\\), t\\(_2\\), d\\(_2\\), a\\(_2\\), s\\(_2\\), …. t\\(_n\\), d\\(_n\\), a\\(_n\\), s\\(_n\\),\\(T\\)\nwhere the first number corresponds to the trajectory index, then d\\(_i\\), a\\(_i\\), s\\(_i\\) correspond to the diffusion coefficient, anomalous exponent and diffusive state of the \\(i\\)-th segment. For the latter, we have the following code: - 0: immobile - 1: confined - 2: free (unconstrained) - 3: directed\nLast, t\\(_j\\) corresponds to the \\(j\\)-th changepoints. The last changepoint \\(T\\) corresponds to the length of the trajectory. Each prediction must contain \\(C\\) changepoints and \\(C\\) segments property values. If this is not fulfilled, the whole trajectory is considered as mispredicted.\nThe .txt file will be first inspected. The data will then be collected into a dataframe\n\nsource\n\ncheck_prediction_length\n\n check_prediction_length (pred)\n\nGiven a trajectory segments prediction, checks whether it has C changepoints and C+1 segments properties values. As it must also contain the index of the trajectory, this is summarized by being multiple of 4. In some cases, the user needs to also predict the final point of the trajectory. In this case, we will have a residu of 1.\n\nsource\n\n\nseparate_prediction_values\n\n separate_prediction_values (pred)\n\nGiven a prediction over trjaectory segments, extracts the predictions for each segment property as well as the changepoint values.\n\nsource\n\n\nload_file_to_df\n\n load_file_to_df (path_file, columns=['traj_idx', 'Ds', 'alphas',\n                  'states', 'changepoints'])\n\nGiven the path of a .txt file, extract the segmentation predictions based on the rules of the ANDI 2 challenge022\nSaving fake data for test\n\nfile_gt, file_p = [], []\nT = 200; ngts = 10;\nfor traj in range(100):\n    GT_cp = np.sort(np.random.choice(np.arange(1,T), ngts, replace = False))\n    preds_cp = np.sort(np.random.choice(np.arange(1,T+50), ngts, replace = False)) \n\n    GT_alpha = np.random.rand(GT_cp.shape[0]+1)\n    preds_alpha = GT_alpha# + 0.1 #np.random.randn(preds_cp.shape[0]+1)*0.1\n\n    GT_D = np.abs(np.random.randn(GT_cp.shape[0]+1)*10)\n    preds_D = GT_D + 1.5 #np.abs(np.random.randn(preds_cp.shape[0]+1))*1.6\n    \n    GT_state = np.random.randint(0, high = 5, size = GT_cp.shape[0]+1)\n    preds_state = np.random.randint(0, high = 5, size = preds_cp.shape[0]+1)\n    \n    list_gt, list_p = [traj, GT_D[0], GT_alpha[0], GT_state[0]], [traj, preds_D[0], preds_alpha[0], preds_state[0]]\n    for gtc, gta, gtd, gts, pc, pa, pd, ps in zip(GT_cp, GT_alpha[1:], GT_D[1:], GT_state[1:], preds_cp, preds_alpha[1:], preds_D[1:], preds_state[1:]):\n        list_gt += [gtc, gtd, gta, gts]\n        list_p += [pc, pd, pa, ps]\n        \n    file_gt.append(list_gt)\n    if traj != 6:\n        file_p.append(list_p)\n        \npred_path, true_path = 'pred_test.txt', 'true_test.txt'\nnp.savetxt(true_path, file_gt, delimiter=',')\nnp.savetxt(pred_path, file_p, delimiter=',')\n\nRecovering the data\n\npred_path, true_path = 'pred_test.txt', 'true_test.txt'\n\ndf_pred = load_file_to_df(pred_path)\ndf_true = load_file_to_df(true_path)\n\n\nsource\n\n\nerror_SingleTraj_dataset\n\n error_SingleTraj_dataset (df_pred, df_true, threshold_error_alpha=None,\n                           max_val_alpha=2, min_val_alpha=0,\n                           threshold_error_D=None, max_val_D=1000000.0,\n                           min_val_D=1e-06, threshold_error_s=None,\n                           threshold_cp=None, prints=True,\n                           disable_tqdm=False)\n\nGiven two dataframes, corresponding to the predictions and true labels of a set of trajectories from the ANDI 2 challenge022, calculates the corresponding metrics Columns must be for both (no order needed): traj_idx | alphas | Ds | changepoints | states df_true must also contain a column ‘T’.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf_pred\ndataframe\n\nPredictions\n\n\ndf_true\ndataframe\n\nGroundtruth\n\n\nthreshold_error_alpha\nNoneType\nNone\n(same for D, s, cp) Maximum possible error allowed. If bigger, it is substituted by this error.\n\n\nmax_val_alpha\nint\n2\n(same for D, s, cp) Maximum value of the parameter.\n\n\nmin_val_alpha\nint\n0\n(same for D, s, cp) Minimum value of the parameter.\n\n\nthreshold_error_D\nNoneType\nNone\n\n\n\nmax_val_D\nfloat\n1000000.0\n\n\n\nmin_val_D\nfloat\n1e-06\n\n\n\nthreshold_error_s\nNoneType\nNone\n\n\n\nthreshold_cp\nNoneType\nNone\n\n\n\nprints\nbool\nTrue\n\n\n\ndisable_tqdm\nbool\nFalse\nIf True, disables the progress bar.\n\n\nReturns\ntuple\n\n- rmse_CP: root mean squared error change points- JI: Jaccard index change points- error_alpha: mean absolute error anomalous exponents- error_D: mean square log error diffusion coefficients- error_s: Jaccar index diffusive states\n\n\n\n\nTest\nTwo datasets with same number of trajs\n\ntrajs, labels = models_phenom().immobile_traps(T = 200, N = 250, alphas=0.5, Ds = 1, L = 20, Nt = 100, Pb = 1, Pu = 0.5)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_trues = array_to_df(trajs, labels)\n\ntrajs, labels = models_phenom().immobile_traps(T = 200, N = 250, alphas=[0.5, 0.1], Ds = 1, L = 20, Nt = 100, Pb = 1, Pu = 0.5)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_preds = array_to_df(trajs, labels)\n\n\n\n\n\n\n\n\nerror_SingleTraj_dataset(df_preds, df_trues, prints = True, disable_tqdm=True);\n\nSummary of metrics assesments:\n\nChangepoint Metrics \nRMSE: 4.285 \nJaccard Index: 0.421 \n\nDiffusion property metrics \nMetric anomalous exponent: 0.30181845745010744 \nMetric diffusion coefficient: 0.22899984105532623 \nMetric diffusive state: 0.5233668341708543\n\n\nTwo datasets with different number of trajectories\n\ntrajs, labels = models_phenom().immobile_traps(T = 200, N = 350, alphas=[0.5,0.01], Ds = [1., 0.1], L = 20, Nt = 100, Pb = 1, Pu = 0.5)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_trues = array_to_df(trajs, labels, label_values=[0.5, 1], diff_states=[3, 2])\n\ntrajs, labels = models_phenom().immobile_traps(T = 200, N = 250, alphas=[0.5, 0.1], Ds = 1, L = 20, Nt = 100, Pb = 1, Pu = 0.5)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_preds = array_to_df(trajs, labels, label_values=[0.5, 1], diff_states=[3, 2])\n\n\n\n\n\n\n\n\nerror_SingleTraj_dataset(df_preds, df_trues, prints = True)\n\n\n\n\nSummary of metrics assesments:\n\n100 missing trajectory/ies. \n\nChangepoint Metrics \nRMSE: 4.051 \nJaccard Index: 0.441 \n\nDiffusion property metrics \nMetric anomalous exponent: 0.35483874584715985 \nMetric diffusion coefficient: 3.1690909054732668 \nMetric diffusive state: 0.4913685263947961\n\n\n(4.050708208970335,\n 0.4407643312101911,\n 0.35483874584715985,\n 3.1690909054732668,\n 0.4913685263947961)\n\n\n\ntrajs, labels = models_phenom().immobile_traps(T = 200, N = 5, alphas=[0.5,0.01], Ds = [1., 0.1], L = 20, Nt = 100, Pb = 1, Pu = 0.5)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_preds  = array_to_df(trajs, labels, label_values=[0.5, 1], diff_states=[3, 2])\n\ntrajs, labels = models_phenom().multi_state(T = 200, N = 7, L = 20, M = np.array([[0.9,0.1],[0.9,0.1]]))\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_trues  = array_to_df(trajs, labels, label_values=[0.5, 1], diff_states=[3, 2])\n\n\n\n\n\n\n\n\nerror_SingleTraj_dataset(df_preds, df_trues, prints = True);\n\n\n\n\nSummary of metrics assesments:\n\n2 missing trajectory/ies. \n\nChangepoint Metrics \nRMSE: 2.903 \nJaccard Index: 0.188 \n\nDiffusion property metrics \nMetric anomalous exponent: 0.8269399281523714 \nMetric diffusion coefficient: 8.262443034681892 \nMetric diffusive state: 0.41379310344827586\n\n\nDataset with no changepoints\n\nL = 250\n# TRUES\ntrajs, labels = models_phenom().single_state(T = 200, N = 250, alphas=[0.5, 0.01], Ds = [1,0], L = L)\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_trues = array_to_df(trajs, labels, fov_length = L)\n\n# PREDS\ntrajs, labels = models_phenom().single_state(T = 200, N = 250, L = L)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_preds = array_to_df(trajs, labels, fov_length = L)\n\n\n\n\n\n\n\n\nerror_SingleTraj_dataset(df_preds, df_trues, prints = True, disable_tqdm=True);\n\nSummary of metrics assesments:\n\nChangepoint Metrics \nRMSE: 0 \nJaccard Index: 1 \n\nDiffusion property metrics \nMetric anomalous exponent: 0.5025766161725703 \nMetric diffusion coefficient: 0.0 \nMetric diffusive state: 1.0"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#single-trajectory",
    "href": "lib_nbs/utils_challenge.html#single-trajectory",
    "title": "utils_challenge",
    "section": "Single trajectory",
    "text": "Single trajectory\n\nsource\n\nrun_single_task\n\n run_single_task (exp_nums, track, submit_dir, truth_dir)\n\n\nsource\n\n\nwhen_error_single\n\n when_error_single (wrn_str)"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#ensemble",
    "href": "lib_nbs/utils_challenge.html#ensemble",
    "title": "utils_challenge",
    "section": "Ensemble",
    "text": "Ensemble\n\nsource\n\nrun_ensemble_task\n\n run_ensemble_task (exp_nums, track, submit_dir, truth_dir)"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#parent-program",
    "href": "lib_nbs/utils_challenge.html#parent-program",
    "title": "utils_challenge",
    "section": "Parent program",
    "text": "Parent program\n\nsource\n\ncodalab_scoring\n\n codalab_scoring (INPUT_DIR=None, OUTPUT_DIR=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nINPUT_DIR\nNoneType\nNone\ndirectory to where to find the reference and predicted labes\n\n\nOUTPUT_DIR\nNoneType\nNone\ndirectory where the scores will be saved (scores.txt)\n\n\n\n\nsource\n\n\nlistdir_nohidden\n\n listdir_nohidden (path)"
  },
  {
    "objectID": "lib_nbs/analysis.html",
    "href": "lib_nbs/analysis.html",
    "title": "analysis",
    "section": "",
    "text": "Angle calculation\n\nsource\n\ndataset_angles\n\n dataset_angles (trajs:list)\n\nGiven a set of trajectories, calculate all angles between displacements\n\n\n\n\nType\nDetails\n\n\n\n\ntrajs\nlist\nset of trajectories from which to calculate angles\n\n\nReturns\nlist\nlist of angles between displacements\n\n\n\n\nsource\n\n\nget_angle\n\n get_angle (a:tuple, b:tuple, c:tuple)\n\nCalculates the angle between the segments generate by three points\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\na\ntuple\n2d position point A\n\n\nb\ntuple\n2d position point B\n\n\nc\ntuple\n2d position point C\n\n\nReturns\ntuple\nangle between segments AB and BC points\n\n\n\n\n\n\nMSD based analysis\n\nsource\n\nmsd_analysis\n\n msd_analysis ()\n\nContains mean squared displacement (MSD) based methods to analyze trajectories.\n\nsource\n\n\nmsd_analysis.tamsd\n\n msd_analysis.tamsd (trajs:numpy.ndarray, t_lags:numpy.ndarray, dim=1)\n\nCalculates the time average mean squared displacement (TA-MSD) of a trajectory at various time lags,\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrajs\nndarray\n\nSet of trajectories of dimenions NxTxD (N: number of trajectories, T: lenght, D: dimension)\n\n\nt_lags\nndarray\n\nTime lags used for the TA-MSD\n\n\ndim\nint\n1\nDimension of the trajectories (currently only 1 and 2 supported)\n\n\nReturns\nnp.array\n\nTA-MSD of each trayectory / t_lag\n\n\n\n\nsource\n\n\nmsd_analysis.get_diff_coeff\n\n msd_analysis.get_diff_coeff (trajs:numpy.ndarray, t_lags:list=None)\n\nCalculates the diffusion coefficient of a trajectory by means of the linear fitting of the TA-MSD.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrajs\nndarray\n\n\n\n\nt_lags\nlist\nNone\nTime lags used for the TA-MSD.\n\n\nReturns\nnp.array\n\nDiffusion coefficient of the given trajectory. \n\n\n\nHere we show an example for the calculation of the diffusion coefficient of a 2D Brownian motion trajectory. We create trajectories from displacements of variance \\(\\sigma =1\\), which results in a diffusion coefficient \\(D=0.5\\).\n\npos = np.cumsum(np.random.randn(1070, 100, 2), axis = 1)\nD = msd_analysis().get_diff_coeff(pos)\n\nplt.hist(D, bins = 30);\nplt.axvline(np.mean(D), c = 'k', label = f'Mean of predictions = {np.round(np.mean(D), 2)}')                                                                  \nplt.axvline(0.5, c = 'r', ls = '--', label = 'Expected')\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\nsource\n\n\nmsd_analysis.get_exponent\n\n msd_analysis.get_exponent (trajs:numpy.ndarray, t_lags:list=None)\n\nCalculates the diffusion coefficient of a trajectory by means of the linear fitting of the TA-MSD.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrajs\nndarray\n\n\n\n\nt_lags\nlist\nNone\nTime lags used for the TA-MSD.\n\n\nReturns\nnp.array\n\nDiffusion coefficient of the given trajectory. \n\n\n\nTo showcase this function, we generate fractional brownian motion trajectories with \\(\\alpha = 0.5\\) and calculate their exponent:\n\ntrajs, _ = models_phenom().single_state(N = 1001, T = 100, alphas = 0.5, dim = 2)\nalpha = msd_analysis().get_exponent(trajs.transpose(1,0,2))\n\nplt.hist(alpha, bins = 30);\nplt.axvline(np.mean(alpha), c = 'k', label = f'Mean of predictions = {np.round(np.mean(alpha), 2)}')\nplt.axvline(0.5, c = 'r', ls = '--', label = 'Expected')\nplt.legend()\n\n(1001, 100, 2)\n\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\n\n\nVelocity Autocorrelation Function (VACF)\n\nsource\n\nvacf\n\n vacf (trajs, delta_t:int|list|numpy.ndarray=1,\n       taus:bool|list|numpy.ndarray=None)\n\nCalculates the velocity autocorrelation function for the given set of trajectories.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrajs\nnp.array\n\nNxT matrix containing N trajectories of length T.\n\n\ndelta_t\nint | list | numpy.ndarray\n1\nIf not None, the vacf is calculated in the demanded time lags.\n\n\ntaus\nbool | list | numpy.ndarray\nNone\nTime windows at wich the vacf is calculated.\n\n\nReturns\nnp.array\n\nVACF of the given trajectories and the given time windows. \n\n\n\nWe show here an example of the VACF for FBM trajectories at various time lages, showing that they all coincide (as expected for this diffusion model).\n\ndeltats = np.arange(1, 5).tolist()\ntaus = np.arange(0, 100)\ntrajs, _ = models_phenom().single_state(N = 200, T = 200, alphas = 0.5)\ntrajs = trajs.transpose(1, 0, 2)[:,:,0]\n\nfor deltat in deltats:  \n    v = vacf(trajs, deltat, taus)        \n    plt.plot(taus/deltat, v.flatten(), 'o-', alpha = 0.4)    \nplt.xlim(-1, 10)\nplt.ylabel('VACF'); plt.xlabel(r'$\\tau / \\delta$')\n\nText(0.5, 0, '$\\\\tau / \\\\delta$')\n\n\n\n\n\n\n\n\nConvex hull analysis\n\nsource\n\nCH_changepoints\n\n CH_changepoints (trajs, tau:int=10, metric:{'volume','area'}='volume')\n\nComputes the changes points a multistate trajectory based on the Convex Hull approach proposed in PRE 96 (022144), 2017.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrajs\nnp.array\n\nNxT matrix containing N trajectories of length T.\n\n\ntau\nint\n10\nTime window over which the CH is calculated.\n\n\nmetric\n{‘volume’, ‘area’}\nvolume\nCalculate change points w.r.t. area or volume of CH.\n\n\nReturns\nlist\n\nChange points of the given trajectory.\n\n\n\nWe showcase the use of the convex hull in a Brownian motion trajectory with two distinct diffusion coefficients, one 10 times the other:\n\n# Generate trajectories and plot change points\nT = 100; on = 40; off = 60; tau = 5\ntraj = np.random.randn(T, 2)\ntraj[on:off, :] = traj[on:off, :]*10\ntraj = traj.cumsum(0)\nplt.axvline(on-tau, c = 'k')\nplt.axvline(off-tau, c = 'k', label = 'True change points')\n\n# Calculate variable Sd \nSd = np.zeros(traj.shape[0]-2*tau)\nfor k in range(traj.shape[0]-2*tau):       \n    Sd[k] = ConvexHull(traj[k:(k+2*tau)], ).volume  \n\n# Compute change points both with volume and area\nCPs = CH_changepoints([traj], tau = tau)[0].flatten()-tau\nCPs_a = CH_changepoints([traj], tau = tau, metric = 'area')[0].flatten()-tau\n\n\n# Plot everything\nlabel_cp = 'CH Volume'\nfor cp in CPs:\n    plt.axvline(cp, c = 'g', alpha = 0.8, ls = '--', label = label_cp)\n    label_cp = ''\nlabel_cp = 'CH Area'    \nfor cp in CPs_a:\n    plt.axvline(cp, alpha = 0.8, ls = '--', c = 'orange', label = label_cp)\n    label_cp = ''\n    \nplt.plot(Sd, '-o')\nplt.axhline(Sd.mean(), label = 'CH Volume mean', c = 'g',)\nplt.legend()\nplt.xlabel('n'); plt.ylabel(r'$S_d(n)$')\n\nText(0, 0.5, '$S_d(n)$')\n\n\n\n\n\n\n\n\nCramér-Rao lower bound\n\nExpressions of the CRLB for the estimation of the diffusion coefficient for trajectories without noise, for dimension 1 and 2, at different trajectory lengths\n\n\nsource\n\nCRLB_D\n\n CRLB_D (T:int, dim:int=1)\n\nCalculates the bound for S(D)/D, i.e. ratio between the standard deviation and the expected value of D This holds for x->0 only (i.e. no noise)! See PRE 85, 061916 (2012) for full equation.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nint\n\nLength of the trajectory\n\n\ndim\nint\n1\nDimension of the trajectoy\n\n\nReturns\nfloat\n\nCramér-Rao bound\n\n\n\nHere we show how for Brownian motion trajectories, the MSD with is optimal and converges to the CRLB.\n\n#eval: false\nTs = np.arange(10, 100) # Trajectory lengths\nDs = np.arange(0.5, 2.1, 0.5) # We will test on different diffusion coefficients\n\n# Saving the standard deviations\nstd_ds = np.zeros((len(Ds), len(Ts)))\n\nfor idxD, D in enumerate(tqdm(Ds)):\n    \n    # Generate the BM trajectories with given parameters\n    trajs = (np.sqrt(2*D)*np.random.randn(int(1e4), Ts[-1], 1)).cumsum(1) \n\n    for idxL, T in enumerate(tqdm(Ts)):\n        \n        std_ds[idxD, idxL] = np.array(msd_analysis().get_diff_coeff(trajs = trajs[:, :T], t_lags = [1,2])).std()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(1,4, figsize = (4*3,3), tight_layout = True)\n\nfor idx, (ax, std_d, D) in enumerate(zip(axs.flatten(), std_ds, Ds)):\n    ax.plot(Ts, std_d/D, label = r'MSD$_{1-2}$')\n    ax.scatter(Ts[::3], CRLB_D(Ts)[::3], label = 'Cramer-Rao bound', alpha = 0.9, facecolor = 'w', edgecolor = 'C1')\n    ax.set_title(fr'$D = {D}$')\naxs[0].legend()\nplt.setp(axs[0], ylabel = 'std(D) / D')\nplt.setp(axs, xlabel = 'Traj. length');\n\n\n\n\nHere is now the same calculation for 2D trajectories, showing that we get the same convergence!\n\nfig, axs = plt.subplots(1,4, figsize = (4*3,3), tight_layout = True)\n\nfor idx, (ax, std_d, D) in enumerate(zip(axs.flatten(), std_ds, Ds)):\n    ax.plot(Ts, std_d/D, label = r'MSD$_{1-2}$', c = 'C2')\n    ax.scatter(Ts[::3], CRLB_D(Ts, dim = 2)[::3], label = 'Cramer-Rao bound', alpha = 0.9, facecolor = 'w', edgecolor = 'C3')\n    ax.set_title(fr'$D = {D}$')\naxs[0].legend()\nplt.setp(axs[0], ylabel = 'std(D) / D')\nplt.setp(axs, xlabel = 'Traj. length');"
  },
  {
    "objectID": "lib_nbs/utils_videos.html",
    "href": "lib_nbs/utils_videos.html",
    "title": "utils_videos",
    "section": "",
    "text": "source\n\n\n\n play_video (video, figsize=(5, 5), fps=10)\n\nDisplays a stack of images as a video inside jupyter notebooks.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvideo\nndarray\n\nStack of images.\n\n\nfigsize\ntuple\n(5, 5)\nCanvas size of the video.\n\n\nfps\nint\n10\nVideo frame rate.\n\n\nReturns\nVideo object\n\nReturns a video player with input stack of images.\n\n\n\n\nsource\n\n\n\n\n convert_uint8 (vid, with_vips=False)\n\nConverts a stack of images in to 8bit pixel format.\nThis is a helper function for transform_to_video\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvid\nndarray\n\nStack of images.\n\n\nwith_vips\nbool\nFalse\n\n\n\nReturns\nndarray\n\nImage stack in 8bit.\n\n\n\n\nsource\n\n\n\n\n psf_width (NA=1.46, wavelength=5e-07, resolution=1e-07)\n\nComputes the PSF full width at half maximum (FWHM).\nThis is a helper function for transform_to_video\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nNA\nfloat\n1.46\nNumerical aperture.\n\n\nwavelength\nfloat\n5e-07\nWavelength.\n\n\nresolution\nfloat\n1e-07\nResolution of the camera.\n\n\nReturns\nint\n\nPSF width in pixels.\n\n\n\n\nsource\n\n\n\n\n func_poisson_noise ()\n\nApplies poisson noise to an image.\nThis is a custom DeepTrack feature, and a helper function for transform_to_video\n\nsource\n\n\n\n\n mask (circle_radius, particle_list=[])\n\nComputes binary masks for particles in microscopy videos.\nThis is a custom DeepTrack feature, and a helper function for transform_to_video."
  },
  {
    "objectID": "lib_nbs/utils_videos.html#usage-example",
    "href": "lib_nbs/utils_videos.html#usage-example",
    "title": "utils_videos",
    "section": "Usage example",
    "text": "Usage example\nPlease check tutorials for more detailed examples\nImport packages\n\nfrom andi_datasets.models_phenom import models_phenom\n\nGenerating trajectories and passing them through transform_to_video to generate videos\n\ntrajs_test, _ = models_phenom().single_state(N = 50, T = 100, L = 128, Ds = 1)\nvideo, masks = transform_to_video(trajs_test,\n                                  with_masks=True,\n                                  get_vip_particles=np.arange(55).tolist(),\n                                  particle_props = {\"z\": 0},\n                                  background_props = {\"background_mean\": 0,\n                                                      \"background_std\": 0}, \n                                 optics_props = {'origin': [0,0,20,20]})\n\nPlay generated video in jupyter notebook\n\nplay_video(video)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nPlot the first frames of the video and the masks\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nimg2 = ax1.imshow(video[1], cmap=\"gray\")\nax1.set_title(\"Frame 0\")\nax2.imshow(masks[1], cmap=\"gray\")\nax2.set_title(\"Mask 0\")\n\nText(0.5, 1.0, 'Mask 0')\n\n\n\n\n\nOverlay trajectories on the first frame of the video\n\nplt.figure(figsize=(5, 5))\nplt.imshow(video[0], cmap=\"gray\", zorder = -1)\nfor traj in np.moveaxis(trajs_test, 0, 1):\n    plt.plot(traj[:,1], traj[:,0], alpha=0.2)\nplt.xlim(0,L); plt.ylim(0,L)\nplt.show()"
  },
  {
    "objectID": "changes_andi_v2.html",
    "href": "changes_andi_v2.html",
    "title": "Version changes",
    "section": "",
    "text": "This blog recapitulates the main changes of the andi_datasets library. These have two goals: simplify and standarize how to access the different available diffusion models, and most importantly, include the diffusion models that will be considered during the second ANDI challenge.\n\n\nUsing a hyphen in the library’s name carried some problems. Mainly, due how pip and python deal with them. In our case, in order to install the library, you had to use pip install andi-datasets (sadly, the package andi already exists…), but the to import the package, you had to call import andi. This was inconsistent, and carried some other problems, mostly related to the nbdev package (see below). That is way we have changed the name of the package, such that now, it is found in pip as\npip install andi_datasets\nand is imported in python via import andi_datasets.\n\n\n\nOne of the main changes is that we have switched to a nbdev-like library, where all the code is developed in notebooks and the compiled into .py files via the ndbev compiler. In terms of the library usage, nothing has really changed (aside of all the listed below), but we find that this way of developing code will help us maintain a better package and ease adding new features.\nThen, we have restructured the whole package to accommodate the new diffusion models as well as standarize the use of the library. Here is a little scheme of how the library is organized:\n\nIn summary, the classes datasets_XXX are used to generate, save and load the trajectories generated by the classes models_XXX. These store the diffusion models, either theoretical or phenomenological. The former were used for the ANDI 1 challenge. You can find more details about them in the challenge’s paper or in this updated notebook tutorial. The latter are the basis of the ANDI 2022 challenge and are simulated by means of fractional brownian motion plus some extra tweaks. You can start playing around with this models and learn from them via this notebook tutorial.\nOn the other hand, we have created a completely new class that gathers the generators for the various Challenge datasets. This allows you to generate datasets similar to the ones that will be used in challenge. Note that the one for Challenge 2 is subject to changes!\nWe have also created a new class, called analysis that allows you to access common analysis methods for diffusion trajectories, as for instance MSD based fittings for the diffusion coefficient and anomalous exponents, calculations of the velocity autocorrelation function,…\nLast, we have organized the different auxiliary functions in three files:\n\nutils_videos allows to merge our library with deeptrack. This allows to generate experimentally realistic videos from the trajectories you create with andi-datasets. A tutorial about it can be found in the Tutorials tab.\nutils_trajectories gathers all functions related to the creation of trajectories.\nutils_challenge gathers all functions needed to correctly prepare the trajectories and its labels for their use in the AnDi Challenge. It also contains the metrics and the evaluators for the second challenge.\n\n\n\n\nTo be fair, accessing directly the diffusion models in the last version was a bit messy. That’s what new versions are here for! Let’s focus first on the theoretical models. Now they can simply be from the models_theory class. Their inputs stay the same as before (alpha for anomalous exponent and T for the length of the trajectory). We have added an extra variable to set the dimension of the diffusion easily. To give an example, a 3D trajectory of ATTM can now be easily created using:\n from andi_dataset.models_theory import models_theory\n\n trajectory = models_theory().attm(alpha = 0.8, T = 10, dimension = 3)\nAgain, more examples on this theoretical models are given in their corresponding tutorial notebook\nFor the new phenomenological models, it works the same! However, in this case we have a few more parameters than just the anomalous exponent. You can explore them here.\n\n\n\n\nVariable name change in models_theory().create_dataset: N is now N_model. In this way, it is more clear that N_model refers to number of trajectories per model and we use N for total number of trajectories in other functions.\nCorrected how noise is applied in Task3. Now the noise is added after the segmentation. This ensures that two subsequent segments don’t have different noises.\nAdded an extra variable in datasets_challenge().challenge_theory_dataset, return_noise which, if True, makes the function output the noise amplitudes added to each trajectory. This will help users to get all the information they need when creating their datasets.\nChange how noise is applied in models_theory.challenge_theory_dataset, such that all components of trajectories with dimension 2 and 3 have the same noise amplitude."
  },
  {
    "objectID": "changes_andi_v2.html#section-1",
    "href": "changes_andi_v2.html#section-1",
    "title": "Version changes",
    "section": "2.1.0",
    "text": "2.1.0"
  }
]