{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a62681d-41ec-4186-af26-792ea1bbfd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets_challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360cc678-cd30-4b1d-b6cc-91259b80379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from andi_datasets.utils_challenge import segs_inside_fov, continuous_label_to_list, extract_ensemble, label_filter\n",
    "from andi_datasets.datasets_phenom import datasets_phenom\n",
    "from andi_datasets.datasets_theory import datasets_theory\n",
    "from andi_datasets.utils_trajectories import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48093981-9a0f-4b6c-ace2-914a1da1b151",
   "metadata": {},
   "source": [
    "# ANDI challenge 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d759395-c204-43c1-b274-bd4bb56bd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def challenge_2020_dataset(N = 1000, max_T = 1000, min_T = 10,\n",
    "                           tasks = [1, 2, 3],\n",
    "                           dimensions = [1, 2, 3],\n",
    "                           load_dataset = False, save_dataset = False, path_datasets = '', load_labels = True,\n",
    "                           load_trajectories = False, save_trajectories = False, path_trajectories = 'datasets/',\n",
    "                           N_save = 1000, t_save = 1000,\n",
    "                           return_noise = False):  \n",
    "    ''' Creates a dataset similar to the one given by in the ANDI challenge. \n",
    "    Check the webpage of the challenge for more details. The default values\n",
    "    are similar to the ones used to generate the available dataset.\n",
    "    Arguments:  \n",
    "        :N (int, numpy.array):\n",
    "            - if int, number of trajectories per class (i.e. exponent and model) in the dataset.\n",
    "            - if numpy.array, number of trajectories per classes: size (number of models)x(number of classes)    \n",
    "        :max_T (int):\n",
    "            - Maximum length of the trajectories in the dataset.\n",
    "        :min_T (int):\n",
    "            - Minimum length of the trajectories in the dataset.\n",
    "        :tasks (int, array):\n",
    "            - Task(s) of the ANDI for which datasets will be generated. Task 1 corresponds to the\n",
    "            anomalous exponent estimation, Task 2 to the model prediction and Task 3 to the segmen-\n",
    "            tation problem.\n",
    "        :dimensions (int, array):\n",
    "            - Task(s) for which trajectories will be generated. Three possible values: 1, 2 and 3.\n",
    "        :load_dataset (bool):\n",
    "            - if True, the module loads the trajectories from the files task1.txt, task2.txt and\n",
    "            task3.txt and the labels from ref1.txt, ref2.txt and ref3.txt. If the trajectories do\n",
    "            not exist but the file does, the module returns and empty dataset.                \n",
    "        :save_dataset (bool):\n",
    "            - if True, the module saves the datasets in a .txt following the format discussed in the \n",
    "            webpage of the comptetion.\n",
    "        :path_datasets (str):\n",
    "            - path from where to load the dataset.\n",
    "        :load_labels (bool):\n",
    "            - If False, only loads trajectories and avoids the files refX.txt.\n",
    "        :load_trajectories (bool):\n",
    "            - if True, the module loads the trajectories of an .h5 file.  \n",
    "        :save_trajectories (bool):\n",
    "            - if True, the module saves a .h5 file for each model considered, with N_save trajectories \n",
    "              and T = T_save.\n",
    "        :path_trajectories (str):\n",
    "            - path from where to load trajectories.\n",
    "        :N_save (int):\n",
    "            - Number of trajectories to save for each exponents/model. Advise: save at the beggining\n",
    "              a big dataset (i.e. with default t_save N_save) which allows you to load any other combiantion\n",
    "              of T and N.\n",
    "        :t_save (int):\n",
    "            - Length of the trajectories to be saved. See comments on N_save.   \n",
    "        :return_noise (bool):\n",
    "            - If True, returns the amplitudes of the noises added to the trajectories.\n",
    "    Return:\n",
    "        The function returns 6 variables, three variables for the trajectories and three \n",
    "        for the corresponding labels. Each variable is a list of three lists. Each of the\n",
    "        three lists corresponds to a given dimension, in ascending order. If one of the\n",
    "        tasks/dimensions was not calculated, the given list will be empty\n",
    "        :Xn (list of three lists):\n",
    "            - Trajectories corresponding to Task n (with n = 1,2,3). \n",
    "        :Yn (list of three lists):\n",
    "            - Labels corresponding to Task n                \n",
    "        :loc_noise_tn (list of three lists):\n",
    "            - if return_noise = True, collects the amplitude of the localization noise for task n.\n",
    "        :diff_noise_tn (list of three lists):\n",
    "            - if return_noise = True, collects the amplitude of the diffusion noise for task n. \n",
    "        The two previous variables are currently NOT SAVED NOR LOADED internally!\n",
    "            '''\n",
    "\n",
    "    print(f'Creating a dataset for task(s) {tasks} and dimension(s) {dimensions}.')\n",
    "\n",
    "    # Checking inputs for errors\n",
    "    if isinstance(dimensions, int) or isinstance(dimensions, float):\n",
    "        dimensions = [dimensions]\n",
    "    if isinstance(tasks, int) or isinstance(tasks, float):\n",
    "        tasks = [tasks]\n",
    "\n",
    "    # Define return datasets\n",
    "    X1 = [[],[],[]]; X2 = [[],[],[]]; X3 = [[],[],[]]\n",
    "    Y1 = [[],[],[]]; Y2 = [[],[],[]]; Y3 = [[],[],[]]\n",
    "\n",
    "    if return_noise:\n",
    "        loc_noise_t1 = [[],[],[]]; loc_noise_t2 = [[],[],[]]; loc_noise_t3 = [[],[],[]]\n",
    "        diff_noise_t1 = [[],[],[]]; diff_noise_t2 = [[],[],[]]; diff_noise_t3 = [[],[],[]]\n",
    "\n",
    "    if load_dataset or save_dataset:\n",
    "        # Define name of result files, if needed\n",
    "        task1 = path_datasets+'task1.txt'; ref1 = path_datasets+'ref1.txt'\n",
    "        task2 = path_datasets+'task2.txt'; ref2 = path_datasets+'ref2.txt'\n",
    "        task3 = path_datasets+'task3.txt'; ref3 = path_datasets+'ref3.txt'\n",
    "\n",
    "    # Loading the datasets if chosen.\n",
    "    if load_dataset:            \n",
    "        for idx, (task, lab) in enumerate(zip([task1, task2, task3], [ref1, ref2, ref3])):\n",
    "            if idx+1 in tasks:\n",
    "\n",
    "                try:\n",
    "                    t = csv.reader(open(task,'r'), delimiter=';', \n",
    "                                    lineterminator='\\n',quoting=csv.QUOTE_NONNUMERIC)\n",
    "                    if load_labels:\n",
    "                        l = csv.reader(open(lab,'r'), delimiter=';', \n",
    "                                        lineterminator='\\n',quoting=csv.QUOTE_NONNUMERIC)\n",
    "                except:\n",
    "                    raise FileNotFoundError(f'File for task {idx+1} not found.')\n",
    "\n",
    "\n",
    "                if load_labels:                    \n",
    "                    for trajs, labels in zip(t, l):   \n",
    "                        if task == task1:                            \n",
    "                            X1[int(trajs[0])-1].append(trajs[1:])\n",
    "                            Y1[int(trajs[0])-1].append(labels[1])\n",
    "                        if task == task2:\n",
    "                            X2[int(trajs[0])-1].append(trajs[1:])\n",
    "                            Y2[int(trajs[0])-1].append(labels[1])\n",
    "                        if task == task3:\n",
    "                            X3[int(trajs[0])-1].append(trajs[1:])\n",
    "                            Y3[int(trajs[0])-1].append(labels[1:]) \n",
    "                else:\n",
    "                    for trajs in t:   \n",
    "                        if task == task1:                            \n",
    "                            X1[int(trajs[0])-1].append(trajs[1:])\n",
    "                        if task == task2:\n",
    "                            X2[int(trajs[0])-1].append(trajs[1:])\n",
    "                        if task == task3:\n",
    "                            X3[int(trajs[0])-1].append(trajs[1:])\n",
    "\n",
    "                # Checking that the dataset exists in the files\n",
    "                for dim in dimensions:\n",
    "                    if task == task1 and X1[dim-1] == []:\n",
    "                        raise FileNotFoundError('Dataset for dimension '+str(dim)+' not contained in file task1.txt.')\n",
    "                    if task == task2 and X2[dim-1] == []:\n",
    "                        raise FileNotFoundError('Dataset for dimension '+str(dim)+' not contained in file task2.txt.')\n",
    "                    if task == task3 and X3[dim-1] == []:\n",
    "                        raise FileNotFoundError('Dataset for dimension '+str(dim)+' not contained in file task3.txt.')\n",
    "        if load_labels:\n",
    "            return X1, Y1, X2, Y2, X3, Y3        \n",
    "        else: \n",
    "            return X1, X2, X3     \n",
    "\n",
    "\n",
    "    exponents = np.arange(0.05, 2.01, 0.05)\n",
    "    n_exp = len(exponents)\n",
    "    # Trajectories per model and exponent. Arbitrarely chosen to obtain balanced classes\n",
    "    n_per_model = np.ceil(1.6*N/5)\n",
    "    subdif, superdif = n_exp//2, n_exp//2+1\n",
    "    n_per_class =  np.zeros((len(datasets_theory().avail_models_name), n_exp))\n",
    "    # ctrw, attm\n",
    "    n_per_class[:2, :subdif] = np.ceil(n_per_model/subdif)\n",
    "    # fbm\n",
    "    n_per_class[2, :] = np.ceil(n_per_model/(n_exp-1))\n",
    "    n_per_class[2, exponents == 2] = 0 # FBM can't be ballistic\n",
    "    # lw\n",
    "    n_per_class[3, subdif:] = np.ceil((n_per_model/superdif)*0.8)\n",
    "    # sbm\n",
    "    n_per_class[4, :] = np.ceil(n_per_model/n_exp)\n",
    "\n",
    "    # Define return datasets\n",
    "    X1 = [[],[],[]]; X2 = [[],[],[]]; X3 = [[],[],[]]\n",
    "    Y1 = [[],[],[]]; Y2 = [[],[],[]]; Y3 = [[],[],[]]  \n",
    "\n",
    "    # Initialize the files\n",
    "    if save_dataset:\n",
    "        if 1 in tasks:\n",
    "            csv.writer(open(task1,'w'), delimiter=';', lineterminator='\\n',)\n",
    "            csv.writer(open(ref1,'w'), delimiter=';', lineterminator='\\n',)\n",
    "        elif 2 in tasks:\n",
    "            csv.writer(open(task2,'w'), delimiter=';', lineterminator='\\n',)\n",
    "            csv.writer(open(ref2,'w'), delimiter=';',lineterminator='\\n',)\n",
    "        elif 3 in tasks:\n",
    "            csv.writer(open(task3,'w'), delimiter=';', lineterminator='\\n',)\n",
    "            csv.writer(open(ref3,'w'), delimiter=';',lineterminator='\\n',)\n",
    "\n",
    "    for dim in dimensions:             \n",
    "        # Generate the dataset of the given dimension\n",
    "        print(f'Generating dataset for dimension {dim}.')\n",
    "        dataset = datasets_theory().create_dataset(T = max_T, N_models = n_per_class, exponents = exponents, \n",
    "                                       dimension = dim, models = np.arange(len(datasets_theory().avail_models_name)),\n",
    "                                       load_trajectories = False, save_trajectories = False, N_save = 100,\n",
    "                                       path = path_trajectories)            \n",
    "\n",
    "        # Normalize trajectories\n",
    "        n_traj = dataset.shape[0]\n",
    "        norm_trajs = normalize(dataset[:, 2:].reshape(n_traj*dim, max_T))\n",
    "        dataset[:, 2:] = norm_trajs.reshape(dataset[:, 2:].shape)\n",
    "\n",
    "        # Save unnoisy dataset for task3\n",
    "        dataset_t3 = dataset.copy()\n",
    "\n",
    "        # Add localization error, Gaussian noise with sigma = [0.1, 0.5, 1]                \n",
    "        loc_error_amplitude = np.random.choice(np.array([0.1, 0.5, 1]), size = n_traj).repeat(dim)\n",
    "        loc_error = (np.random.randn(n_traj*dim, int(max_T)).transpose()*loc_error_amplitude).transpose()                        \n",
    "        dataset = datasets_theory().create_noisy_localization_dataset(dataset, dimension = dim, T = max_T, noise_func = loc_error)\n",
    "\n",
    "\n",
    "        # Add random diffusion coefficients            \n",
    "        trajs = dataset[:, 2:].reshape(n_traj*dim, max_T)\n",
    "        displacements = trajs[:, 1:] - trajs[:, :-1]\n",
    "        # Get new diffusion coefficients and displacements\n",
    "        diffusion_coefficients = np.random.randn(n_traj).repeat(dim)\n",
    "        new_displacements = (displacements.transpose()*diffusion_coefficients).transpose()  \n",
    "        # Generate new trajectories and add to dataset\n",
    "        new_trajs = np.cumsum(new_displacements, axis = 1)\n",
    "        new_trajs = np.concatenate((np.zeros((new_trajs.shape[0], 1)), new_trajs), axis = 1)\n",
    "        dataset[:, 2:] = new_trajs.reshape(dataset[:, 2:].shape)\n",
    "\n",
    "\n",
    "\n",
    "        # Task 1 - Anomalous exponent\n",
    "        if 1 in tasks:         \n",
    "            # Creating semi-balanced datasets\n",
    "            n_exp_max = int(np.ceil(1.1*N/n_exp))\n",
    "            for exponent in exponents:\n",
    "                dataset_exp = dataset[dataset[:, 1] == exponent].copy()\n",
    "                dataset_exp = dataset_exp[:n_exp_max, :]\n",
    "                try:\n",
    "                    dataset_1 = np.concatenate((dataset_1, dataset_exp), axis = 0) \n",
    "                except:\n",
    "                    dataset_1 = dataset_exp\n",
    "\n",
    "            # Shuffle trajectories and noise                \n",
    "            p = np.random.permutation(dataset_1.shape[0])                \n",
    "            diffusion_coefficients_t1 = diffusion_coefficients[p].copy()\n",
    "            loc_error_amplitude_t1 = loc_error_amplitude[p].copy()\n",
    "            dataset_1 = dataset_1[p]\n",
    "\n",
    "            # Saving noise with correct number of elements\n",
    "            if return_noise:\n",
    "                loc_noise_t1[dim-1] = loc_error_amplitude_t1[:N]\n",
    "                diff_noise_t1[dim-1] = diffusion_coefficients_t1[:N]\n",
    "\n",
    "            for traj in dataset_1[:N, :]:             \n",
    "                # Cutting trajectories\n",
    "                cut_T = np.random.randint(min_T, max_T) \n",
    "                traj_cut = datasets_theory()._cut_trajectory(traj[2:], cut_T, dim=dim).tolist()                         \n",
    "                # Saving dataset\n",
    "                X1[dim-1].append(traj_cut)\n",
    "                Y1[dim-1].append(np.around(traj[1], 2))\n",
    "                if save_dataset:                        \n",
    "                    datasets_theory()._save_row(np.append(dim, traj_cut), task1)\n",
    "                    datasets_theory()._save_row(np.append(dim, np.around([traj[1]], 2)), ref1)\n",
    "\n",
    "        # Task 2 - Diffusion model\n",
    "        if 2 in tasks:   \n",
    "            # Creating semi-balanced datasets\n",
    "            # If number of traejectories N is too small, consider at least\n",
    "            # one trajectory per model\n",
    "            n_per_model = max(1, int(1.1*N/5))                    \n",
    "            for model in range(5):\n",
    "                dataset_mod = dataset[dataset[:, 0] == model].copy()\n",
    "                dataset_mod = dataset_mod[:n_per_model, :]\n",
    "                try:\n",
    "                    dataset_2 = np.concatenate((dataset_2, dataset_mod), axis = 0) \n",
    "                except:\n",
    "                    dataset_2 = dataset_mod\n",
    "\n",
    "            # Shuffle trajectories and noise                \n",
    "            p = np.random.permutation(dataset_2.shape[0])                \n",
    "            diffusion_coefficients_t2 = diffusion_coefficients[p].copy()\n",
    "            loc_error_amplitude_t2 = loc_error_amplitude[p].copy()\n",
    "            dataset_2 = dataset_2[p]\n",
    "            # Saving noise wityh correct number of elements\n",
    "            if return_noise:\n",
    "                loc_noise_t2[dim-1] = loc_error_amplitude_t2[:N]\n",
    "                diff_noise_t2[dim-1] = diffusion_coefficients_t2[:N]\n",
    "\n",
    "            for traj in dataset_2[:N, :]:             \n",
    "                # Cutting trajectories\n",
    "                cut_T = np.random.randint(min_T, max_T) \n",
    "                traj_cut = datasets_theory()._cut_trajectory(traj[2:], cut_T, dim=dim).tolist() \n",
    "                # Saving dataset   \n",
    "                X2[dim-1].append(traj_cut)\n",
    "                Y2[dim-1].append(np.around(traj[0], 2))    \n",
    "                if save_dataset:\n",
    "                    datasets_theory()._save_row(np.append(dim, traj_cut), task2)\n",
    "                    datasets_theory()._save_row(np.append(dim, traj[0]), ref2)   \n",
    "\n",
    "\n",
    "        # Task 3 - Segmentated trajectories\n",
    "        if 3 in tasks:  \n",
    "            # Create a copy of the dataset and use it to create the \n",
    "            # segmented dataset\n",
    "            dataset_copy1 = dataset_t3.copy()\n",
    "            dataset_copy2 = dataset_t3.copy()\n",
    "\n",
    "            # Shuffling the hard way\n",
    "            order_dataset1 = np.random.choice(np.arange(n_traj), n_traj, replace = False)        \n",
    "            order_dataset2 = np.random.choice(np.arange(n_traj), n_traj, replace = False)        \n",
    "            dataset_copy1 = dataset_copy1[order_dataset1] \n",
    "            dataset_copy2 = dataset_copy1[order_dataset2] \n",
    "\n",
    "            seg_dataset = datasets_theory().create_segmented_dataset(dataset_copy1, dataset_copy2, dimension = dim)        \n",
    "            seg_dataset = np.c_[np.ones(n_traj)*dim, seg_dataset]     \n",
    "\n",
    "            # Checking that there are no segmented trajectories with same exponent and model \n",
    "            # in each segment. First we compute the difference between labels\n",
    "            diff = np.abs(seg_dataset[:, 2]-seg_dataset[:, 4]) + np.abs(seg_dataset[:, 3]-seg_dataset[:, 5])\n",
    "            # Then, if there are repeated labels, we eliminate those trajectories\n",
    "            while len(np.argwhere(diff == 0)) > 0: \n",
    "                seg_dataset = np.delete(seg_dataset, np.argwhere(diff == 0), axis = 0)\n",
    "                # If the size of the dataset is too small, we generate new segmented trajectories\n",
    "                # and add them to the dataset\n",
    "                if seg_dataset.shape[0] < N:\n",
    "\n",
    "                    # Shuffling the hard way\n",
    "                    new_order_dataset1 = np.random.choice(np.arange(n_traj), n_traj, replace = False)        \n",
    "                    new_order_dataset2 = np.random.choice(np.arange(n_traj), n_traj, replace = False)        \n",
    "                    dataset_copy1 = dataset_copy1[new_order_dataset1] \n",
    "                    dataset_copy2 = dataset_copy1[new_order_dataset2] \n",
    "\n",
    "                    order_dataset1 = np.concatenate((order_dataset1, new_order_dataset1))\n",
    "                    order_dataset2 = np.concatenate((order_dataset2, new_order_dataset2))\n",
    "\n",
    "                    aux_seg_dataset = datasets_theory().create_segmented_dataset(dataset_copy1, dataset_copy2, dimension = dim) \n",
    "                    aux_seg_dataset = np.c_[np.ones(aux_seg_dataset.shape[0])*dim, aux_seg_dataset] \n",
    "                    seg_dataset = np.concatenate((seg_dataset, aux_seg_dataset), axis = 0)\n",
    "\n",
    "                    diff = np.abs(seg_dataset[:, 2]-seg_dataset[:, 4]) + np.abs(seg_dataset[:, 3]-seg_dataset[:, 5])\n",
    "                else:\n",
    "                    break \n",
    "\n",
    "            # Add localization error, Gaussian noise with sigma = [0.1, 0.5, 1]                \n",
    "            loc_error_amplitude_t3 = np.random.choice(np.array([0.1, 0.5, 1]), size = seg_dataset.shape[0]).repeat(dim)\n",
    "            loc_error_t3 = (np.random.randn(seg_dataset.shape[0]*dim, 200).transpose()*loc_error_amplitude_t3).transpose()\n",
    "\n",
    "            seg_dataset[:, 4:] = datasets_theory().create_noisy_localization_dataset(seg_dataset[:, 4:], \n",
    "                                                                        dimension = dim, T = 200, \n",
    "                                                                        noise_func = loc_error_t3)\n",
    "\n",
    "\n",
    "            # Add random diffusion coefficients            \n",
    "            trajs = seg_dataset[:, 6:].reshape(seg_dataset.shape[0]*dim, 200)\n",
    "            displacements = trajs[:, 1:] - trajs[:, :-1]\n",
    "            # Get new diffusion coefficients and displacements\n",
    "            diffusion_coefficients_t3 = np.random.randn(seg_dataset.shape[0]).repeat(dim)\n",
    "            new_displacements = (displacements.transpose()*diffusion_coefficients_t3).transpose()  \n",
    "            # Generate new trajectories and add to dataset\n",
    "            new_trajs = np.cumsum(new_displacements, axis = 1)\n",
    "            new_trajs = np.concatenate((np.zeros((new_trajs.shape[0], 1)), new_trajs), axis = 1)\n",
    "            seg_dataset[:, 6:] = new_trajs.reshape(seg_dataset[:, 6:].shape)\n",
    "\n",
    "            if return_noise:\n",
    "                loc_noise_t3[dim-1] = loc_error_amplitude_t3[:N].tolist()\n",
    "                diff_noise_t3[dim-1] = diffusion_coefficients_t3[:N].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            X3[dim-1] = seg_dataset[:N, 6:]\n",
    "            Y3[dim-1] = seg_dataset[:N, :6]\n",
    "\n",
    "            if save_dataset:\n",
    "                for label, traj in zip(seg_dataset[:N, :6], seg_dataset[:N, 6:]):\n",
    "                    datasets_theory()._save_row(np.append(dim, traj), task3)\n",
    "                    datasets_theory()._save_row(np.around(label, 2), ref3) \n",
    "\n",
    "\n",
    "    if return_noise:\n",
    "        return [X1, Y1, loc_noise_t1, diff_noise_t1, \n",
    "                X2, Y2, loc_noise_t2, diff_noise_t2, \n",
    "                X3, Y3, loc_noise_t3, diff_noise_t3] \n",
    "    else:\n",
    "        return X1, Y1, X2, Y2, X3, Y3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ae8f0-8497-47bb-b863-a08be03b32d9",
   "metadata": {},
   "source": [
    "# ANDI challenge 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dcdc6a6-b1e1-4964-832a-e555b0723cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _df_andi2: \n",
    "    'This class is used to define some of the default values set for the ANDI 2022 challenge'\n",
    "    def __init__(self):        \n",
    "        # General parameters\n",
    "\n",
    "        self.T = 500                   # Length of simulated trajectories\n",
    "        self._min_T = 20               # Minimal length of output trajectories\n",
    "        self.FOV_L = 128               # Length side of the FOV (px)\n",
    "        self.L = 1.2*self.FOV_L          # Length of the simulated environment\n",
    "        'PARTICLES MOVE TOO SLOWLY WITH D = 0.1!!'\n",
    "        self.D = 1                     # Baseline diffusion coefficient (px^2/frame)\n",
    "        self.density = 2               # Particle density   \n",
    "        self.N = 50                    # Number of particle in the whole experiment\n",
    "        self.sigma_noise = 0.12        # Variance of the localization noise\n",
    "\n",
    "        self.label_filter = lambda x: label_filter(x, window_size = 5, min_seg = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c0cb87f-adec-4856-a168-0817bb23b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_dic_andi2(model):\n",
    "    ''' Given the number label of diffusion model, returns a default\n",
    "    dictionary of the model's parameters to be fed to create_dataset\n",
    "    The numeration is as follow:\n",
    "            1: single state\n",
    "            2: N-state\n",
    "            3: immobilization\n",
    "            4: dimerization\n",
    "            5: confinement\n",
    "    Args:\n",
    "        :model (int in [1,6]): number of the diffusion model\n",
    "    Return:\n",
    "        :dic (dictionary): dictionary containing the default parameters\n",
    "        for ANDI2022 of the indicated model.\n",
    "    '''\n",
    "\n",
    "    dic = {'N': _df_andi2().N,\n",
    "           'T': _df_andi2().T,\n",
    "           'L': _df_andi2().L}\n",
    "\n",
    "    # alpha and D for single-state and immobilization\n",
    "    if model == 1 or model == 3:    \n",
    "        dic.update({'Ds': [_df_andi2().D, _df_andi2().D*0.01], # mean and varianve for D\n",
    "                    'alphas': np.array([np.random.rand()*(1.5-0.5)+0.5, 0.01])})\n",
    "\n",
    "    # alphas and Ds for 2-state, confinement and dimerization\n",
    "    if model == 2 or model == 4 or model == 5:            \n",
    "\n",
    "        fast_D = _df_andi2().D + np.random.randn()*_df_andi2().D*0.01\n",
    "        slow_D = fast_D*np.random.rand()*(0.1-0.01)+0.01    \n",
    "\n",
    "        alpha1 = np.random.rand()*(1.2-0.8)+0.8\n",
    "        # The second state will be at least 0.2 afar. We make sure not being \n",
    "        # outside [0,2]        \n",
    "        alpha2 = alpha1 - (np.random.rand()*(0.6-0.2)+0.2)\n",
    "\n",
    "        dic.update({'Ds': np.array([[fast_D, 0.01],\n",
    "                                    [slow_D, 0.01]]),\n",
    "                    'alphas': np.array([[alpha1, 0.01],\n",
    "                                        [alpha2, 0.01]])})\n",
    "\n",
    "    # Particle/trap radius and ninding and unbinding probs for dimerization and immobilization\n",
    "    if model == 3 or model == 4:\n",
    "        dic.update({'Pu': 0.01,                           # Unbinding probability\n",
    "                    'Pb': 1})                             # Binding probabilitiy\n",
    "\n",
    "    if model == 1:\n",
    "        dic.update({'model': datasets_phenom().avail_models_name[0]})\n",
    "\n",
    "    if model == 2:\n",
    "        dic.update({'model': datasets_phenom().avail_models_name[1],\n",
    "                    'M': np.array([[0.99, 0.01],            # Transition Matrix\n",
    "                                   [0.01, 0.99]]),\n",
    "                    'return_state_num': True              # To get the state numeration back, , hence labels.shape = TxNx4\n",
    "                   })\n",
    "    if model == 3:\n",
    "        dic.update({'model': datasets_phenom().avail_models_name[2],\n",
    "                    'Nt': 300,            # Number of traps (density = 1 currently)\n",
    "                    'r': 0.4}             # Size of trap\n",
    "                  )\n",
    "    if model == 4:\n",
    "        dic.update({'model': datasets_phenom().avail_models_name[3],\n",
    "                    'r': 0.6,                 # Size of particles\n",
    "                    'return_state_num': True  # To get the state numeration back, hence labels.shape = TxNx4\n",
    "                   })\n",
    "\n",
    "    if model == 5:\n",
    "        dic.update({'model': datasets_phenom().avail_models_name[4],\n",
    "                    'r': 5,\n",
    "                    'Nc': 30,\n",
    "                    'trans': 0.1})\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e25ea3-a26d-48c2-ba2b-47a0ac8bcd18",
   "metadata": {},
   "source": [
    "## Generating function\n",
    "This function generates trajectory datasets like the ones considered in the ANDI 2022 Challenge. It is based in `models_phenom.create_dataset` but also applies:\n",
    "\n",
    "- Apply Field of View (FOV)\n",
    "- Add localization noise\n",
    "- Smooth labels\n",
    "- Extracts ensemble properties\n",
    "- Correct labeling of trajectories\n",
    "\n",
    "\n",
    "**Inputs:**\n",
    "- Number of experiments (one experiment = one model).\n",
    "\n",
    "    For each experiment:\n",
    "    - Number of particles\n",
    "    - Number of FOVs\n",
    "    - Parameters of the model\n",
    "    - Mininum length of trajectories\n",
    "    \n",
    "**Outputs:** (this should be the same as the expected challenge inputs)\n",
    "    \n",
    "- For each FOV:\n",
    "    - Ensemble properties (Compulsory: model, $\\alpha$ and $D$ distribution)\n",
    "    - Trajectory properties (list of properties: $\\alpha_1$, $D_1$, CP$_1$, $\\alpha_2$, $D_2$, CP$_2$,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1382f9b4-dcf0-4729-ba00-a130e5765901",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def challenge_2022_dataset( \n",
    "                          experiments = 5,\n",
    "                          dics = None,\n",
    "                          repeat_exp = True,\n",
    "                          num_fovs = 20,\n",
    "                          return_timestep_labs = False,\n",
    "                          save_data = False,\n",
    "                          path = 'data/',\n",
    "                          prefix = '',\n",
    "                          video_dataset = False\n",
    "                            ):\n",
    "    ''' \n",
    "    Creates a datasets with same structure as ones given in the ANDI 2022 challenge. Default values for the\n",
    "    various diffusion models have been set such as to be in the same ranges as the ones expected for the\n",
    "    challenge. For details, check the ANDI 2022 challenge webpage.\n",
    "    This function will generate as many experiments (associated to one the diffusion models) as demanded.\n",
    "    There are two ways of defining that:\n",
    "        - Give number of experiments (and optional parameters such as repeat_exp) to create. The diffusion\n",
    "        parameters are then taken from the default values are taken from datasets_phenom._df_andi2.\n",
    "        - Feed a list of dictionaries (dics) from which data will be generated\n",
    "    For each experiment, as many field of view as wanted can be generated        \n",
    "\n",
    "    Args:  \n",
    "        :experiments (int, list): - if int: Number of experiments to generate. Each experiment is \n",
    "                                            generated from one of the available diffusion models.  \n",
    "                                  - if list: diffusion models to generate\n",
    "        :dics (dictionary, list of dics): if given, uses this to set the parameters of the experiments\n",
    "                                          Must be of length equal to experiments. This overrides any\n",
    "                                          info about chosen models, as the model is set by the dictio-\n",
    "                                          nary.\n",
    "        :repeat_exp (bool, list): (Does not enter into play if experiments is list)\n",
    "                                  - True: picks at random the diffusion model from the pool\n",
    "                                  - False: picks the diffusion in an ordered way from the pool\n",
    "        :num_fovs (int): Number of field of views to get trajectories from in each experiment.\n",
    "        :return_timestep_labs (bool): if True, the output trajectories dataframes containing also the \n",
    "                                      labels alpha, D and state at each time step.\n",
    "        :save_data (bool): if True, saves all pertinent data.\n",
    "        :path (str): path where to store the data\n",
    "        :prefix (str): extra prefix that can be added in front of the files' names.\n",
    "\n",
    "    Return:\n",
    "        :trajs_out (list): list of lenght (experiments x num_fovs). Each elements are is dataframe\n",
    "                           containing the trajectories of a particular experiment/fov, in order of \n",
    "                           generation (i.e. [exp1_fov1, exp1_fov2, ..., exp2_fov1 ....]).\n",
    "                           If return_timestep_labs = True, the dataframes also contain the labels\n",
    "                           at each time step\n",
    "        :labels_traj_out (list): list of same length of trajs_out containing the labels of the \n",
    "                                 corresponding trajectories. Each element contains a list with the \n",
    "                                 labels of each trajectory, following the scheme:\n",
    "                                 [idx_traj, D_1, alpha_1, state_1, CP_1, D_2, alpha_2, .... state_N]\n",
    "        :labels_ens_out (list): list of same length of trajs_out containing the ensemble labels of \n",
    "                                given experiment. See description of output matrix in \n",
    "                                utils_challenge._extract_ensemble()\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "    # Set prefixes for saved files\n",
    "    if save_data:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        pf_labs_traj = path+prefix+'traj_labs'\n",
    "        pf_labs_ens = path+prefix+'ens_labs'\n",
    "        pf_trajs = path+prefix+'trajs'\n",
    "\n",
    "    if return_timestep_labs:\n",
    "        df_list = []\n",
    "\n",
    "    # Sets the models of the experiments that will be output by the function\n",
    "    if dics is None:\n",
    "        if isinstance(experiments, int):\n",
    "            if repeat_exp: # If experiments can be repeated, we just sample randomly\n",
    "                model_exp = np.random.randint(len(datasets_phenom().avail_models_name), size = experiments)\n",
    "            else: # If not, we sampled them in an ordered way\n",
    "                if experiments >= len(datasets_phenom().avail_models_name):\n",
    "                    num_repeats = (experiments % len(datasets_phenom().avail_models_name))+1\n",
    "                else:\n",
    "                    num_repeats = 1\n",
    "                model_exp = np.tile(np.arange(len(datasets_phenom().avail_models_name)), num_repeats)[:experiments]\n",
    "            # We add one to get into non-Python numeration\n",
    "            model_exp += 1\n",
    "        else:\n",
    "            model_exp = experiments\n",
    "    # If list of dics is given, then just create a list of length = len(dics)\n",
    "    else: \n",
    "        model_exp = [0]*len(dics)\n",
    "\n",
    "    # Output lists\n",
    "    trajs_out, labels_traj_out, labels_ens_out = [], [], []\n",
    "    for idx_experiment, model in enumerate(tqdm(model_exp)):\n",
    "\n",
    "        ''' Generate the trajectories '''\n",
    "        if dics is None:\n",
    "            dic = _get_dic_andi2(model)\n",
    "        else:\n",
    "            dic = dics[idx_experiment]\n",
    "            # Overide the info about model\n",
    "            model = datasets_phenom().avail_models_name.index(dic['model'])+1\n",
    "\n",
    "        trajs, labels = datasets_phenom().create_dataset(dics = dic)\n",
    "\n",
    "        # Add noise the trajectories\n",
    "        trajs += np.random.randn(*trajs.shape)*_df_andi2().sigma_noise    \n",
    "\n",
    "        ''' Apply the FOV '''\n",
    "        for fov in range(num_fovs):\n",
    "\n",
    "            # Checking if file exist and creating an error\n",
    "            if save_data:\n",
    "                if os.path.exists(pf_labs_traj+f'_exp_{idx_experiment}_fov_{fov}.txt') or os.path.exists(pf_labs_ens+f'_exp_{idx_experiment}_fov_{fov}.txt'):\n",
    "                    raise FileExistsError(f'Target files for experiment {idx_experiment} and FOV {fov}. Delete the file or change path/prefix.')            \n",
    "\n",
    "\n",
    "\n",
    "            # We take as min/max for the fovs a 5 % distance of L\n",
    "            dist = 0.05\n",
    "            min_fov = int(dist*_df_andi2().L)\n",
    "            max_fov = int((1-dist)*_df_andi2().L)-_df_andi2().FOV_L\n",
    "            # sample the position of the FOV\n",
    "            fov_origin = (np.random.randint(min_fov, max_fov), np.random.randint(min_fov, max_fov))\n",
    "\n",
    "\n",
    "            ''' Go over trajectories in FOV (copied from utils_trajectories for efficiency) '''\n",
    "            trajs_fov, array_labels_fov, list_labels_fov, idx_segs_fov, frames_fov = [], [], [], [], []\n",
    "            idx_seg = -1\n",
    "\n",
    "            # Total frames\n",
    "            frames = np.arange(trajs.shape[0])\n",
    "            for idx, (traj, label) in enumerate(zip(trajs[:, :, :].transpose(1,0,2),\n",
    "                                                    labels[:, :, :].transpose(1,0,2))):\n",
    "                nan_segms = segs_inside_fov(traj, \n",
    "                                            fov_origin = fov_origin,\n",
    "                                            fov_length = _df_andi2().FOV_L,\n",
    "                                            cutoff_length = _df_andi2()._min_T)\n",
    "\n",
    "                if nan_segms is not None:\n",
    "                    for idx_nan in nan_segms:  \n",
    "                        idx_seg+= 1\n",
    "\n",
    "                        seg_x = traj[idx_nan[0]:idx_nan[1], 0]\n",
    "                        seg_y = traj[idx_nan[0]:idx_nan[1], 1]\n",
    "\n",
    "\n",
    "                        trajs_fov.append(np.vstack((seg_x, seg_y)).transpose())\n",
    "                        frames_fov.append(frames[idx_nan[0]:idx_nan[1]])\n",
    "\n",
    "                        lab_seg = []\n",
    "                        for idx_lab in range(labels.shape[-1]):\n",
    "                            lab_seg.append(_df_andi2().label_filter(label[idx_nan[0]:idx_nan[1], idx_lab]))\n",
    "                        lab_seg = np.vstack(lab_seg).transpose()                    \n",
    "                        array_labels_fov.append(lab_seg)\n",
    "\n",
    "                        # Tranform continuous labels to list for correct output\n",
    "                        if model == 2 or model == 4: \n",
    "                            # if multi-state or dimerization, we get rid of the label of state numbering\n",
    "                            CP, alphas, Ds, states = continuous_label_to_list(lab_seg[:, :-1])\n",
    "                        else:\n",
    "                            CP, alphas, Ds, states = continuous_label_to_list(lab_seg)\n",
    "                        \n",
    "                        # Extract final point of trajectory \n",
    "                        T = CP[-1]\n",
    "                        CP = CP[:-1]\n",
    "                        list_gt = [idx_seg, Ds[0], alphas[0], states[0]]\n",
    "                        for gtc, gta, gtd, gts in zip(CP, alphas[1:], Ds[1:], states[1:]):\n",
    "                            list_gt += [gtc, gtd, gta, gts]\n",
    "                        # Add end point of trajectory\n",
    "                        list_gt.append(T)\n",
    "                        list_labels_fov.append(list_gt)     \n",
    "\n",
    "                        if save_data:\n",
    "                            with open(pf_labs_traj+f'_exp_{idx_experiment}_fov_{fov}.txt', 'a') as f:\n",
    "                                writer = csv.writer(f, delimiter=',', lineterminator='\\n',)\n",
    "                                writer.writerow(list_gt)\n",
    "\n",
    "                        # Save index of segment with its length to latter append in the dataframe    \n",
    "                        idx_segs_fov.append(np.ones_like(seg_x)*idx_seg)\n",
    "\n",
    "            '''Extract ensemble trajectories''' \n",
    "            ensemble_fov = extract_ensemble(np.concatenate(array_labels_fov)[:, -1], dic)\n",
    "\n",
    "            df_data = np.hstack((np.expand_dims(np.concatenate(idx_segs_fov), axis=1),\n",
    "                                 np.expand_dims(np.concatenate(frames_fov), axis=1),\n",
    "                                 np.concatenate(trajs_fov)))\n",
    "            df_traj = pd.DataFrame(df_data, columns = ['traj_idx', 'frame', 'x', 'y']) \n",
    "\n",
    "\n",
    "            if return_timestep_labs:\n",
    "                array_labels_fov = np.concatenate(array_labels_fov)\n",
    "                df_traj['alpha'] = array_labels_fov[:, 0]\n",
    "                df_traj['D'] = array_labels_fov[:, 1]\n",
    "                df_traj['state'] = array_labels_fov[:, 2]\n",
    "\n",
    "            if save_data:\n",
    "                # Trajectories                    \n",
    "                df_traj.to_csv(pf_trajs+f'_exp_{idx_experiment}_fov_{fov}.csv', index = False)\n",
    "                # Ensemble labels\n",
    "                with open(pf_labs_ens+f'_exp_{idx_experiment}_fov_{fov}.txt', 'a') as f:\n",
    "                    if model == 2: num_states = dic['alphas'].shape[0]\n",
    "                    elif model == 1: num_states = 1\n",
    "                    else: num_states = 2\n",
    "                    model_n = dic['model']\n",
    "                    f.write(f'model: {model_n}; num_state: {num_states} \\n')\n",
    "                    np.savetxt(f, ensemble_fov, delimiter = ';')\n",
    "\n",
    "\n",
    "            # Add data to main lists (trajectories and lists with labels)   \n",
    "            trajs_out.append(df_traj)\n",
    "            labels_traj_out.append(list_labels_fov)\n",
    "            labels_ens_out.append(ensemble_fov)\n",
    "\n",
    "    return trajs_out, labels_traj_out, labels_ens_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c3aecac-8e56-4e3a-a79f-832ae7002f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a748ab034a44dcb346e163e9061c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dph = datasets_phenom()\n",
    "num_experiments, num_fovs = 1, 1\n",
    "df_list, lab_t, lab_e = challenge_2022_dataset(experiments = num_experiments,\n",
    "                                                   num_fovs = num_fovs, \n",
    "                                                   return_timestep_labs=True, \n",
    "                                                   repeat_exp = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60cefc4-1963-4ed6-bb25-c284abc75b75",
   "metadata": {},
   "source": [
    "# NBDEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62454379-407c-43ce-a651-447bc1942f62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted analysis.ipynb.\n",
      "Converted datasets_challenge.ipynb.\n",
      "Converted datasets_phenom.ipynb.\n",
      "Converted datasets_theory.ipynb.\n",
      "Converted models_phenom.ipynb.\n",
      "Converted models_theory.ipynb.\n",
      "Converted utils_challenge.ipynb.\n",
      "Converted utils_trajectories.ipynb.\n",
      "Converted utils_videos.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca83211-09d8-4c85-87d3-9763378ba09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
