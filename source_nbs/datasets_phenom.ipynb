{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380a05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets_phenom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0d11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from andi_datasets.models_phenom import models_phenom\n",
    "from andi_datasets.datasets_phenom import *\n",
    "\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c5c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dedb5e0",
   "metadata": {},
   "source": [
    "# Class constructor\n",
    "\n",
    "The class is initiated by accessing the `models_phenom` class and inspecting the available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70947895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class datasets_phenom():\n",
    "    def __init__(self):\n",
    "            ''' Constructor of the class '''\n",
    "            self._get_models()\n",
    "        \n",
    "    def _get_models(self):        \n",
    "        '''Loading subclass of models'''\n",
    "\n",
    "        available_models = inspect.getmembers(models_phenom(), inspect.ismethod)      \n",
    "        available_models = available_models[1:] # we need this to get rid of the init\n",
    "        self.avail_models_name = [x[0] for x in available_models]\n",
    "        self.avail_models_func = [x[1] for x in available_models]\n",
    "        \n",
    "    def _get_inputs_models(self, model, get_default_values = False):\n",
    "        \n",
    "        model_f = self.avail_models_func[self.avail_models_name.index(model)] \n",
    "        defaults = inspect.getfullargspec(model_f).defaults\n",
    "        params = inspect.getfullargspec(model_f).args[1:]\n",
    "        if get_default_values:\n",
    "            return params, defaults\n",
    "        else:\n",
    "            return params\n",
    "        \n",
    "    def _get_states(self):\n",
    "        ''' Definition of the possible states found in the ANDI 2022 challenge and their \n",
    "        assigned label:\n",
    "        0: immobile; 1: confined; 2: brownian; 3: anomalous '''\n",
    "        \n",
    "        self._states = ['immobile', 'confined', 'brownian', 'anomalous']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00c135",
   "metadata": {},
   "source": [
    "# `create_dataset`\n",
    "\n",
    "This function receives a list of dictionaries, each containing the properties of the trajectories to be created. The compulsory input for each dictionary is the key `model`, which defined the phenomenological diffusion model from which to create the trajectories. The rest of the properties are the ones of the model called. If no properties are given, the function automatically choses the default parameters of the model (check `models_phenom` for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1077ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class datasets_phenom(datasets_phenom):\n",
    "                \n",
    "    def create_dataset(self,\n",
    "                       T = None,\n",
    "                       N_model = None,\n",
    "                       dics = False,  \n",
    "                       path = '',\n",
    "                       save = False, load = False):\n",
    "        \n",
    "        self.T = T\n",
    "        self.N_model = N_model\n",
    "        self.path = path\n",
    "        self.dics = dics\n",
    "        \n",
    "        '''Managing dictionaries'''\n",
    "        # If the input is a single dictionary, transform it to list\n",
    "        if isinstance(self.dics, dict): self.dics = [self.dics]\n",
    "        # if dics is False, we select trajectories from all models with default values\n",
    "        if self.dics is False: self.dics = [{'model': model} for model in self.avail_models_name]\n",
    "\n",
    "                    \n",
    "        '''Managing folders of the datasets'''  \n",
    "        self.save = save\n",
    "        self.load = load\n",
    "        if self.save or self.load:                \n",
    "            if self.load:\n",
    "                self.save = False            \n",
    "            if not os.path.exists(self.path) and self.load:\n",
    "                raise FileNotFoundError('The directory from where you want to load the dataset does not exist')                \n",
    "            if not os.path.exists(self.path) and self.save:\n",
    "                os.makedirs(self.path) \n",
    "                \n",
    "                \n",
    "        '''Create trajectories'''\n",
    "        trajs, labels = self._create_trajectories()\n",
    "        \n",
    "        return trajs, labels                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662e5de",
   "metadata": {},
   "source": [
    "# `_create_trajectories`, `_save_trajectories`, `_load_trajectories`\n",
    "Auxiliary functions to `create_trajectories` that allow for creating, load and saving trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95578b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class datasets_phenom(datasets_phenom):   \n",
    "    \n",
    "    def _create_trajectories(self):\n",
    "\n",
    "        for dic in self.dics:\n",
    "            \n",
    "            dataset_idx, df = self._inspect_dic(dic)\n",
    "            \n",
    "            # If the dataset does not yet exists\n",
    "            if dataset_idx is False:\n",
    "                # Retrive name and function of diffusion model\n",
    "                model_f = self.avail_models_func[self.avail_models_name.index(dic['model'])]\n",
    "                # Create dictionary with only arguments\n",
    "                dic_args = dict(dic); dic_args.pop('model')\n",
    "                \n",
    "                trajs, labels = model_f(**dic_args)\n",
    "                \n",
    "                # Save the trajectories if asked\n",
    "                if self.save:\n",
    "                    self._save_trajectories(trajs = trajs,\n",
    "                                            labels = labels,\n",
    "                                            dic = dic, \n",
    "                                            df = df,\n",
    "                                            dataset_idx = dataset_idx,\n",
    "                                            path = self.path)                    \n",
    "            else:\n",
    "                trajs, labels = self._load_trajectories(model_name = dic['model'],\n",
    "                                                        dataset_idx = dataset_idx,\n",
    "                                                        path = self.path)\n",
    "                \n",
    "            # Stack dataset\n",
    "            try:\n",
    "                data_t = np.hstack((data_t, trajs))                    \n",
    "                data_l = np.hstack((data_l, labels))\n",
    "            except:\n",
    "                data_t = trajs\n",
    "                data_l = labels\n",
    "                    \n",
    "        return data_t, data_l  \n",
    "    \n",
    "    def _save_trajectories(self, trajs, labels, dic, df, dataset_idx, path):\n",
    "        \n",
    "        file_name = path+dic['model']+'_'+str(df.shape[0])+'.npy'\n",
    "        \n",
    "        # Save information in CSV handler\n",
    "        df = df.append(dic, ignore_index = True)\n",
    "        df.to_csv(path+dic['model']+'.csv')\n",
    "        \n",
    "        # Save trajectories and labels\n",
    "        data = np.stack((trajs, labels))\n",
    "        np.save(file_name, data)\n",
    "        \n",
    "    def _load_trajectories(self, model_name, dataset_idx, path):\n",
    "        \n",
    "        file_name = path+model_name+'_'+str(dataset_idx)+'.npy'\n",
    "        data = np.load(file_name)\n",
    "        return data[0], data[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433ac6d",
   "metadata": {},
   "source": [
    "# `_inspect_dic`\n",
    "Given a dictionary, this function checks that it fulfils the constraints of the program and checks the validity of the save/load actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bc810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class datasets_phenom(datasets_phenom):   \n",
    "\n",
    "    def _inspect_dic(self, dic):\n",
    "        '''Checks the information of the dictionaries and managesloading/saving information.'''        \n",
    "            \n",
    "        # Add time and number of trajectories information\n",
    "        if self.N_model is not None:\n",
    "            dic['N'] = self.N_model\n",
    "        if self.T is not None:\n",
    "            dic['T'] = self.T\n",
    "\n",
    "        # Check if CSV with information of dataset exists. If not, create it\n",
    "        model_m = dic['model']\n",
    "        model_f = self.avail_models_func[self.avail_models_name.index(model_m)]    \n",
    "        # Check arguments and defaults from model's function            \n",
    "        args = inspect.getfullargspec(model_f).args[1:]\n",
    "        defaults = inspect.getfullargspec(model_f).defaults\n",
    "        try:\n",
    "            df = pd.read_csv(self.path+model_m+'.csv', index_col=0)\n",
    "        except:                \n",
    "            # convert to dataframe and add model\n",
    "            df = pd.DataFrame(columns = args+['model'])                \n",
    "\n",
    "        # Assign missing keys in dic with default values\n",
    "        for arg, default in zip(args, defaults):\n",
    "            if arg not in dic.keys():\n",
    "                dic[arg] = default\n",
    "\n",
    "        # Check if updated keys of dic equal keys of csv.\n",
    "        if set(list(df.keys())) != set(list(dic.keys())):\n",
    "            raise ValueError('Input model dictionary does not match modelÂ´s properties')\n",
    "\n",
    "        # Check if the dataset already exists:\n",
    "        df_conditions = df.copy()\n",
    "        df_conditions = df_conditions.where(pd.notnull(df_conditions), None) # Need in case of empty elements because deafults are None\n",
    "        for key in dic:\n",
    "            # We need to transform it to str to do a fair comparison between matrices (e.g. transition matrix, Ds, alphas,...)\n",
    "            df_conditions = df_conditions.loc[(df_conditions[key].astype(str) == str(dic[key]))]\n",
    "            if len(df_conditions.index) == 0:\n",
    "                break\n",
    "\n",
    "        # If dataset exists\n",
    "        if len(df_conditions.index) > 0:\n",
    "            # if the dataset exists and save was True, do not save but load\n",
    "            if self.save:\n",
    "                wrn_str = f'The dataset you want to save already exists (file: {model_m}_{df_conditions.index[0]}.npy). Switching to Load mode.'\n",
    "                warnings.warn(wrn_str)\n",
    "                dataset_idx = df_conditions.index[0] \n",
    "            elif self.load:\n",
    "                dataset_idx = df_conditions.index[0]\n",
    "            else:\n",
    "                dataset_idx = False                 \n",
    "\n",
    "        # If dataset does no exists\n",
    "        else:         \n",
    "            if self.load:\n",
    "                raise ValueError('The dataset you want to load does not exist.')\n",
    "            else: # If the dataset does not exist, append empty string.\n",
    "                # This allows to mix saving and loading\n",
    "                dataset_idx = False\n",
    "                \n",
    "        return dataset_idx, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48e4ce-68d8-44e6-8af9-7f2ab03b5bd0",
   "metadata": {},
   "source": [
    "# `_get_args`\n",
    "Given the name of a model, returns its input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1c94758-6ee6-4c5c-bd90-a0213363ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class datasets_phenom(datasets_phenom):  \n",
    "    def _get_args(self, model, return_defaults = False):\n",
    "        model_f = self.avail_models_func[self.avail_models_name.index(model)]    \n",
    "        # Check arguments and defaults from model's function            \n",
    "        args = inspect.getfullargspec(model_f).args[1:]\n",
    "        defaults = inspect.getfullargspec(model_f).defaults\n",
    "        if return_defaults:\n",
    "            return args, defaults\n",
    "        else:\n",
    "            return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de1b2c-963b-408b-8ce8-52f3defb1cc7",
   "metadata": {},
   "source": [
    "# Define states from given labels\n",
    "Given an array of labels and their correspondance to the ANDI 2022 state labels, return an array with the state of each trajectory at every timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc55733-2481-4cda-9d16-ef470668d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class datasets_phenom(datasets_phenom):\n",
    "    def _extract_state(self, label_values, states, labels):\n",
    "        '''\n",
    "        Inputs:\n",
    "        :label_values (array) (size: # of states): values of any property for every existing state.\n",
    "        :states (array) (size: # of states): labels correspoding to each state as defined in the\n",
    "        ANDI 2022 state labels: 0: immobile; 1: confined; 2: brownian; 3: anomalous.''\n",
    "        :labels (array) (size: N x T): values of the labels over time\n",
    "        '''\n",
    "        \n",
    "        dummy_labels = np.zeros_like(labels)\n",
    "        \n",
    "        for lab_val, state in zip(label_values, states):\n",
    "            dummy_labels[labels == lab_val] = state\n",
    "            \n",
    "        return dummy_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35f371e2-a7e9-41d5-88ac-7f37c82202f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0, 1, 0],\n",
       "        [0, 1, 1, 0]]),\n",
       " array([[7, 4, 7, 4],\n",
       "        [4, 7, 7, 4]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randint(0, 2, (2, 4))\n",
    "val = [0, 1]; state = [4,7]\n",
    "b = datasets_phenom()._extract_state(val, state, a)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30b097",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b76af133-61b3-4bbc-a303-d8a1b38f1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from andi_datasets.datasets_phenom import datasets_phenom\n",
    "dp = datasets_phenom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50a92940-cf1c-45bb-aeed-bb3cbc5458d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['N', 'T', 'L', 'r', 'Pu', 'Pb', 'D', 'alpha', 'Nt', 'traps_pos', 'deltaT'],\n",
       " (10, 100, 5, 1, 0.1, 0.01, [1, 1, 1], [1, 1, 1], 10, None, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp._get_args('immobile_traps', return_defaults=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c44635b4-11ad-46b9-8051-54c932f4ef97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1c8ca705c14492afe0b34f81a83479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9eccb7f2564c389a55e75d4b9979d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'datasets/'\n",
    "\n",
    "main =  [{'model': 'dimerization', 'N': 40},\n",
    "         {'model': 'immobile_traps', 'N': 53}\n",
    "        ]\n",
    "\n",
    "\n",
    "trajs, labels = dp.create_dataset(T = 500, dics = main, N_model = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31ece761-d167-4242-87c2-c11405a3c5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 93, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1969e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NBDEV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f33eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted analysis_methods.ipynb.\n",
      "Converted datasets_phenom.ipynb.\n",
      "Converted datasets_theory.ipynb.\n",
      "Converted models_phenom.ipynb.\n",
      "Converted models_theory.ipynb.\n",
      "Converted utils_challenge.ipynb.\n",
      "Converted utils_trajectories.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc036d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
