{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ed7396-cf14-41bb-bc47-d2aa02adb540",
   "metadata": {},
   "source": [
    "# datasets_phenom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets_phenom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from andi_datasets.models_phenom import models_phenom\n",
    "from andi_datasets.utils_trajectories import segs_inside_fov\n",
    "from andi_datasets.utils_challenge import label_filter, continuous_label_to_list, extract_ensemble\n",
    "\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dedb5e0",
   "metadata": {},
   "source": [
    "## Class constructor\n",
    "\n",
    "The class is initiated by accessing the `models_phenom` class and inspecting the available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70947895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class datasets_phenom():\n",
    "    def __init__(self,\n",
    "                models_class = models_phenom()):\n",
    "            ''' Constructor of the class '''\n",
    "            self.models_class = models_class\n",
    "            self._get_models()\n",
    "        \n",
    "    def _get_models(self):        \n",
    "        '''Loads the available models from the subclass'''\n",
    "\n",
    "        available_models = inspect.getmembers(self.models_class, inspect.ismethod)      \n",
    "        available_models = available_models[1:][::-1] # we need this to get rid of the init\n",
    "        self.avail_models_name = [x[0] for x in available_models]\n",
    "        self.avail_models_func = [x[1] for x in available_models]\n",
    "        \n",
    "    def _get_inputs_models(self, model, get_default_values = False):\n",
    "        \n",
    "        model_f = self.avail_models_func[self.avail_models_name.index(model)] \n",
    "        defaults = inspect.getfullargspec(model_f).defaults\n",
    "        params = inspect.getfullargspec(model_f).args[1:]\n",
    "        if get_default_values:\n",
    "            return params, defaults\n",
    "        else:\n",
    "            return params\n",
    "        \n",
    "    def _get_states(self):\n",
    "        ''' Definition of the possible states found in the ANDI 2022 challenge and their \n",
    "        assigned label:\n",
    "        0: immobile; 1: confined; 2: brownian; 3: anomalous '''\n",
    "        \n",
    "        self._states = ['immobile', 'confined', 'brownian', 'anomalous']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00c135",
   "metadata": {},
   "source": [
    "# `create_dataset`\n",
    "\n",
    "This function receives a list of dictionaries, each containing the properties of the trajectories to be created. The compulsory input for each dictionary is the key `model`, which defined the phenomenological diffusion model from which to create the trajectories. The rest of the properties are the ones of the model called. If no properties are given, the function automatically choses the default parameters of the model (check `models_phenom` for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "#| export\n",
    "\n",
    "class datasets_phenom(datasets_phenom):\n",
    "                \n",
    "    def create_dataset(self,\n",
    "                       dics = False,\n",
    "                       T = None,\n",
    "                       N_model = None,  \n",
    "                       path = '',\n",
    "                       save = False, load = False):\n",
    "        ''' Given a list of dictionaries, generates trajectories of the demanded properties. \n",
    "        This function checks and handles the input dataset and the manages both the creation,\n",
    "        loading and saving of trajectories.\n",
    "        Args:\n",
    "            :dics (list, dictionary, bool):\n",
    "                - if list or dictionary: the function generates trajectories with the \n",
    "                properties stated in each dictionary.\n",
    "                - if bool: the function generates trajectories with default parameters.\n",
    "                set for the ANDI2022 challenge for every available diffusion model.\n",
    "            :T (int or None): \n",
    "                - if int: overrides the values of trajectory length in the dictionaries.\n",
    "                - if None: uses the trajectory length values in the dictionaries.\n",
    "            :N_model (int or None):\n",
    "                - if int: overrides the values of number of trajectories in the dictionaries.\n",
    "                - if None: uses the number of trajectories in the dictionaries\n",
    "            :save (bool): if True, saves the generated dataset (see self._save_trajectories).\n",
    "            :load (bool): if True, loads a dataset from path (see self._load_trajectories).\n",
    "            :path (str): path from where to save or load the dataset.\n",
    "        '''\n",
    "        \n",
    "        self.T = T\n",
    "        self.N_model = N_model\n",
    "        self.path = path\n",
    "        self.dics = dics\n",
    "        \n",
    "        '''Managing dictionaries'''\n",
    "        # If the input is a single dictionary, transform it to list\n",
    "        if isinstance(self.dics, dict): self.dics = [self.dics]\n",
    "        # if dics is False, we select trajectories from all models with default values\n",
    "        if self.dics is False: self.dics = [{'model': model} for model in self.avail_models_name]\n",
    "\n",
    "                    \n",
    "        '''Managing folders of the datasets'''  \n",
    "        self.save = save\n",
    "        self.load = load\n",
    "        if self.save or self.load:                \n",
    "            if self.load:\n",
    "                self.save = False            \n",
    "            if not os.path.exists(self.path) and self.load:\n",
    "                raise FileNotFoundError('The directory from where you want to load the dataset does not exist')                \n",
    "            if not os.path.exists(self.path) and self.save:\n",
    "                os.makedirs(self.path) \n",
    "                \n",
    "                \n",
    "        '''Create trajectories'''\n",
    "        trajs, labels = self._create_trajectories()\n",
    "        \n",
    "        return trajs, labels                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662e5de",
   "metadata": {},
   "source": [
    "# `_create_trajectories`, `_save_trajectories`, `_load_trajectories`\n",
    "Auxiliary functions to `create_trajectories` that allow for creating, load and saving trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95578b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class datasets_phenom(datasets_phenom):   \n",
    "    \n",
    "    def _create_trajectories(self):\n",
    "        ''' Given a list of dictionaries, generates trajectories of the demanded properties.\n",
    "        First checks in the .csv of each demanded model if a dataset of similar properties\n",
    "        exists. If it does, it loads it from the corresponding file.\n",
    "        Return:\n",
    "            :data_t (array): array containing the generated trajectories\n",
    "            :data_l (array): array containing the corresponding labels.\n",
    "        '''\n",
    "\n",
    "        for dic in self.dics:\n",
    "            \n",
    "            dataset_idx, df = self._inspect_dic(dic)\n",
    "            \n",
    "            # If the dataset does not yet exists\n",
    "            if dataset_idx is False:\n",
    "                # Retrieve name and function of diffusion model\n",
    "                model_f = self.avail_models_func[self.avail_models_name.index(dic['model'])]\n",
    "                # Create dictionary with only arguments\n",
    "                dic_args = dict(dic); dic_args.pop('model')\n",
    "                \n",
    "                trajs, labels = model_f(**dic_args)\n",
    "                \n",
    "                # Save the trajectories if asked\n",
    "                if self.save:\n",
    "                    self._save_trajectories(trajs = trajs,\n",
    "                                            labels = labels,\n",
    "                                            dic = dic, \n",
    "                                            df = df,\n",
    "                                            dataset_idx = dataset_idx,\n",
    "                                            path = self.path)                    \n",
    "            else:\n",
    "                trajs, labels = self._load_trajectories(model_name = dic['model'],\n",
    "                                                        dataset_idx = dataset_idx,\n",
    "                                                        path = self.path)\n",
    "                \n",
    "            # Stack dataset\n",
    "            try:\n",
    "                data_t = np.hstack((data_t, trajs))                    \n",
    "                data_l = np.hstack((data_l, labels))\n",
    "            except:\n",
    "                data_t = trajs\n",
    "                data_l = labels\n",
    "                    \n",
    "        return data_t, data_l  \n",
    "    \n",
    "    def _save_trajectories(self, trajs, labels, dic, df, dataset_idx, path):\n",
    "        ''' Given a set of trajectories and labels, saves two things:\n",
    "                - In the .csv corresponding to the demanded model, all the input parameters \n",
    "                of the generated dataset.\n",
    "                - In a .npy file, the trajectories and labels generated.\n",
    "        '''\n",
    "        \n",
    "        file_name = path+dic['model']+'_'+str(df.shape[0])+'.npy'\n",
    "        \n",
    "        # Save information in CSV handler\n",
    "        df = df.append(dic, ignore_index = True)\n",
    "        df.to_csv(path+dic['model']+'.csv')\n",
    "        \n",
    "        # Save trajectories and labels\n",
    "        data = np.dstack((trajs, labels))\n",
    "        np.save(file_name, data)\n",
    "        \n",
    "    def _load_trajectories(self, model_name, dataset_idx, path):\n",
    "        ''' Given the path for a dataset, loads the trajectories and labels'''\n",
    "        \n",
    "        file_name = path+model_name+'_'+str(dataset_idx)+'.npy'\n",
    "        data = np.load(file_name)\n",
    "        return data[:, :, :2], data[:, :  , 2:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910bb7b-38be-4eb1-b00c-e12376b6da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dph = datasets_phenom()\n",
    "path = 'data/'\n",
    "\n",
    "main =  [{'model': 'dimerization', 'N': 40}\n",
    "        ]\n",
    "\n",
    "\n",
    "trajs, labels = dph.create_dataset(T = None, dics = main, N_model = None, path = path, load = False)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433ac6d",
   "metadata": {},
   "source": [
    "# `_inspect_dic`\n",
    "Given a dictionary, this function checks that it fulfils the constraints of the program and checks the validity of the save/load actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class datasets_phenom(datasets_phenom):   \n",
    "\n",
    "    def _inspect_dic(self, dic):\n",
    "        '''Checks the information of the input dictionaries, complete missing information with defeault \n",
    "        values and then decides about loading/saving depending on parameters.\n",
    "        '''        \n",
    "            \n",
    "        # Add time and number of trajectories information\n",
    "        if self.N_model is not None:\n",
    "            dic['N'] = self.N_model\n",
    "        if self.T is not None:\n",
    "            dic['T'] = self.T\n",
    "\n",
    "        # Check if CSV with information of dataset exists. If not, create it\n",
    "        model_m = dic['model']\n",
    "        model_f = self.avail_models_func[self.avail_models_name.index(model_m)]    \n",
    "        # Check arguments and defaults from model's function            \n",
    "        args = inspect.getfullargspec(model_f).args[1:]\n",
    "        defaults = inspect.getfullargspec(model_f).defaults\n",
    "        try:\n",
    "            df = pd.read_csv(self.path+model_m+'.csv', index_col=0)\n",
    "        except:                \n",
    "            # convert to dataframe and add model\n",
    "            df = pd.DataFrame(columns = args+['model'])                \n",
    "\n",
    "        # Assign missing keys in dic with default values\n",
    "        for arg, default in zip(args, defaults):\n",
    "            if arg not in dic.keys():\n",
    "                dic[arg] = default\n",
    "\n",
    "        # Check if updated keys of dic equal keys of csv.\n",
    "        if set(list(df.keys())) != set(list(dic.keys())):\n",
    "            raise ValueError('Input model dictionary does not match models properties')\n",
    "\n",
    "        # Check if the dataset already exists:\n",
    "        df_conditions = df.copy()\n",
    "        df_conditions = df_conditions.where(pd.notnull(df_conditions), None) # Need in case of empty elements because deafults are None\n",
    "        for key in dic:\n",
    "            # We need to transform it to str to do a fair comparison between matrices (e.g. transition matrix, Ds, alphas,...)\n",
    "            df_conditions = df_conditions.loc[(df_conditions[key].astype(str) == str(dic[key]))]\n",
    "            if len(df_conditions.index) == 0:\n",
    "                break\n",
    "\n",
    "        # If dataset exists\n",
    "        if len(df_conditions.index) > 0:\n",
    "            # if the dataset exists and save was True, do not save but load\n",
    "            if self.save:\n",
    "                wrn_str = f'The dataset you want to save already exists (file: {model_m}_{df_conditions.index[0]}.npy). Switching to Load mode.'\n",
    "                warnings.warn(wrn_str)\n",
    "                dataset_idx = df_conditions.index[0] \n",
    "            elif self.load:\n",
    "                dataset_idx = df_conditions.index[0]\n",
    "            else:\n",
    "                dataset_idx = False                 \n",
    "\n",
    "        # If dataset does no exists\n",
    "        else:         \n",
    "            if self.load:\n",
    "                raise ValueError('The dataset you want to load does not exist.')\n",
    "            else: # If the dataset does not exist, append empty string.\n",
    "                # This allows to mix saving and loading\n",
    "                dataset_idx = False\n",
    "                \n",
    "        return dataset_idx, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48e4ce-68d8-44e6-8af9-7f2ab03b5bd0",
   "metadata": {},
   "source": [
    "# `_get_args`\n",
    "Given the name of a model, returns its input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c94758-6ee6-4c5c-bd90-a0213363ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class datasets_phenom(datasets_phenom):  \n",
    "    def _get_args(self, model, return_defaults = False):\n",
    "        ''' Given the name of a diffusion model, return its inputs arguments.\n",
    "        Args:\n",
    "            :model (str): name of the diffusion model (see self.available_models_name)\n",
    "            :return_defaults (optional, bool): if True, the function will also return\n",
    "            the default values of each input argument.\n",
    "        Return:\n",
    "            :args (list): list of input arguments\n",
    "            :defaults (optional, list): list of default value for the input arguments.\n",
    "        '''\n",
    "        model_f = self.avail_models_func[self.avail_models_name.index(model)]    \n",
    "        # Check arguments and defaults from model's function            \n",
    "        args = inspect.getfullargspec(model_f).args[1:]\n",
    "        defaults = inspect.getfullargspec(model_f).defaults\n",
    "        if return_defaults:\n",
    "            return args, defaults\n",
    "        else:\n",
    "            return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c56280-9c97-45ca-aa74-2272deb1916e",
   "metadata": {},
   "source": [
    "# Challenge 2022 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d934936-9348-45d1-8e4c-3e396a1a1a8a",
   "metadata": {},
   "source": [
    "## Default challenge parameters\n",
    "This function generates dictionaries of plausible parameters for each experiment. We start with the following groudbase values:\n",
    "\n",
    "\n",
    "- Fielf of view (fov): $128 x 128 px^2$, with a pixel size of 100 nm, for a size of the box $FOV=12.8 \\ \\mu m$. We will simulate trajectories in a box of $L = 2*FOV$ and the only considers segments inside the fov. This allows to eliminate problems with boundaries.\n",
    "- Frame rate $= 0.1 Hz$, i.e. $\\Delta t = 100 \\ ms  = 0.1 \\ s$ .\n",
    "- Typical $D = 0.01 \\ \\mu m^2/s$. To calculate the input of the program, we need to consider the following:\n",
    "    - As a working definition of $D$, valid also for anomalous diffusion, we consider it as proporcional to the variance of the displacements along one dimension at the shortest time lag, i.e. $\\sigma_{\\Delta x}^2= 2  D \\Delta t $\n",
    "    - Given the values of pixel size and frame rate, in adimensional unit $D$ is given by: $D= 0.01 \\ \\frac{\\mu m^2}{s} \\ \\frac{0.1 s/ \\Delta t }{  0.01 \\mu m^2/px^2} = 0.1 px^2/\\Delta t $ .\n",
    "    - Localization precision $\\sigma_{x} = 12 \\ nm = \\frac{12 \\ nm} {100 \\ nm/px}  = 0.12 \\ px$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292589ca-2b22-4663-859e-32faea4e9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class datasets_phenom(datasets_phenom): \n",
    "    \n",
    "    class _df_andi2: \n",
    "        'This class is used to define some of the default values set for the ANDI 2022 challenge'\n",
    "        def __init__(self):        \n",
    "            # General parameters\n",
    "            \n",
    "            self.T = 500                   # Length of simulated trajectories\n",
    "            self._min_T = 20               # Minimal length of output trajectories\n",
    "            self.FOV_L = 128               # Length side of the FOV (px)\n",
    "            self.L = 1.2*self.FOV_L          # Length of the simulated environment\n",
    "            'PARTICLES MOVE TOO SLOWLY WITH D = 0.1!!'\n",
    "            self.D = 1                     # Baseline diffusion coefficient (px^2/frame)\n",
    "            self.density = 2               # Particle density   \n",
    "            self.N = 50                    # Number of particle in the whole experiment\n",
    "            self.sigma_noise = 0.12        # Variance of the localization noise\n",
    "            \n",
    "            self.label_filter = lambda x: label_filter(x, window_size = 5, min_seg = 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dbe77b-32d9-4a4f-a67a-3f4cb21b7fef",
   "metadata": {},
   "source": [
    "### `get_dic_andi2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5669e10-0fe0-45c7-876e-6c8b5e2cacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class datasets_phenom(datasets_phenom):             \n",
    "    def _get_dic_andi2(self, model):\n",
    "        ''' Given the number label of diffusion model, returns a default\n",
    "        dictionary of the model's parameters to be fed to create_dataset\n",
    "        The numeration is as follow:\n",
    "                1: single state\n",
    "                2: N-state\n",
    "                3: immobilization\n",
    "                4: dimerization\n",
    "                5: confinement\n",
    "        Args:\n",
    "            :model (int in [1,6]): number of the diffusion model\n",
    "        Return:\n",
    "            :dic (dictionary): dictionary containing the default parameters\n",
    "            for ANDI2022 of the indicated model.\n",
    "        '''\n",
    "        \n",
    "        dic = {'N': self._df_andi2().N,\n",
    "               'T': self._df_andi2().T,\n",
    "               'L': self._df_andi2().L}\n",
    "        \n",
    "        # alpha and D for single-state and immobilization\n",
    "        if model == 1 or model == 3:    \n",
    "            dic.update({'Ds': [self._df_andi2().D, self._df_andi2().D*0.01], # mean and varianve for D\n",
    "                        'alphas': np.array([np.random.rand()*(1.5-0.5)+0.5, 0.01])})\n",
    "            \n",
    "        # alphas and Ds for 2-state, confinement and dimerization\n",
    "        if model == 2 or model == 4 or model == 5:            \n",
    "            \n",
    "            fast_D = self._df_andi2().D + np.random.randn()*self._df_andi2().D*0.01\n",
    "            slow_D = fast_D*np.random.rand()*(0.1-0.01)+0.01    \n",
    "            \n",
    "            alpha1 = np.random.rand()*(1.2-0.8)+0.8\n",
    "            # The second state will be at least 0.2 afar. We make sure not being \n",
    "            # outside [0,2]        \n",
    "            alpha2 = alpha1 - (np.random.rand()*(0.6-0.2)+0.2)\n",
    "            \n",
    "            dic.update({'Ds': np.array([[fast_D, 0.01],\n",
    "                                        [slow_D, 0.01]]),\n",
    "                        'alphas': np.array([[alpha1, 0.01],\n",
    "                                            [alpha2, 0.01]])})\n",
    "            \n",
    "        # Particle/trap radius and ninding and unbinding probs for dimerization and immobilization\n",
    "        if model == 3 or model == 4:\n",
    "            dic.update({'Pu': 0.01,                           # Unbinding probability\n",
    "                        'Pb': 1})                             # Binding probabilitiy\n",
    "            \n",
    "        if model == 1:\n",
    "            dic.update({'model': self.avail_models_name[0]})\n",
    "        \n",
    "        if model == 2:\n",
    "            dic.update({'model': self.avail_models_name[1],\n",
    "                        'M': np.array([[0.99, 0.01],            # Transition Matrix\n",
    "                                       [0.01, 0.99]]),\n",
    "                        'return_state_num': True              # To get the state numeration back, , hence labels.shape = TxNx4\n",
    "                       })\n",
    "        if model == 3:\n",
    "            dic.update({'model': self.avail_models_name[2],\n",
    "                        'Nt': 300,            # Number of traps (density = 1 currently)\n",
    "                        'r': 0.4}             # Size of trap\n",
    "                      )\n",
    "        if model == 4:\n",
    "            dic.update({'model': self.avail_models_name[3],\n",
    "                        'r': 0.6,                 # Size of particles\n",
    "                        'return_state_num': True  # To get the state numeration back, hence labels.shape = TxNx4\n",
    "                       })\n",
    "        \n",
    "        if model == 5:\n",
    "            dic.update({'model': self.avail_models_name[4],\n",
    "                        'r': 5,\n",
    "                        'Nc': 30,\n",
    "                        'trans': 0.1})\n",
    "\n",
    "        return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324074f8-432e-464c-b85d-9fbcff852ef0",
   "metadata": {},
   "source": [
    "## Generating function\n",
    "This function generates trajectory datasets like the ones considered in the ANDI 2022 Challenge. It is based in `models_phenom.create_dataset` but also applies:\n",
    "\n",
    "- Apply Field of View (FOV)\n",
    "- Add localization noise\n",
    "- Smooth labels\n",
    "- Extracts ensemble properties\n",
    "- Correct labeling of trajectories\n",
    "\n",
    "\n",
    "**Inputs:**\n",
    "- Number of experiments (one experiment = one model).\n",
    "\n",
    "    For each experiment:\n",
    "    - Number of particles\n",
    "    - Number of FOVs\n",
    "    - Parameters of the model\n",
    "    - Mininum length of trajectories\n",
    "    \n",
    "**Outputs:** (this should be the same as the expected challenge inputs)\n",
    "    \n",
    "- For each FOV:\n",
    "    - Ensemble properties (Compulsory: model, $\\alpha$ and $D$ distribution)\n",
    "    - Trajectory properties (list of properties: $\\alpha_1$, $D_1$, CP$_1$, $\\alpha_2$, $D_2$, CP$_2$,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e690bf-c32c-4e6a-bc35-b5292bdd8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class datasets_phenom(datasets_phenom):\n",
    "    \n",
    "    def challenge_2022_dataset(self, \n",
    "                              experiments = 5,\n",
    "                              dics = None,\n",
    "                              repeat_exp = True,\n",
    "                              num_fovs = 20,\n",
    "                              return_timestep_labs = False,\n",
    "                              save_data = False,\n",
    "                              path = 'data/',\n",
    "                              prefix = ''\n",
    "                                ):\n",
    "        ''' \n",
    "        Creates a datasets with same structure as ones given in the ANDI 2022 challenge. Default values for the\n",
    "        various diffusion models have been set such as to be in the same ranges as the ones expected for the\n",
    "        challenge. For details, check the ANDI 2022 challenge webpage.\n",
    "        This function will generate as many experiments (associated to one the diffusion models) as demanded.\n",
    "        There are two ways of defining that:\n",
    "            - Give number of experiments (and optional parameters such as repeat_exp) to create. The diffusion\n",
    "            parameters are then taken from the default values are taken from datasets_phenom._df_andi2.\n",
    "            - Feed a list of dictionaries (dics) from which data will be generated\n",
    "        For each experiment, as many field of view as wanted can be generated        \n",
    "        \n",
    "        Args:  \n",
    "            :experiments (int, list): - if int: Number of experiments to generate. Each experiment is \n",
    "                                                generated from one of the available diffusion models.  \n",
    "                                      - if list: diffusion models to generate\n",
    "            :dics (dictionary, list of dics): if given, uses this to set the parameters of the experiments\n",
    "                                              Must be of length equal to experiments. This overrides any\n",
    "                                              info about chosen models, as the model is set by the dictio-\n",
    "                                              nary.\n",
    "            :repeat_exp (bool, list): (Does not enter into play if experiments is list)\n",
    "                                      - True: picks at random the diffusion model from the pool\n",
    "                                      - False: picks the diffusion in an ordered way from the pool\n",
    "            :num_fovs (int): Number of field of views to get trajectories from in each experiment.\n",
    "            :return_timestep_labs (bool): if True, the output trajectories dataframes containing also the \n",
    "                                          labels alpha, D and state at each time step.\n",
    "            :save_data (bool): if True, saves all pertinent data.\n",
    "            :path (str): path where to store the data\n",
    "            :prefix (str): extra prefix that can be added in front of the files' names.\n",
    "            \n",
    "        Return:\n",
    "            :trajs_out (list): list of lenght (experiments x num_fovs). Each elements are is dataframe\n",
    "                               containing the trajectories of a particular experiment/fov, in order of \n",
    "                               generation (i.e. [exp1_fov1, exp1_fov2, ..., exp2_fov1 ....]).\n",
    "                               If return_timestep_labs = True, the dataframes also contain the labels\n",
    "                               at each time step\n",
    "            :labels_traj_out (list): list of same length of trajs_out containing the labels of the \n",
    "                                     corresponding trajectories. Each element contains a list with the \n",
    "                                     labels of each trajectory, following the scheme:\n",
    "                                     [idx_traj, D_1, alpha_1, state_1, CP_1, D_2, alpha_2, .... state_N]\n",
    "            :labels_ens_out (list): list of same length of trajs_out containing the ensemble labels of \n",
    "                                    given experiment. See description of output matrix in \n",
    "                                    utils_challenge._extract_ensemble()\n",
    "            '''\n",
    "        \n",
    "\n",
    "\n",
    "        # Set prefixes for saved files\n",
    "        if save_data:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            pf_labs_traj = path+prefix+'traj_labs'\n",
    "            pf_labs_ens = path+prefix+'ens_labs'\n",
    "            pf_trajs = path+prefix+'trajs'\n",
    "\n",
    "        if return_timestep_labs:\n",
    "            df_list = []\n",
    "\n",
    "        # Sets the models of the experiments that will be output by the function\n",
    "        if dics is None:\n",
    "            if isinstance(experiments, int):\n",
    "                if repeat_exp: # If experiments can be repeated, we just sample randomly\n",
    "                    model_exp = np.random.randint(len(self.avail_models_name), size = experiments)\n",
    "                else: # If not, we sampled them in an ordered way\n",
    "                    if experiments >= len(self.avail_models_name):\n",
    "                        num_repeats = (experiments % len(self.avail_models_name))+1\n",
    "                    else:\n",
    "                        num_repeats = 1\n",
    "                    model_exp = np.tile(np.arange(len(self.avail_models_name)), num_repeats)[:experiments]\n",
    "                # We add one to get into non-Python numeration\n",
    "                model_exp += 1\n",
    "            else:\n",
    "                model_exp = experiments\n",
    "        # If list of dics is given, then just create a list of length = len(dics)\n",
    "        else: \n",
    "            model_exp = [0]*len(dics)\n",
    "\n",
    "        # Output lists\n",
    "        trajs_out, labels_traj_out, labels_ens_out = [], [], []\n",
    "        for idx_experiment, model in enumerate(tqdm(model_exp)):\n",
    "            \n",
    "            ''' Generate the trajectories '''\n",
    "            if dics is None:\n",
    "                dic = self._get_dic_andi2(model)\n",
    "            else:\n",
    "                dic = dics[idx_experiment]\n",
    "                # Overide the info about model\n",
    "                model = self.avail_models_name.index(dic['model'])+1\n",
    "                \n",
    "            trajs, labels = self.create_dataset(dics = dic)\n",
    "\n",
    "            # Add noise the trajectories\n",
    "            trajs += np.random.randn(*trajs.shape)*self._df_andi2().sigma_noise    \n",
    "\n",
    "            ''' Apply the FOV '''\n",
    "            for fov in range(num_fovs):\n",
    "                \n",
    "                # Checking if file exist and creating an error\n",
    "                if save_data:\n",
    "                    if os.path.exists(pf_labs_traj+f'_exp_{idx_experiment}_fov_{fov}.txt') or os.path.exists(pf_labs_ens+f'_exp_{idx_experiment}_fov_{fov}.txt'):\n",
    "                        raise FileExistsError(f'Target files for experiment {idx_experiment} and FOV {fov}. Delete the file or change path/prefix.')            \n",
    "\n",
    "\n",
    "                        \n",
    "                # We take as min/max for the fovs a 5 % distance of L\n",
    "                dist = 0.05\n",
    "                min_fov = int(dist*self._df_andi2().L)\n",
    "                max_fov = int((1-dist)*self._df_andi2().L)-self._df_andi2().FOV_L\n",
    "                # sample the position of the FOV\n",
    "                fov_origin = (np.random.randint(min_fov, max_fov), np.random.randint(min_fov, max_fov))\n",
    "               \n",
    "\n",
    "                ''' Go over trajectories in FOV (copied from utils_trajectories for efficiency) '''\n",
    "                trajs_fov, array_labels_fov, list_labels_fov, idx_segs_fov, frames_fov = [], [], [], [], []\n",
    "                idx_seg = -1\n",
    "                \n",
    "                # Total frames\n",
    "                frames = np.arange(trajs.shape[0])\n",
    "                for idx, (traj, label) in enumerate(zip(trajs[:, :, :].transpose(1,0,2),\n",
    "                                                        labels[:, :, :].transpose(1,0,2))):\n",
    "                    nan_segms = segs_inside_fov(traj, \n",
    "                                                fov_origin = fov_origin,\n",
    "                                                fov_length = self._df_andi2().FOV_L,\n",
    "                                                cutoff_length = self._df_andi2()._min_T)\n",
    "\n",
    "                    if nan_segms is not None:\n",
    "                        for idx_nan in nan_segms:  \n",
    "                            idx_seg+= 1\n",
    "\n",
    "                            seg_x = traj[idx_nan[0]:idx_nan[1], 0]\n",
    "                            seg_y = traj[idx_nan[0]:idx_nan[1], 1]\n",
    "                            \n",
    "                            \n",
    "                            trajs_fov.append(np.vstack((seg_x, seg_y)).transpose())\n",
    "                            frames_fov.append(frames[idx_nan[0]:idx_nan[1]])\n",
    "                            \n",
    "                            lab_seg = []\n",
    "                            for idx_lab in range(labels.shape[-1]):\n",
    "                                lab_seg.append(self._df_andi2().label_filter(label[idx_nan[0]:idx_nan[1], idx_lab]))\n",
    "                            lab_seg = np.vstack(lab_seg).transpose()                    \n",
    "                            array_labels_fov.append(lab_seg)\n",
    "\n",
    "                            # Tranform continuous labels to list for correct output\n",
    "                            if model == 2 or model == 4: \n",
    "                                # if multi-state or dimerization, we get rid of the label of state numbering\n",
    "                                CP, alphas, Ds, states = continuous_label_to_list(lab_seg[:, :-1])\n",
    "                            else:\n",
    "                                CP, alphas, Ds, states = continuous_label_to_list(lab_seg)\n",
    "\n",
    "                            list_gt = [idx_seg, Ds[0], alphas[0], states[0]]\n",
    "                            for gtc, gta, gtd, gts in zip(CP, alphas[1:], Ds[1:], states[1:]):\n",
    "                                list_gt += [gtc, gtd, gta, gts]\n",
    "                            list_labels_fov.append(list_gt)     \n",
    "\n",
    "                            if save_data:\n",
    "                                with open(pf_labs_traj+f'_exp_{idx_experiment}_fov_{fov}.txt', 'a') as f:\n",
    "                                    writer = csv.writer(f, delimiter=',', lineterminator='\\n',)\n",
    "                                    writer.writerow(list_gt)\n",
    "\n",
    "                            # Save index of segment with its length to latter append in the dataframe    \n",
    "                            idx_segs_fov.append(np.ones_like(seg_x)*idx_seg)\n",
    "\n",
    "                '''Extract ensemble trajectories''' \n",
    "                ensemble_fov = extract_ensemble(np.concatenate(array_labels_fov)[:, -1], dic)\n",
    "                \n",
    "                df_data = np.hstack((np.expand_dims(np.concatenate(idx_segs_fov), axis=1),\n",
    "                                     np.expand_dims(np.concatenate(frames_fov), axis=1),\n",
    "                                     np.concatenate(trajs_fov)))\n",
    "                df_traj = pd.DataFrame(df_data, columns = ['traj_idx', 'frame', 'x', 'y']) \n",
    "\n",
    "\n",
    "                if return_timestep_labs:\n",
    "                    array_labels_fov = np.concatenate(array_labels_fov)\n",
    "                    df_traj['alpha'] = array_labels_fov[:, 0]\n",
    "                    df_traj['D'] = array_labels_fov[:, 1]\n",
    "                    df_traj['state'] = array_labels_fov[:, 2]\n",
    "\n",
    "                if save_data:\n",
    "                    # Trajectories                    \n",
    "                    df_traj.to_csv(pf_trajs+f'_exp_{idx_experiment}_fov_{fov}.csv', index = False)\n",
    "                    # Ensemble labels\n",
    "                    with open(pf_labs_ens+f'_exp_{idx_experiment}_fov_{fov}.txt', 'a') as f:\n",
    "                        if model == 2: num_states = dic['alphas'].shape[0]\n",
    "                        elif model == 1: num_states = 1\n",
    "                        else: num_states = 2\n",
    "                        model_n = dic['model']\n",
    "                        f.write(f'model: {model_n}; num_state: {num_states} \\n')\n",
    "                        np.savetxt(f, ensemble_fov, delimiter = ';')\n",
    "\n",
    "\n",
    "                # Add data to main lists (trajectories and lists with labels)   \n",
    "                trajs_out.append(df_traj)\n",
    "                labels_traj_out.append(list_labels_fov)\n",
    "                labels_ens_out.append(ensemble_fov)\n",
    "\n",
    "        return trajs_out, labels_traj_out, labels_ens_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ebbd5-ade4-4afa-a435-1db6068a4d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6bd47fc6b14d858f793d2ac379c798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m dph \u001b[38;5;241m=\u001b[39m datasets_phenom()\n\u001b[0;32m      2\u001b[0m num_experiments, num_fovs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 3\u001b[0m df_list, lab_t, lab_e \u001b[38;5;241m=\u001b[39m \u001b[43mdph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchallenge_2022_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiments\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_experiments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mnum_fovs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_fovs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mreturn_timestep_labs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mrepeat_exp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mdatasets_phenom.challenge_2022_dataset\u001b[1;34m(self, experiments, dics, repeat_exp, num_fovs, return_timestep_labs, save_data, path, prefix)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# Overide the info about model\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavail_models_name\u001b[38;5;241m.\u001b[39mindex(dic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 102\u001b[0m trajs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Add noise the trajectories\u001b[39;00m\n\u001b[0;32m    105\u001b[0m trajs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m*\u001b[39mtrajs\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df_andi2()\u001b[38;5;241m.\u001b[39msigma_noise    \n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mdatasets_phenom.create_dataset\u001b[1;34m(self, dics, T, N_model, path, save, load)\u001b[0m\n\u001b[0;32m     51\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath) \n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m'''Create trajectories'''\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m trajs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trajs, labels\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mdatasets_phenom._create_trajectories\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create dictionary with only arguments\u001b[39;00m\n\u001b[0;32m     22\u001b[0m dic_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(dic); dic_args\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m trajs, labels \u001b[38;5;241m=\u001b[39m model_f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdic_args)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Save the trajectories if asked\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave:\n",
      "File \u001b[1;32mc:\\users\\gorka\\github\\andi_datasets\\andi_datasets\\models_phenom.py:1067\u001b[0m, in \u001b[0;36mmodels_phenom.confinement\u001b[1;34m(self, N, T, Ds, alphas, gamma_d, epsilon_a, L, deltaT, r, comp_center, Nc, trans)\u001b[0m\n\u001b[0;32m   1060\u001b[0m alphas_traj, Ds_traj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_diff_parameters(alphas \u001b[38;5;241m=\u001b[39m alphas,\n\u001b[0;32m   1061\u001b[0m                                                     Ds \u001b[38;5;241m=\u001b[39m Ds,\n\u001b[0;32m   1062\u001b[0m                                                     num_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1063\u001b[0m                                                     epsilon_a \u001b[38;5;241m=\u001b[39m epsilon_a,\n\u001b[0;32m   1064\u001b[0m                                                     gamma_d \u001b[38;5;241m=\u001b[39m gamma_d)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# Get trajectory from single traj function\u001b[39;00m\n\u001b[1;32m-> 1067\u001b[0m pos, lab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_confinement_traj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mDs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDs_traj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43malphas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malphas_traj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mdeltaT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdeltaT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mcomp_center\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcomp_center\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mNc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1076\u001b[0m data[:, n, :] \u001b[38;5;241m=\u001b[39m pos\n\u001b[0;32m   1077\u001b[0m labels[:, n, :] \u001b[38;5;241m=\u001b[39m lab\n",
      "File \u001b[1;32mc:\\users\\gorka\\github\\andi_datasets\\andi_datasets\\models_phenom.py:1023\u001b[0m, in \u001b[0;36mmodels_phenom._confinement_traj\u001b[1;34m(T, Ds, alphas, L, deltaT, r, comp_center, Nc, trans)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# Define state of particles based on the state array. First free/directed\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alphas[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m models_phenom()\u001b[38;5;241m.\u001b[39malpha_directed:\n\u001b[1;32m-> 1023\u001b[0m     labels[state \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m models_phenom()\u001b[38;5;241m.\u001b[39mlab_state\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1025\u001b[0m     labels[state \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m models_phenom()\u001b[38;5;241m.\u001b[39mlab_state\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dph = datasets_phenom()\n",
    "num_experiments, num_fovs = 5, 5\n",
    "df_list, lab_t, lab_e = dph.challenge_2022_dataset(experiments = num_experiments,\n",
    "                                                   num_fovs = num_fovs, \n",
    "                                                   return_timestep_labs=True, \n",
    "                                                   repeat_exp = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d8b32-d6f4-457a-a5ce-041e07478675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b848055d2c46468c6a95284a1745da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dph = datasets_phenom()\n",
    "num_experiments = 1\n",
    "dic = dph._get_dic_andi2(3)\n",
    "    \n",
    "df_list, lab_t, lab_e = dph.challenge_2022_dataset(save_data = False,\n",
    "                                                   dics = [dic],\n",
    "                                                   num_fovs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1969e0e",
   "metadata": {},
   "source": [
    "# NBDEV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4eb709-6e09-409a-a2f8-6546bdfd8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc036d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
