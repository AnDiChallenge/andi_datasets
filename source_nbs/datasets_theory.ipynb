{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91791a46",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Changes in ANDI 1.0\n",
    "\n",
    "- class `andi` $\\rightarrow$ `andi_theory`\n",
    "\n",
    "- function `andi.andi_dataset` $\\rightarrow$ `andi_theory.challenge_2020_dataset`\n",
    "\n",
    "\n",
    "- variable name change in `andi_theory.create_dataset`: `N` $\\rightarrow$ `N_model`. In this way, it is more clear that `N_model` refers to number of trajectories per model and we use `N` for total number of trajectories in other functions\n",
    "\n",
    "- corrected how noise is applied in Task3. Now the noise is added after the segmentation.\n",
    "\n",
    "- Added an extra variable in `andi_theory.challenge_2020_dataset`, `return_noise` which, if `True`, makes the function output the noise amplitudes added to each trajectory\n",
    "\n",
    "- Change how noise is applied in `andi_theory.challenge_2020_dataset`, such that all components of trajectories with dimension 2 and 3 have the same noise amplitude.\n",
    "\n",
    "## To do:\n",
    "- [ ] simplifiy num_per_class such that N_models can only be int (see datasets_phenom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6adb1f2",
   "metadata": {},
   "source": [
    "# Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dfbce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import andi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c157e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets_theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a775b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import os\n",
    "import inspect\n",
    "import h5py\n",
    "from tqdm import trange\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c41e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from andi_datasets.utils_trajectories import normalize\n",
    "from andi_datasets.models_theory import models_theory as models_theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e4fb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main fuctions\n",
    "All this class was inhereted from andi-datasets 1.0. Small changes to nomenclature were implemented, but the structure is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62cd3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class datasets_theory():\n",
    "     \n",
    "    def __init__(self):        \n",
    "        '''Constructor of the class'''\n",
    "        self._dimension = 1\n",
    "        self._get_models()\n",
    "        \n",
    "    def _get_models(self):        \n",
    "        '''Loading subclass of models'''\n",
    "        if self._dimension == 1:\n",
    "            self._models = models_theory._oneD()\n",
    "        elif self._dimension == 2:\n",
    "            self._models = models_theory._twoD()\n",
    "        elif self._dimension == 3:\n",
    "            self._models = models_theory._threeD()\n",
    "        else:\n",
    "            raise ValueError('Our current understanding of the physical world is three dimensional and so are the diffusion models available in this class')\n",
    "                \n",
    "        available_models = inspect.getmembers(self._models, inspect.ismethod)      \n",
    "        self.avail_models_name = [x[0] for x in available_models]\n",
    "        self.avail_models_func = [x[1] for x in available_models]\n",
    "    \n",
    "    def create_dataset(self, T, N_models, exponents, models,\n",
    "                       dimension = 1,\n",
    "                       save_trajectories = False, load_trajectories = False, \n",
    "                       path = 'datasets/',\n",
    "                       N_save = 1000, t_save = 1000):        \n",
    "        ''' Create a dataset of trajectories\n",
    "        Args:\n",
    "            :T (int):\n",
    "                - length of the trajectories.   \n",
    "            :N_models (int, numpy.array):\n",
    "                - if int, number of trajectories per class (i.e. exponent and model) in the dataset.\n",
    "                - if numpy.array, number of trajectories per classes: size (number of models)x(number of classes)    \n",
    "            :exponents (float, array):\n",
    "                - anomalous exponents to include in the dataset. Allows for two digit precision.\n",
    "            :models (bool, int, list):\n",
    "                - labels of the models to include in the dataset. Correspodance between models and labels\n",
    "                  is given by self.label_correspodance, defined at init.\n",
    "                  If int/list, choose the given models. If False, choose all of them.\n",
    "            :dimensions (int):\n",
    "                - Dimensions of the generated trajectories. Three possible values: 1, 2 and 3.\n",
    "            :save_trajectories (bool):\n",
    "                - - if True, the module saves a .h5 file for each model considered, with N_save trajectories \n",
    "                  and T = T_save.\n",
    "            :load_trajectories (bool):\n",
    "                - if True, the module loads the trajectories of an .h5 file.\n",
    "            :path (str):\n",
    "                - path to the folder where to save/load the trajectories dataset.\n",
    "            :N_save (int):\n",
    "                - Number of trajectories to save for each exponents/model. Advise: save at the beggining\n",
    "                  a big dataset (t_save ~ 1e3 and N_save ~ 1e4) which allows you to load any other combiantion\n",
    "                  of T and N_models.\n",
    "            :t_save (int):\n",
    "                - Length of the trajectories to be saved. See comments on N_save.                \n",
    "        Return:\n",
    "            :data_models (numpy.array):\n",
    "                - Dataset of trajectories of lenght Nx(T+2), with the following structure:\n",
    "                    o First column: model label \n",
    "                    o Second column: value of the anomalous exponent\n",
    "                    o 2:T columns: trajectories'''\n",
    "                    \n",
    "        '''Managing probable errors in inputs'''\n",
    "        if T < 2:\n",
    "            raise ValueError('The time of the trajectories has to be bigger than 1.')       \n",
    "        if isinstance(exponents, int) or isinstance(exponents, float):\n",
    "            exponents = [exponents]\n",
    "        \n",
    "        '''Managing folders of the datasets'''       \n",
    "        if save_trajectories or load_trajectories:                \n",
    "            if load_trajectories:\n",
    "                save_trajectories = False            \n",
    "            if not os.path.exists(path) and load_trajectories:\n",
    "                raise FileNotFoundError('The directory from where you want to load the dataset does not exist')                \n",
    "            if not os.path.exists(path) and save_trajectories:\n",
    "                os.makedirs(path)  \n",
    "                \n",
    "        '''Establish dimensions and corresponding models'''\n",
    "        self._dimension = dimension\n",
    "        self._get_models()\n",
    "                \n",
    "        '''Managing models to load'''       \n",
    "        # Load from a list of models\n",
    "        if isinstance(models, list): \n",
    "            self._models_name = [self.avail_models_name[idx] for idx in models]     \n",
    "            self._models_func = [self.avail_models_func[idx] for idx in models]\n",
    "        # Load from a single model\n",
    "        elif isinstance(models, int) and not isinstance(models, bool):\n",
    "            self._models_name = [self.avail_models_name[models]]\n",
    "            self._models_func = [self.avail_models_func[models]]\n",
    "        # Load all available models\n",
    "        else: \n",
    "            self._models_name =  self.avail_models_name\n",
    "            self._models_func =  self.avail_models_func\n",
    "            \n",
    "        '''Managing number of trajectory per class:\n",
    "            - Defines array num_class as a function of N'''                            \n",
    "        if isinstance(N_models, int): \n",
    "            n_per_class = N_models*np.ones((len(self._models_name), len(exponents)))\n",
    "            \n",
    "        elif type(N_models).__module__ == np.__name__: \n",
    "            if len(self._models_name) != N_models.shape[0] or len(exponents) != N_models.shape[1]:\n",
    "                raise ValueError('Mismatch between the dimensions of N and the number of different classes.'+\n",
    "                                 f'N must be either an int (balanced classes) or an array of length {len(models)}x'\n",
    "                                 f'{len(exponents)} (inbalaced classes).') \n",
    "            n_per_class = N_models\n",
    "        else:\n",
    "            raise TypeError('Type of variable N not recognized.')\n",
    "                    \n",
    "        '''Defining default values for saved datasets''' \n",
    "        N_save = np.ones_like(n_per_class)*N_save\n",
    "        # If the number of class of a given class is bigger than N_save, we\n",
    "        # change the value of N_save for that particular class.\n",
    "        N_save = np.max([N_save, n_per_class], axis = 0)      \n",
    "                \n",
    "        ''' Loading/Saving/Creating datasets'''\n",
    "        if load_trajectories:\n",
    "            data_models = self._load_trajectories(T = T,\n",
    "                                                 exponents = exponents,\n",
    "                                                 models_name = self._models_name,\n",
    "                                                 dimension = self._dimension,\n",
    "                                                 n_per_class = n_per_class,\n",
    "                                                 path = path,\n",
    "                                                 N_save = N_save,\n",
    "                                                 t_save = t_save)\n",
    "        elif save_trajectories:\n",
    "            self._save_trajectories(exponents = exponents,\n",
    "                                   dimension = self._dimension,\n",
    "                                   models_name = self._models_name,\n",
    "                                   models_func = self._models_func,\n",
    "                                   path = path, \n",
    "                                   n_per_class = n_per_class,\n",
    "                                   N_save = N_save,\n",
    "                                   t_save = t_save)\n",
    "            \n",
    "            data_models = self._load_trajectories(T = T,\n",
    "                                                 exponents = exponents,\n",
    "                                                 dimension = self._dimension,\n",
    "                                                 models_name = self._models_name,                                                 \n",
    "                                                 n_per_class = n_per_class,\n",
    "                                                 path = path,\n",
    "                                                 N_save = N_save,\n",
    "                                                 t_save = t_save)\n",
    "            \n",
    "        else:           \n",
    "            data_models = self._create_trajectories(T = T,                                                   \n",
    "                                                   exponents = exponents, \n",
    "                                                   dimension = self._dimension,\n",
    "                                                   models_name = self._models_name,\n",
    "                                                   models_func = self._models_func,\n",
    "                                                   n_per_class = n_per_class)       \n",
    "            \n",
    "        return data_models\n",
    "    \n",
    "    def _load_trajectories(self, T, exponents, dimension, \n",
    "                                models_name, n_per_class, \n",
    "                                path, N_save = 1000, t_save = 1000):\n",
    "        ''' Load trajectories from a h5py file of the given path. The name of the datasets in the\n",
    "        file have the following structure: \n",
    "            '(exponent with 2 digit_precision)_T_(lenght of trajectories in the dataset)_N_(number of trajectories in the dataset)'\n",
    "        Arguments: \n",
    "            :T (int):\n",
    "                - length of the trajectories.   \n",
    "            :exponents (array):\n",
    "                - anomalous exponents to include in the dataset. Allows for two digit precision.\n",
    "            :dimension (int):\n",
    "                - Dimensions of the generated trajectories. Three possible values: 1, 2 and 3.\n",
    "            :models_name (list of str):\n",
    "                - names of the models to include in the output dataset. \n",
    "            :n_per_class:\n",
    "                - number of trajectories to consider per exponent/model.\n",
    "            :path (str):\n",
    "                - path to the folder from where to load the trajectories dataset.\n",
    "            :t_save (int):\n",
    "                - length of the trajectories in the datasets to load.\n",
    "            :N_save (array):\n",
    "                - number of trajectories contained in the datasets to load.                  \n",
    "        Return:\n",
    "            :dataset (numpy.array):\n",
    "                - Dataset of trajectories of lenght (number of models)x(T+2), with the following structure:\n",
    "                    o First column: model label \n",
    "                    o Second column: value of the anomalous exponent\n",
    "                    o 2:T columns: trajectories'''\n",
    "                    \n",
    "        '''Establish dimensions and corresponding models'''\n",
    "        self._dimension = dimension\n",
    "        self._get_models()\n",
    "            \n",
    "        \n",
    "        if isinstance(models_name, int):\n",
    "            models_name = [models_name]\n",
    "               \n",
    "        for idx_m, name in enumerate(models_name):        \n",
    "            hf = h5py.File(path+name+'.h5', 'r+')\n",
    "            \n",
    "            for idx_e, exp  in enumerate(exponents):\n",
    "                \n",
    "                name_dataset = f'{exp:.2f}_T_{t_save}_N_'+ \\\n",
    "                                str(int(N_save[idx_m, idx_e]))+f'_dim_{self._dimension}'  \n",
    "                \n",
    "                n = int(n_per_class[idx_m, idx_e])\n",
    "                if n == 0:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    data = (hf.get(name_dataset)[()][:n,:self._dimension*T]) \n",
    "                except:\n",
    "                    raise TypeError('The dataset you want to load does not exist.')\n",
    "                    \n",
    "                \n",
    "                data = self._label_trajectories(trajs = data, model_name = name, exponent = exp)                \n",
    "                            \n",
    "                if idx_e + idx_m == 0:\n",
    "                    dataset = data\n",
    "                else:\n",
    "                    dataset = np.concatenate((dataset, data), axis = 0) \n",
    "        return dataset\n",
    "     \n",
    "    def _save_trajectories(self, exponents, models_name, models_func, path, n_per_class,\n",
    "                          N_save = 1000, t_save = 1000, dimension = 1):\n",
    "        ''' Saves a dataset for the exponents and models considered. \n",
    "        Arguments:   \n",
    "            :exponents (array):\n",
    "                - anomalous exponents to include in the dataset. Allows for two digit precision.\n",
    "            :models_name (list of str):\n",
    "                - names of the models to include in the output dataset. \n",
    "            :models_func (list of funcs):\n",
    "                - function generating the models to include in the output dataset. \n",
    "            :path (str):\n",
    "                - path to the folder where to save the trajectories dataset.\n",
    "            :t_save (int):\n",
    "                - length of the trajectories to save in the datasets.\n",
    "            :N_save (array):\n",
    "                - number of trajectories to include in the datasets saved.\n",
    "            :dimension (int):\n",
    "                - Dimensions of the generated trajectories. Three possible values: 1, 2 and 3.\n",
    "        No return           '''     \n",
    "    \n",
    "        '''Establish dimensions and corresponding models'''\n",
    "        self._dimension = dimension\n",
    "        self._get_models()        \n",
    "        \n",
    "        for idx_m, (name, func) in enumerate(zip(models_name, models_func)):\n",
    "            \n",
    "            if os.path.isfile(path+name+'.h5'):\n",
    "                action = 'r+'\n",
    "            else:\n",
    "                action = 'w'\n",
    "            with h5py.File(path+name+'.h5', action) as hf:\n",
    "                \n",
    "                for idx_e, exp in enumerate(exponents): \n",
    "                    if n_per_class[idx_m, idx_e] == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    n = int(N_save[idx_m, idx_e])                    \n",
    "                    name_dataset = f'{exp:.2f}_T_{t_save}_N_{n}_dim_{self._dimension}' \n",
    "                    \n",
    "                    if name_dataset not in hf:  \n",
    "                        \n",
    "                        data = np.zeros((n, self._dimension*t_save))                           \n",
    "                        # TQDM variables\n",
    "                        tq = trange(n)\n",
    "                        tq.set_postfix(saving = True, model = name, exponent = exp)\n",
    "                        for i in tq:\n",
    "                            data[i, :] = func(t_save, exp)                           \n",
    "                            \n",
    "                        hf.create_dataset(name_dataset, data=data)\n",
    "                        \n",
    "                    else:\n",
    "                        print(f'The dataset for {name} with exponent {round(exp,3)}'\n",
    "                                +' already exists, no need of saving it again.')\n",
    "            \n",
    "        \n",
    "    def _create_trajectories(self, T, exponents, dimension, models_name, models_func, n_per_class):  \n",
    "        ''' create a dataset for the exponents and models considered. \n",
    "        Arguments:  \n",
    "            :T (int):\n",
    "                - length of the trajectories.   \n",
    "            :exponents (array):\n",
    "                - anomalous exponents to include in the dataset. Allows for two digit precision.\n",
    "            :dimension (int):\n",
    "                - Dimensions of the generated trajectories. Three possible values: 1, 2 and 3.\n",
    "            :models_name (list of str):\n",
    "                - names of the models to include in the output dataset. \n",
    "            :models_func (list of funcs):\n",
    "                - function generating the models to include in the output dataset. \n",
    "            :n_per_class:\n",
    "                - number of trajectories to consider per exponent/model. \n",
    "        Return:\n",
    "            :dataset (numpy.array):\n",
    "                - Dataset of trajectories of lenght (number of models)x(T+2), with the following structure:\n",
    "                    o First column: model label.\n",
    "                    o Second column: value of the anomalous exponent.\n",
    "                    o 2:T columns: trajectories.'''\n",
    "            \n",
    "        for idx_m, (name, func) in enumerate(zip(models_name, models_func)):\n",
    "            for idx_e, exp in enumerate(exponents):\n",
    "                \n",
    "                \n",
    "                n = int(n_per_class[idx_m, idx_e])\n",
    "                data = np.zeros((n, self._dimension*T))  \n",
    "                for i in range(n):\n",
    "                    data[i, :] = func(T, exp)\n",
    "                    \n",
    "                data = self._label_trajectories(trajs = data, model_name = name, exponent = exp)   \n",
    "                \n",
    "                if idx_e + idx_m == 0:\n",
    "                    dataset = data\n",
    "                else:\n",
    "                    dataset = np.concatenate((dataset, data), axis = 0)\n",
    "                \n",
    "        return dataset\n",
    "                \n",
    "            \n",
    "    def _label_trajectories(self, trajs, model_name, exponent):\n",
    "        ''' Labels given trajectories given the corresponding label for the model and exponent.\n",
    "        For models, the label correspond to the position of the model in self.avail_models_name.\n",
    "        For exponents, the label if the value of the exponent.\n",
    "        Arguments:\n",
    "            :trajs (numpy array):\n",
    "                - trajectories to label\n",
    "            :model_name (str):\n",
    "                - name of the model from which the trajectories are coming from.\n",
    "            :exponent (float):\n",
    "                - Anomalous exponent of the trajectories. \n",
    "        Return:\n",
    "            :trajs (numpy array):\n",
    "                - Labelled trajectoreis, with the following structure:\n",
    "                    o First column: model label\n",
    "                    o Second columnd: exponent label\n",
    "                    o Rest of the array: trajectory.   '''\n",
    "        \n",
    "        label_model = self.avail_models_name.index(model_name)          \n",
    "         \n",
    "        labels_mod = np.ones((trajs.shape[0], 1))*label_model\n",
    "        labels_alpha = np.ones((trajs.shape[0], 1))*exponent\n",
    "        trajs = np.concatenate((labels_mod, labels_alpha, trajs), axis = 1)\n",
    "        \n",
    "        return trajs\n",
    "\n",
    "    def create_noisy_localization_dataset(self, \n",
    "                                          dataset = False,\n",
    "                                          T = False, N = False, exponents = False, models = False, dimension = 1,\n",
    "                                          noise_func = False, sigma = 1, mu = 0,\n",
    "                                          save_trajectories = False, load_trajectories = False, \n",
    "                                          path = 'datasets/',\n",
    "                                          N_save = 1000, t_save = 1000): \n",
    "        ''' Create a dataset of noisy trajectories. This function creates trajectories with _create_trajectories\n",
    "        and then adds given noise to them.        \n",
    "        Arguments: All arguments are the same as _create_trajectories but noise_func\n",
    "            :dataset (bool, numpy array):\n",
    "                - If False, creates a dataset with the given parameters. If numpy array, dataset to which the\n",
    "                  function applies the noise.\n",
    "            :noise_func (bool, function):\n",
    "                - if False, the noise added to the trajectories will be Gaussian distributed, with \n",
    "                  variance sigma and mean value mu.\n",
    "                - if function, uses the given function to generate noise to be added to the trajectory. The \n",
    "                  function must have as input two ints, N and M and the output must be a matrix of size NxM.\n",
    "        Return:\n",
    "            :data_models (numpy.array):\n",
    "                - Dataset of trajectories of lenght Nx(T+2), with the following structure:\n",
    "                    o First column: model label \n",
    "                    o Second column: value of the anomalous exponent\n",
    "                    o 2:T columns: trajectories'''\n",
    "                    \n",
    "        if not dataset.any():\n",
    "            dataset = self.create_dataset(T, N, exponents, models, dimension,\n",
    "                                                     save_trajectories, load_trajectories, \n",
    "                                                     path,\n",
    "                                                     N_save, t_save)\n",
    "            \n",
    "        # Add the noise to the trajectories  \n",
    "        trajs = dataset[:, 2:].reshape(dataset.shape[0]*dimension, T)\n",
    "        trajs = self._add_noisy_localization(trajs, noise_func, sigma, mu)\n",
    "        \n",
    "        dataset[:, 2:] = trajs.reshape(dataset.shape[0], T*dimension)\n",
    "        \n",
    "        return dataset    \n",
    "    \n",
    "    def create_noisy_diffusion_dataset(self, \n",
    "                                       dataset = False,\n",
    "                                       T = False, N = False, exponents = False, models = False, dimension = 1,\n",
    "                                       diffusion_coefficients = False,\n",
    "                                       save_trajectories = False, load_trajectories = False, \n",
    "                                       path = 'datasets/',\n",
    "                                       N_save = 1000, t_save = 1000): \n",
    "        ''' Create a dataset of noisy trajectories. This function creates trajectories with `_create_trajectories`\n",
    "        and then adds given noise to them.        \n",
    "        Arguments: All arguments are the same as `_create_trajectories` but dataset and diffusion_coefficients\n",
    "            :dataset (bool, numpy array):\n",
    "                - If False, creates a dataset with the given parameters. If numpy array, dataset to which the\n",
    "                  function applies the noise.\n",
    "            :noise_func (bool, function):\n",
    "                - if False, the noise added to the trajectories will be Gaussian distributed, with \n",
    "                  variance sigma and mean value mu.\n",
    "                - if function, uses the given function to generate noise to be added to the trajectory. The \n",
    "                  function must have as input two ints, N and M and the output must be a matrix of size NxM.\n",
    "                 - if numpy array, sums it to the trajectories\n",
    "        Return:\n",
    "            :data_models (numpy.array):\n",
    "                - Dataset of trajectories of lenght Nx(T+2), with the following structure:\n",
    "                    o First column: model label \n",
    "                    o Second column: value of the anomalous exponent\n",
    "                    o 2:T columns: trajectories'''\n",
    "                    \n",
    "        if not dataset.any():\n",
    "            dataset = self.create_dataset(T, N, exponents, models, dimension,\n",
    "                                                     save_trajectories, load_trajectories, \n",
    "                                                     path,\n",
    "                                                     N_save, t_save)\n",
    "        # Add the noise to the trajectories \n",
    "        trajs = dataset[:, 2:].reshape(dataset.shape[0]*dimension, T)\n",
    "        trajs = self._add_noisy_diffusion(trajs, diffusion_coefficients)\n",
    "        \n",
    "        dataset[:, 2:] = trajs.reshape(dataset.shape[0], T*dimension)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_noisy_localization(trajs, noise_func = False, sigma = 1, mu = 0):\n",
    "        \n",
    "        if isinstance(noise_func, np.ndarray):\n",
    "            noise_matrix = noise_func \n",
    "        elif not noise_func:\n",
    "            noise_matrix = sigma*np.random.randn(trajs.shape[0], trajs.shape[1])+mu\n",
    "        elif hasattr(noise_func, '__call__'):\n",
    "            noise_matrix = noise_func(trajs.shape[0], trajs.shape[1])             \n",
    "        else:\n",
    "            raise ValueError('noise_func has to be either False for Gaussian noise, a Python function or numpy array.')\n",
    "        \n",
    "        trajs += noise_matrix \n",
    "        \n",
    "        return trajs\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_noisy_diffusion(trajs, diffusion_coefficients = False):\n",
    "        \n",
    "        # First normalize the trajectories\n",
    "        trajs = normalize(trajs)\n",
    "        # If no new diffusion coefficients given, create new ones randonmly\n",
    "        if not diffusion_coefficients:\n",
    "            diffusion_coefficients = np.random.randn(trajs.shape[0])\n",
    "        # Apply new diffusion coefficients\n",
    "        trajs = (trajs.transpose()*diffusion_coefficients).transpose()\n",
    "        \n",
    "        return trajs\n",
    "\n",
    "    @staticmethod\n",
    "    def create_segmented_dataset(dataset1, dataset2, dimension = 1, \n",
    "                                 final_length = 200, random_shuffle = False):\n",
    "        ''' Creates a dataset with trajectories which change feature after a time\n",
    "        't_change'. \n",
    "        Arguments:\n",
    "            :dataset1 (numpy.array):\n",
    "                - array of size Nx(t+2), where the first columns values correspond\n",
    "                to the labels of the model and anomalous exponent. The rest \n",
    "                correspond to the trajectories of length t.\n",
    "            :dataset2 (numpy.array):\n",
    "                - same as dataset1\n",
    "            :dimension (int):\n",
    "                - Dimensions of the generated trajectories. Three possible values: 1, 2 and 3.\n",
    "            :final_length (int):\n",
    "                - length of the output trajectories.\n",
    "            :random_shuffle (bool):\n",
    "                - If True, shuffles the first axis of dataset1 and dataset2.\n",
    "        Return:\n",
    "            :seg_dataset (numpy.array):\n",
    "                - array of size Nx(t+5) whose columns represent:\n",
    "                    o Column 0: changing time\n",
    "                    o Column 1,2: labels first part of the trajectory (model, exponent)\n",
    "                    o Column 3,4: labels second part of the trajectory (model, exponent)\n",
    "                    o Column 5:(t+5): trajectories of lenght t.'''\n",
    "                    \n",
    "        '''Establish dimensions and corresponding models'''                    \n",
    "        \n",
    "        if dataset1.shape[0] != dataset2.shape[0]:\n",
    "            raise ValueError(f'Input datasets must have the same number of trajectories. Current ones have size {dataset1.shape[0]} and {dataset2.shape[0]}.')\n",
    "        if dataset1.shape[1]-2 < final_length or dataset2.shape[1]-2 < final_length:\n",
    "            raise ValueError(f'The trajectories in the input datasets are too short. They must be at least {final_length} steps long.')\n",
    "        \n",
    "        if random_shuffle:\n",
    "            np.random.shuffle(dataset1)\n",
    "            np.random.shuffle(dataset2)\n",
    "        \n",
    "        n_trajs = dataset1.shape[0]\n",
    "        trajs_1 = np.copy(dataset1[:, 2:].reshape(n_trajs, dimension, int((dataset1.shape[1]-2)/dimension)))\n",
    "        trajs_2 = np.copy(dataset2[:, 2:].reshape(n_trajs, dimension, int((dataset2.shape[1]-2)/dimension)))\n",
    "\n",
    "        trajs_1 = trajs_1[:, :, :final_length]\n",
    "        trajs_2 = trajs_2[:, :, :final_length]\n",
    "\n",
    "        t_change = np.random.randint(1, final_length, n_trajs)\n",
    "\n",
    "        seg_dataset = np.zeros((n_trajs, dimension*final_length+5))\n",
    "        for idx, (tC, traj1, traj2, label1, label2) in enumerate(zip(t_change, \n",
    "                                                                      trajs_1, trajs_2,\n",
    "                                                                      dataset1[:, :2], dataset2[:, :2])):\n",
    "            seg_dataset[idx, 0] = tC\n",
    "            seg_dataset[idx, 1:5] = np.append(label1, label2)\n",
    "\n",
    "            if dimension == 1:\n",
    "                seg_dataset[idx, 5:tC+5] = traj1[:, :tC]\n",
    "                seg_dataset[idx, tC+5:] = traj2[:, tC:final_length]-traj2[:, tC]+traj1[:, tC]\n",
    "\n",
    "            elif dimension == 2 or dimension == 3:\n",
    "                traj2 = (traj2.transpose()-traj2[:, tC]+traj1[:, tC]).transpose()\n",
    "\n",
    "                traj1[:,tC:]  = 0\n",
    "                traj2[:, :tC] = 0\n",
    "\n",
    "                seg_dataset[idx, 5:] = (traj1 + traj2).reshape(dimension*final_length)            \n",
    "            \n",
    "        return seg_dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_row(data, file):\n",
    "        '''Auxiliary function to save append data in existing files using csv\n",
    "        Arguments:\n",
    "            :data (numpy.array):\n",
    "                - row to be appended to the filed\n",
    "            :file (str):\n",
    "                - file where to append data.'''\n",
    "        with open(file, 'a') as f:\n",
    "            writer = csv.writer(f, delimiter=';', lineterminator='\\n',)\n",
    "            writer.writerow(data)\n",
    "\n",
    "    @staticmethod\n",
    "    def _cut_trajectory(traj, t_cut, dim=1):\n",
    "        \"Takes a trajectory and cuts it to `t_cut` length.\"\n",
    "        cut_traj = traj.reshape(dim, -1)[:, :t_cut]\n",
    "        return cut_traj.reshape(-1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205507bf",
   "metadata": {},
   "source": [
    "# Challenge 2020 function generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cd3874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class datasets_theory(datasets_theory):\n",
    "    def challenge_2020_dataset(self, N = 1000, max_T = 1000, min_T = 10,\n",
    "                               tasks = [1, 2, 3],\n",
    "                               dimensions = [1, 2, 3],\n",
    "                               load_dataset = False, save_dataset = False, path_datasets = '', load_labels = True,\n",
    "                               load_trajectories = False, save_trajectories = False, path_trajectories = 'datasets/',\n",
    "                               N_save = 1000, t_save = 1000,\n",
    "                               return_noise = False):  \n",
    "        ''' Creates a dataset similar to the one given by in the ANDI challenge. \n",
    "        Check the webpage of the challenge for more details. The default values\n",
    "        are similar to the ones used to generate the available dataset.\n",
    "        Arguments:  \n",
    "            :N (int, numpy.array):\n",
    "                - if int, number of trajectories per class (i.e. exponent and model) in the dataset.\n",
    "                - if numpy.array, number of trajectories per classes: size (number of models)x(number of classes)    \n",
    "            :max_T (int):\n",
    "                - Maximum length of the trajectories in the dataset.\n",
    "            :min_T (int):\n",
    "                - Minimum length of the trajectories in the dataset.\n",
    "            :tasks (int, array):\n",
    "                - Task(s) of the ANDI for which datasets will be generated. Task 1 corresponds to the\n",
    "                anomalous exponent estimation, Task 2 to the model prediction and Task 3 to the segmen-\n",
    "                tation problem.\n",
    "            :dimensions (int, array):\n",
    "                - Task(s) for which trajectories will be generated. Three possible values: 1, 2 and 3.\n",
    "            :load_dataset (bool):\n",
    "                - if True, the module loads the trajectories from the files task1.txt, task2.txt and\n",
    "                task3.txt and the labels from ref1.txt, ref2.txt and ref3.txt. If the trajectories do\n",
    "                not exist but the file does, the module returns and empty dataset.                \n",
    "            :save_dataset (bool):\n",
    "                - if True, the module saves the datasets in a .txt following the format discussed in the \n",
    "                webpage of the comptetion.\n",
    "            :path_datasets (str):\n",
    "                - path from where to load the dataset.\n",
    "            :load_labels (bool):\n",
    "                - If False, only loads trajectories and avoids the files refX.txt.\n",
    "            :load_trajectories (bool):\n",
    "                - if True, the module loads the trajectories of an .h5 file.  \n",
    "            :save_trajectories (bool):\n",
    "                - if True, the module saves a .h5 file for each model considered, with N_save trajectories \n",
    "                  and T = T_save.\n",
    "            :path_trajectories (str):\n",
    "                - path from where to load trajectories.\n",
    "            :N_save (int):\n",
    "                - Number of trajectories to save for each exponents/model. Advise: save at the beggining\n",
    "                  a big dataset (i.e. with default t_save N_save) which allows you to load any other combiantion\n",
    "                  of T and N.\n",
    "            :t_save (int):\n",
    "                - Length of the trajectories to be saved. See comments on N_save.   \n",
    "            :return_noise (bool):\n",
    "                - If True, returns the amplitudes of the noises added to the trajectories.\n",
    "        Return:\n",
    "            The function returns 6 variables, three variables for the trajectories and three \n",
    "            for the corresponding labels. Each variable is a list of three lists. Each of the\n",
    "            three lists corresponds to a given dimension, in ascending order. If one of the\n",
    "            tasks/dimensions was not calculated, the given list will be empty\n",
    "            :Xn (list of three lists):\n",
    "                - Trajectories corresponding to Task n (with n = 1,2,3). \n",
    "            :Yn (list of three lists):\n",
    "                - Labels corresponding to Task n                \n",
    "            :loc_noise_tn (list of three lists):\n",
    "                - if return_noise = True, collects the amplitude of the localization noise for task n.\n",
    "            :diff_noise_tn (list of three lists):\n",
    "                - if return_noise = True, collects the amplitude of the diffusion noise for task n. \n",
    "            The two previous variables are currently NOT SAVED NOR LOADED internally!\n",
    "                '''\n",
    "                    \n",
    "        print(f'Creating a dataset for task(s) {tasks} and dimension(s) {dimensions}.')\n",
    "        \n",
    "        # Checking inputs for errors\n",
    "        if isinstance(dimensions, int) or isinstance(dimensions, float):\n",
    "            dimensions = [dimensions]\n",
    "        if isinstance(tasks, int) or isinstance(tasks, float):\n",
    "            tasks = [tasks]\n",
    "        \n",
    "        # Define return datasets\n",
    "        X1 = [[],[],[]]; X2 = [[],[],[]]; X3 = [[],[],[]]\n",
    "        Y1 = [[],[],[]]; Y2 = [[],[],[]]; Y3 = [[],[],[]]\n",
    "        \n",
    "        if return_noise:\n",
    "            loc_noise_t1 = [[],[],[]]; loc_noise_t2 = [[],[],[]]; loc_noise_t3 = [[],[],[]]\n",
    "            diff_noise_t1 = [[],[],[]]; diff_noise_t2 = [[],[],[]]; diff_noise_t3 = [[],[],[]]\n",
    "        \n",
    "        if load_dataset or save_dataset:\n",
    "            # Define name of result files, if needed\n",
    "            task1 = path_datasets+'task1.txt'; ref1 = path_datasets+'ref1.txt'\n",
    "            task2 = path_datasets+'task2.txt'; ref2 = path_datasets+'ref2.txt'\n",
    "            task3 = path_datasets+'task3.txt'; ref3 = path_datasets+'ref3.txt'\n",
    "        \n",
    "        # Loading the datasets if chosen.\n",
    "        if load_dataset:            \n",
    "            for idx, (task, lab) in enumerate(zip([task1, task2, task3], [ref1, ref2, ref3])):\n",
    "                if idx+1 in tasks:\n",
    "                    \n",
    "                    try:\n",
    "                        t = csv.reader(open(task,'r'), delimiter=';', \n",
    "                                        lineterminator='\\n',quoting=csv.QUOTE_NONNUMERIC)\n",
    "                        if load_labels:\n",
    "                            l = csv.reader(open(lab,'r'), delimiter=';', \n",
    "                                            lineterminator='\\n',quoting=csv.QUOTE_NONNUMERIC)\n",
    "                    except:\n",
    "                        raise FileNotFoundError(f'File for task {idx+1} not found.')\n",
    "                        \n",
    "                        \n",
    "                    if load_labels:                    \n",
    "                        for trajs, labels in zip(t, l):   \n",
    "                            if task == task1:                            \n",
    "                                X1[int(trajs[0])-1].append(trajs[1:])\n",
    "                                Y1[int(trajs[0])-1].append(labels[1])\n",
    "                            if task == task2:\n",
    "                                X2[int(trajs[0])-1].append(trajs[1:])\n",
    "                                Y2[int(trajs[0])-1].append(labels[1])\n",
    "                            if task == task3:\n",
    "                                X3[int(trajs[0])-1].append(trajs[1:])\n",
    "                                Y3[int(trajs[0])-1].append(labels[1:]) \n",
    "                    else:\n",
    "                        for trajs in t:   \n",
    "                            if task == task1:                            \n",
    "                                X1[int(trajs[0])-1].append(trajs[1:])\n",
    "                            if task == task2:\n",
    "                                X2[int(trajs[0])-1].append(trajs[1:])\n",
    "                            if task == task3:\n",
    "                                X3[int(trajs[0])-1].append(trajs[1:])\n",
    "                                \n",
    "                    # Checking that the dataset exists in the files\n",
    "                    for dim in dimensions:\n",
    "                        if task == task1 and X1[dim-1] == []:\n",
    "                            raise FileNotFoundError('Dataset for dimension '+str(dim)+' not contained in file task1.txt.')\n",
    "                        if task == task2 and X2[dim-1] == []:\n",
    "                            raise FileNotFoundError('Dataset for dimension '+str(dim)+' not contained in file task2.txt.')\n",
    "                        if task == task3 and X3[dim-1] == []:\n",
    "                            raise FileNotFoundError('Dataset for dimension '+str(dim)+' not contained in file task3.txt.')\n",
    "            if load_labels:\n",
    "                return X1, Y1, X2, Y2, X3, Y3        \n",
    "            else: \n",
    "                return X1, X2, X3     \n",
    "\n",
    "            \n",
    "        exponents = np.arange(0.05, 2.01, 0.05)\n",
    "        n_exp = len(exponents)\n",
    "        # Trajectories per model and exponent. Arbitrarely chosen to obtain balanced classes\n",
    "        n_per_model = np.ceil(1.6*N/5)\n",
    "        subdif, superdif = n_exp//2, n_exp//2+1\n",
    "        n_per_class =  np.zeros((len(self.avail_models_name), n_exp))\n",
    "        # ctrw, attm\n",
    "        n_per_class[:2, :subdif] = np.ceil(n_per_model/subdif)\n",
    "        # fbm\n",
    "        n_per_class[2, :] = np.ceil(n_per_model/(n_exp-1))\n",
    "        n_per_class[2, exponents == 2] = 0 # FBM can't be ballistic\n",
    "        # lw\n",
    "        n_per_class[3, subdif:] = np.ceil((n_per_model/superdif)*0.8)\n",
    "        # sbm\n",
    "        n_per_class[4, :] = np.ceil(n_per_model/n_exp)\n",
    "        \n",
    "        # Define return datasets\n",
    "        X1 = [[],[],[]]; X2 = [[],[],[]]; X3 = [[],[],[]]\n",
    "        Y1 = [[],[],[]]; Y2 = [[],[],[]]; Y3 = [[],[],[]]  \n",
    "        \n",
    "        # Initialize the files\n",
    "        if save_dataset:\n",
    "            if 1 in tasks:\n",
    "                csv.writer(open(task1,'w'), delimiter=';', lineterminator='\\n',)\n",
    "                csv.writer(open(ref1,'w'), delimiter=';', lineterminator='\\n',)\n",
    "            elif 2 in tasks:\n",
    "                csv.writer(open(task2,'w'), delimiter=';', lineterminator='\\n',)\n",
    "                csv.writer(open(ref2,'w'), delimiter=';',lineterminator='\\n',)\n",
    "            elif 3 in tasks:\n",
    "                csv.writer(open(task3,'w'), delimiter=';', lineterminator='\\n',)\n",
    "                csv.writer(open(ref3,'w'), delimiter=';',lineterminator='\\n',)\n",
    "        \n",
    "        for dim in dimensions:             \n",
    "            # Generate the dataset of the given dimension\n",
    "            print(f'Generating dataset for dimension {dim}.')\n",
    "            dataset = self.create_dataset(T = max_T, N_models = n_per_class, exponents = exponents, \n",
    "                                           dimension = dim, models = np.arange(len(self.avail_models_name)),\n",
    "                                           load_trajectories = False, save_trajectories = False, N_save = 100,\n",
    "                                           path = path_trajectories)            \n",
    "            \n",
    "            # Normalize trajectories\n",
    "            n_traj = dataset.shape[0]\n",
    "            norm_trajs = normalize(dataset[:, 2:].reshape(n_traj*dim, max_T))\n",
    "            dataset[:, 2:] = norm_trajs.reshape(dataset[:, 2:].shape)\n",
    "    \n",
    "            # Save unnoisy dataset for task3\n",
    "            dataset_t3 = dataset.copy()\n",
    "            \n",
    "            # Add localization error, Gaussian noise with sigma = [0.1, 0.5, 1]                \n",
    "            loc_error_amplitude = np.random.choice(np.array([0.1, 0.5, 1]), size = n_traj).repeat(dim)\n",
    "            loc_error = (np.random.randn(n_traj*dim, int(max_T)).transpose()*loc_error_amplitude).transpose()                        \n",
    "            dataset = self.create_noisy_localization_dataset(dataset, dimension = dim, T = max_T, noise_func = loc_error)\n",
    "\n",
    "            \n",
    "            # Add random diffusion coefficients            \n",
    "            trajs = dataset[:, 2:].reshape(n_traj*dim, max_T)\n",
    "            displacements = trajs[:, 1:] - trajs[:, :-1]\n",
    "            # Get new diffusion coefficients and displacements\n",
    "            diffusion_coefficients = np.random.randn(n_traj).repeat(dim)\n",
    "            new_displacements = (displacements.transpose()*diffusion_coefficients).transpose()  \n",
    "            # Generate new trajectories and add to dataset\n",
    "            new_trajs = np.cumsum(new_displacements, axis = 1)\n",
    "            new_trajs = np.concatenate((np.zeros((new_trajs.shape[0], 1)), new_trajs), axis = 1)\n",
    "            dataset[:, 2:] = new_trajs.reshape(dataset[:, 2:].shape)\n",
    "      \n",
    "            \n",
    "        \n",
    "            # Task 1 - Anomalous exponent\n",
    "            if 1 in tasks:         \n",
    "                # Creating semi-balanced datasets\n",
    "                n_exp_max = int(np.ceil(1.1*N/n_exp))\n",
    "                for exponent in exponents:\n",
    "                    dataset_exp = dataset[dataset[:, 1] == exponent].copy()\n",
    "                    dataset_exp = dataset_exp[:n_exp_max, :]\n",
    "                    try:\n",
    "                        dataset_1 = np.concatenate((dataset_1, dataset_exp), axis = 0) \n",
    "                    except:\n",
    "                        dataset_1 = dataset_exp\n",
    "                \n",
    "                # Shuffle trajectories and noise                \n",
    "                p = np.random.permutation(dataset_1.shape[0])                \n",
    "                diffusion_coefficients_t1 = diffusion_coefficients[p].copy()\n",
    "                loc_error_amplitude_t1 = loc_error_amplitude[p].copy()\n",
    "                dataset_1 = dataset_1[p]\n",
    "                \n",
    "                # Saving noise with correct number of elements\n",
    "                if return_noise:\n",
    "                    loc_noise_t1[dim-1] = loc_error_amplitude_t1[:N]\n",
    "                    diff_noise_t1[dim-1] = diffusion_coefficients_t1[:N]\n",
    "       \n",
    "                for traj in dataset_1[:N, :]:             \n",
    "                    # Cutting trajectories\n",
    "                    cut_T = np.random.randint(min_T, max_T) \n",
    "                    traj_cut = self._cut_trajectory(traj[2:], cut_T, dim=dim).tolist()                         \n",
    "                    # Saving dataset\n",
    "                    X1[dim-1].append(traj_cut)\n",
    "                    Y1[dim-1].append(np.around(traj[1], 2))\n",
    "                    if save_dataset:                        \n",
    "                        self._save_row(np.append(dim, traj_cut), task1)\n",
    "                        self._save_row(np.append(dim, np.around([traj[1]], 2)), ref1)\n",
    "            \n",
    "            # Task 2 - Diffusion model\n",
    "            if 2 in tasks:   \n",
    "                # Creating semi-balanced datasets\n",
    "                # If number of traejectories N is too small, consider at least\n",
    "                # one trajectory per model\n",
    "                n_per_model = max(1, int(1.1*N/5))                    \n",
    "                for model in range(5):\n",
    "                    dataset_mod = dataset[dataset[:, 0] == model].copy()\n",
    "                    dataset_mod = dataset_mod[:n_per_model, :]\n",
    "                    try:\n",
    "                        dataset_2 = np.concatenate((dataset_2, dataset_mod), axis = 0) \n",
    "                    except:\n",
    "                        dataset_2 = dataset_mod\n",
    "                        \n",
    "                # Shuffle trajectories and noise                \n",
    "                p = np.random.permutation(dataset_2.shape[0])                \n",
    "                diffusion_coefficients_t2 = diffusion_coefficients[p].copy()\n",
    "                loc_error_amplitude_t2 = loc_error_amplitude[p].copy()\n",
    "                dataset_2 = dataset_2[p]\n",
    "                # Saving noise wityh correct number of elements\n",
    "                if return_noise:\n",
    "                    loc_noise_t2[dim-1] = loc_error_amplitude_t2[:N]\n",
    "                    diff_noise_t2[dim-1] = diffusion_coefficients_t2[:N]\n",
    "        \n",
    "                for traj in dataset_2[:N, :]:             \n",
    "                    # Cutting trajectories\n",
    "                    cut_T = np.random.randint(min_T, max_T) \n",
    "                    traj_cut = self._cut_trajectory(traj[2:], cut_T, dim=dim).tolist() \n",
    "                    # Saving dataset   \n",
    "                    X2[dim-1].append(traj_cut)\n",
    "                    Y2[dim-1].append(np.around(traj[0], 2))    \n",
    "                    if save_dataset:\n",
    "                        self._save_row(np.append(dim, traj_cut), task2)\n",
    "                        self._save_row(np.append(dim, traj[0]), ref2)   \n",
    "           \n",
    "                     \n",
    "            # Task 3 - Segmentated trajectories\n",
    "            if 3 in tasks:  \n",
    "                # Create a copy of the dataset and use it to create the \n",
    "                # segmented dataset\n",
    "                dataset_copy1 = dataset_t3.copy()\n",
    "                dataset_copy2 = dataset_t3.copy()\n",
    "                \n",
    "                # Shuffling the hard way\n",
    "                order_dataset1 = np.random.choice(np.arange(n_traj), n_traj, replace = False)        \n",
    "                order_dataset2 = np.random.choice(np.arange(n_traj), n_traj, replace = False)        \n",
    "                dataset_copy1 = dataset_copy1[order_dataset1] \n",
    "                dataset_copy2 = dataset_copy1[order_dataset2] \n",
    "                \n",
    "                seg_dataset = self.create_segmented_dataset(dataset_copy1, dataset_copy2, dimension = dim)        \n",
    "                seg_dataset = np.c_[np.ones(n_traj)*dim, seg_dataset]     \n",
    "                \n",
    "                # Checking that there are no segmented trajectories with same exponent and model \n",
    "                # in each segment. First we compute the difference between labels\n",
    "                diff = np.abs(seg_dataset[:, 2]-seg_dataset[:, 4]) + np.abs(seg_dataset[:, 3]-seg_dataset[:, 5])\n",
    "                # Then, if there are repeated labels, we eliminate those trajectories\n",
    "                while len(np.argwhere(diff == 0)) > 0: \n",
    "                    seg_dataset = np.delete(seg_dataset, np.argwhere(diff == 0), axis = 0)\n",
    "                    # If the size of the dataset is too small, we generate new segmented trajectories\n",
    "                    # and add them to the dataset\n",
    "                    if seg_dataset.shape[0] < N:\n",
    "                        \n",
    "                        # Shuffling the hard way\n",
    "                        new_order_dataset1 = np.random.choice(np.arange(n_traj), n_traj, replace = False)        \n",
    "                        new_order_dataset2 = np.random.choice(np.arange(n_traj), n_traj, replace = False)        \n",
    "                        dataset_copy1 = dataset_copy1[new_order_dataset1] \n",
    "                        dataset_copy2 = dataset_copy1[new_order_dataset2] \n",
    "                        \n",
    "                        order_dataset1 = np.concatenate((order_dataset1, new_order_dataset1))\n",
    "                        order_dataset2 = np.concatenate((order_dataset2, new_order_dataset2))\n",
    "                        \n",
    "                        aux_seg_dataset = self.create_segmented_dataset(dataset_copy1, dataset_copy2, dimension = dim) \n",
    "                        aux_seg_dataset = np.c_[np.ones(aux_seg_dataset.shape[0])*dim, aux_seg_dataset] \n",
    "                        seg_dataset = np.concatenate((seg_dataset, aux_seg_dataset), axis = 0)\n",
    "\n",
    "                        diff = np.abs(seg_dataset[:, 2]-seg_dataset[:, 4]) + np.abs(seg_dataset[:, 3]-seg_dataset[:, 5])\n",
    "                    else:\n",
    "                        break \n",
    "                    \n",
    "                # Add localization error, Gaussian noise with sigma = [0.1, 0.5, 1]                \n",
    "                loc_error_amplitude_t3 = np.random.choice(np.array([0.1, 0.5, 1]), size = seg_dataset.shape[0]).repeat(dim)\n",
    "                loc_error_t3 = (np.random.randn(seg_dataset.shape[0]*dim, 200).transpose()*loc_error_amplitude_t3).transpose()\n",
    "                            \n",
    "                seg_dataset[:, 4:] = self.create_noisy_localization_dataset(seg_dataset[:, 4:], \n",
    "                                                                            dimension = dim, T = 200, \n",
    "                                                                            noise_func = loc_error_t3)\n",
    "              \n",
    "                \n",
    "                # Add random diffusion coefficients            \n",
    "                trajs = seg_dataset[:, 6:].reshape(seg_dataset.shape[0]*dim, 200)\n",
    "                displacements = trajs[:, 1:] - trajs[:, :-1]\n",
    "                # Get new diffusion coefficients and displacements\n",
    "                diffusion_coefficients_t3 = np.random.randn(seg_dataset.shape[0]).repeat(dim)\n",
    "                new_displacements = (displacements.transpose()*diffusion_coefficients_t3).transpose()  \n",
    "                # Generate new trajectories and add to dataset\n",
    "                new_trajs = np.cumsum(new_displacements, axis = 1)\n",
    "                new_trajs = np.concatenate((np.zeros((new_trajs.shape[0], 1)), new_trajs), axis = 1)\n",
    "                seg_dataset[:, 6:] = new_trajs.reshape(seg_dataset[:, 6:].shape)\n",
    "                \n",
    "                if return_noise:\n",
    "                    loc_noise_t3[dim-1] = loc_error_amplitude_t3[:N].tolist()\n",
    "                    diff_noise_t3[dim-1] = diffusion_coefficients_t3[:N].tolist()\n",
    "                \n",
    "                \n",
    "                    \n",
    "                    \n",
    "                X3[dim-1] = seg_dataset[:N, 6:]\n",
    "                Y3[dim-1] = seg_dataset[:N, :6]\n",
    "                \n",
    "                if save_dataset:\n",
    "                    for label, traj in zip(seg_dataset[:N, :6], seg_dataset[:N, 6:]):\n",
    "                        self._save_row(np.append(dim, traj), task3)\n",
    "                        self._save_row(np.around(label, 2), ref3) \n",
    "                        \n",
    "        \n",
    "        if return_noise:\n",
    "            return [X1, Y1, loc_noise_t1, diff_noise_t1, \n",
    "                    X2, Y2, loc_noise_t2, diff_noise_t2, \n",
    "                    X3, Y3, loc_noise_t3, diff_noise_t3] \n",
    "        else:\n",
    "            return X1, Y1, X2, Y2, X3, Y3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2bc8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NBDEV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d702f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted datasets_phenom.ipynb.\n",
      "Converted datasets_theory.ipynb.\n",
      "Converted models_phenom.ipynb.\n",
      "Converted models_theory.ipynb.\n",
      "Converted utils_metrics.ipynb.\n",
      "Converted utils_trajectories.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb5bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
