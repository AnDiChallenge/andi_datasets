[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "andi_datasets",
    "section": "",
    "text": "This library has been created in the framework of the Anomalous Diffusion (AnDi) Challenge and allows to create trajectories and datasets from various anomalous diffusion models. You can install the package using:\npip install andi-datasets\nYou can then import the package in a Python3 environment using:\nimport andi_datasets\n\n\nThe first AnDi challenge was held between March and November 2020 and focused on the characterization of trajectories arising from different theoretical diffusion models under various experimental conditions. The results of the challenge are published in this article: Muñoz-Gil et al., Nat Commun 12, 6253 (2021).\nIf you want to reproduce the datasets used during the challenge, please check this tutorial. You can then test your predictions and compare them with the those of challenge participants in this online interactive tool.\n\n\n\nWe are currently preparing the second edition of the AnDi Challenge. Stay tuned, more info will be announced soon in Twitter. If you want to start playing with the new phenomenological diffusion models on which the challenge will be based, you can check this tutorial.\n\n\n\nThe andi_datasets class allows to generate, transform, analyse, save and load diffusion trajectories from a plethora of diffusion models and experimental generated with various diffusion models. The library is structured in two main blocks, containing either theoretical or phenomenological models. Here is a scheme of the library’s content:\n\n\n\nThe library allows to generate trajectories from various anomalous diffusion models: continuous-time random walk (CTRW), fractional Brownian motion (FBM), Lévy walks (LW), annealed transit time model (ATTM) and scaled Brownian motion (SBM). You can generate trajectories with the desired anomalous exponent in either one, two or three dimensions.\nExamples of their use and properties can be found in this tutorial.\n\n\n\nWe have also included models specifically developed to simulate realistic physical systems, in which random events alter the diffusion behaviour of the particle. The sources of these changes can be very broad, from the presence of heterogeneities either in space or time, the possibility of creating dimers or bigger clusters or condensates or the presence of immobile traps in the environment.\nExamples of their use and properties can be found in this tutorial.\n\n\n\n\nThe AnDi challenge is a community effort, hence any contribution to this library is more than welcome. If you think we should include a new model to the library, you can contact us in this mail: andi.challenge@gmail.com. You can also perform pull-requests and open issues with any feedback or comments you may have.\n\n\n\nAll current requirements are declared in the file setting.ini.\nFurther details can be found at the PYPI package webpage."
  },
  {
    "objectID": "lib_nbs/analysis.html",
    "href": "lib_nbs/analysis.html",
    "title": "analysis",
    "section": "",
    "text": "MSD based analysis\n\n\nmsd_analysis\n\n msd_analysis ()\n\nContains mean squared displacement (MSD) based methods to analyze trajectories.\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n---------- in \nCalculates the time average mean squared displacement (TA-MSD) of a trajectory at various time lags,\n...\n  else: warn(msg)\n\n\n\nmsd_analysis.tamsd\n\n msd_analysis.tamsd (traj:numpy.ndarray, t_lags:numpy.ndarray)\n\nCalculates the time average mean squared displacement (TA-MSD) of a trajectory at various time lags,\n\n\n\n\nType\nDetails\n\n\n\n\ntraj\nndarray\nTrajectory from whicto calculate TA-MSD.\n\n\nt_lags\nndarray\nTime lags used for the TA-MSD\n\n\nReturns\nnp.array\nTA-MSD of the given trayectory\n\n\n\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n---------- in \nCalculates the diffusion coefficient of a trajectory by means of the linear\nfitting of the TA-MSD....\n  else: warn(msg)\n\n\n\nmsd_analysis.get_diff_coeff\n\n msd_analysis.get_diff_coeff (traj:numpy.ndarray, t_lags:list=None)\n\nCalculates the diffusion coefficient of a trajectory by means of the linear fitting of the TA-MSD.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntraj\nndarray\n\n1D trajectory from whicto calculate TA-MSD.\n\n\nt_lags\nlist\nNone\nTime lags used for the TA-MSD.\n\n\nReturns\nnp.array\n\nDiffusion coefficient of the given trajectory. \n\n\n\nHere we show an example for the calculation of a Brownian motion trajectory. We create 100 trajectories from displacements of variance \\(\\sigma =1\\), which results in a diffusion coefficient \\(D=0.5\\).\n\nD = []\nfor _ in range(1000):    \n    pos = np.cumsum(np.random.randn(100))\n    D.append(msd_analysis().get_diff_coeff(pos))\n    \nplt.hist(D, bins = 30);\nplt.axvline(np.mean(D), c = 'k', label = f'Mean of predictions = {np.round(np.mean(D), 2)}')                                                                  \nplt.axvline(0.5, c = 'r', ls = '--', label = 'Expected')\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n---------- in \nCalculates the anolaous of a trajectory by means of the linear\nfitting of the logarithm of the TA-MSD....\n  else: warn(msg)\n\n\n\nmsd_analysis.get_exponent\n\n msd_analysis.get_exponent (traj, t_lags:list=None)\n\nCalculates the anolaous of a trajectory by means of the linear fitting of the logarithm of the TA-MSD.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntraj\nnp.array\n\n1D trajectory from whicto calculate TA-MSD.\n\n\nt_lags\nlist\nNone\nTime lags used for the TA-MSD.\n\n\nReturns\nnp.array\n\nAnomalous exponent of the given trajectory. \n\n\n\nTo showcase this function, we generate fractional brownian motion trajectories with \\(\\alpha = 0.5\\) and calculate their exponent:\n\nalpha = []\ntrajs, _ = models_phenom().single_state(N = 1000, T = 100, alphas = 0.5) \nfor traj in trajs.transpose(1,0,2):    \n    alpha.append(msd_analysis().get_exponent(traj[:,0]))\n    \nplt.hist(alpha, bins = 30);\nplt.axvline(np.mean(alpha), c = 'k', label = f'Mean of predictions = {np.round(np.mean(alpha), 2)}')\nplt.axvline(0.5, c = 'r', ls = '--', label = 'Expected')\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\n\n\nVelocity Autocorrelation Function (VACF)\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n---------- in \nCalculates the velocity autocorrelation function for \nthe given set of trajectories....\n  else: warn(msg)\n\n\nvacf\n\n vacf (trajs, delta_t:int|list|numpy.ndarray=1,\n       taus:bool|list|numpy.ndarray=None)\n\nCalculates the velocity autocorrelation function for the given set of trajectories.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrajs\nnp.array\n\nNxT matrix containing N trajectories of length T.\n\n\ndelta_t\nint | list | numpy.ndarray\n1\nIf not None, the vacf is calculated in the demanded time lags.\n\n\ntaus\nbool | list | numpy.ndarray\nNone\nTime windows at wich the vacf is calculated.\n\n\nReturns\nnp.array\n\nVACF of the given trajectories and the given time windows. \n\n\n\nWe show here an example of the VACF for FBM trajectories at various time lages, showing that they all coincide (as expected for this diffusion model).\n\ndeltats = np.arange(1, 5).tolist()\ntaus = np.arange(0, 100)\ntrajs, _ = models_phenom().single_state(N = 200, T = 200, alphas = 0.5)\ntrajs = trajs.transpose(1, 0, 2)[:,:,0]\n\nfor deltat in deltats:  \n    v = vacf(trajs, deltat, taus)        \n    plt.plot(taus/deltat, v.flatten(), 'o-', alpha = 0.4)    \nplt.xlim(-1, 10)\nplt.ylabel('VACF'); plt.xlabel(r'$\\tau / \\delta$')\n\nText(0.5, 0, '$\\\\tau / \\\\delta$')\n\n\n\n\n\n\n\n\nConvex hull analysis\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n---------- in \nComputes the changes points a multistate trajectory based on the Convex Hull approach proposed in PRE 96 (022144), 2017.\n...\n  else: warn(msg)\n\n\nCH_changepoints\n\n CH_changepoints (trajs, tau:int=10, metric:{'volume','area'}='volume')\n\nComputes the changes points a multistate trajectory based on the Convex Hull approach proposed in PRE 96 (022144), 2017.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrajs\nnp.array\n\nNxT matrix containing N trajectories of length T.\n\n\ntau\nint\n10\nTime window over which the CH is calculated.\n\n\nmetric\n{‘volume’, ‘area’}\nvolume\nCalculate change points w.r.t. area or volume of CH.\n\n\nReturns\nlist\n\nChange points of the given trajectory.\n\n\n\nWe showcase the use of the convex hull in a Brownian motion trajectory with two distinct diffusion coefficients, one 10 times the other:\n\n# Generate trajectories and plot change points\nT = 100; on = 40; off = 60;\ntraj = np.random.randn(T, 2)\ntraj[on:off, :] = traj[on:off, :]*10\ntraj = traj.cumsum(0)\nplt.axvline(on-tau, c = 'k')\nplt.axvline(off-tau, c = 'k', label = 'True change points')\n\n# Calculate variable Sd \ntau = 5\nSd = np.zeros(traj.shape[0]-2*tau)\nfor k in range(traj.shape[0]-2*tau):       \n    Sd[k] = ConvexHull(traj[k:(k+2*tau)], ).volume  \n\n# Compute change points both with volume and area\nCPs = CH_changepoints([traj], tau = tau)[0].flatten()-tau\nCPs_a = CH_changepoints([traj], tau = tau, metric = 'area')[0].flatten()-tau\n\n\n# Plot everything\nlabel_cp = 'CH Volume'\nfor cp in CPs:\n    plt.axvline(cp, c = 'g', alpha = 0.8, ls = '--', label = label_cp)\n    label_cp = ''\nlabel_cp = 'CH Area'    \nfor cp in CPs_a:\n    plt.axvline(cp, alpha = 0.8, ls = '--', c = 'orange', label = label_cp)\n    label_cp = ''\n    \nplt.plot(Sd, '-o')\nplt.axhline(Sd.mean(), label = 'CH Volume mean', c = 'g',)\nplt.legend()\nplt.xlabel('n'); plt.ylabel(r'$S_d(n)$')\n\nText(0, 0.5, '$S_d(n)$')"
  },
  {
    "objectID": "lib_nbs/datasets_challenge.html",
    "href": "lib_nbs/datasets_challenge.html",
    "title": "datasets_challenge",
    "section": "",
    "text": "C:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n\n\n\n\n challenge_2020_dataset (N:numpy.ndarray|int=1000, max_T:int=1000,\n                         min_T:int=10, tasks:list|int=[1, 2, 3],\n                         dimensions:list|int=[1, 2, 3],\n                         load_dataset:{'False','True'}=False,\n                         save_dataset:{'False','True'}=False,\n                         path_datasets:str='',\n                         load_labels:{'False','True'}=True,\n                         load_trajectories:{'False','True'}=False,\n                         save_trajectories:{'False','True'}=False,\n                         path_trajectories:str='datasets/',\n                         N_save:int=1000, t_save:int=1000,\n                         return_noise:{'False','True'}=False)\n\nCreates a dataset similar to the one given by in the ANDI challenge. Check the webpage of the challenge for more details. The default values are similar to the ones used to generate the available dataset.\nThe function returns 6 variables, three variables for the trajectories and three for the corresponding labels. Each variable is a list of three lists. Each of the three lists corresponds to a given dimension, in ascending order. If one of the tasks/dimensions was not calculated, the given list will be empty.\nSee the tutorials in our Github repository to learn about this function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nnumpy.ndarray | int\n1000\nNumber of trajectories per class (i.e.size # models x # classes). If int, all classes have same number.\n\n\nmax_T\nint\n1000\nMaximum length of the trajectories in the dataset.\n\n\nmin_T\nint\n10\nMinimum length of the trajectories in the dataset.\n\n\ntasks\nlist | int\n[1, 2, 3]\nTask(s) of the ANDI challenge I for which datasets will be generated.\n\n\ndimensions\nlist | int\n[1, 2, 3]\nDimension(s) for which trajectories will be generated. Three possible values: 1, 2 and 3.\n\n\nload_dataset\n{‘False’, ‘True’}\nFalse\nIf True, the module loads existing datasets from the files task{}.txt and ref{}.txt.\n\n\nsave_dataset\n{‘False’, ‘True’}\nFalse\nIf True, the module saves the datasets in a .txt following the competition format.\n\n\npath_datasets\nstr\n\nPath from where to load the dataset.\n\n\nload_labels\n{‘False’, ‘True’}\nTrue\nIf False, only loads trajectories and avoids the files refX.txt.\n\n\nload_trajectories\n{‘False’, ‘True’}\nFalse\nIf True, the module loads the trajectories of an .h5 file.\n\n\nsave_trajectories\n{‘False’, ‘True’}\nFalse\nIf True, the module saves a .h5 file for each model considered, with N_save trajectories and T = T_save.\n\n\npath_trajectories\nstr\ndatasets/\nPath from where to load trajectories.\n\n\nN_save\nint\n1000\nNumber of trajectories to save for each exponents/model. Advise: save at the beggining a big dataset (i.e. with default t_save N_save) which allows you to load any other combiantion of T and N.\n\n\nt_save\nint\n1000\nLength of the trajectories to be saved. See comments on N_save.\n\n\nreturn_noise\n{‘False’, ‘True’}\nFalse\nIf True, returns the amplitudes of the noises added to the trajectories.\n\n\nReturns\nmultiple\n\nXn (lists): trajectoriesYn (lists): labelsloc_noise_tn (lists): localization noise amplitudesdiff_noise_tn (lists): variance of the diffusion noise"
  },
  {
    "objectID": "lib_nbs/datasets_challenge.html#examples",
    "href": "lib_nbs/datasets_challenge.html#examples",
    "title": "datasets_challenge",
    "section": "Examples",
    "text": "Examples\nWe generate a dataset of trajectories from 5 different experiments. As we are not stating the opposite, each experiment will correspond to one of the 5 diffusion models considered in ANDI2022.\n\nnum_experiments, num_fovs = 5, 1\n\ndics = []\nfor i in range(num_experiments):    \n    dic = _get_dic_andi2(i+1)    \n    dics.append(dic)\n    \ndf_list, _, _ = challenge_2022_dataset(experiments = num_experiments, \n                                       num_fovs = num_fovs, \n                                       dics = dics,\n                                       return_timestep_labs = True\n                                              )\n\n\n\n\n\nDistributions parameters\nWe first check how distributed are the diffusion parameters of the generated trajectories.\n\nfig, axs = plt.subplots(2, len(df_list), figsize = (len(df_list)*2, 2*2), tight_layout = True)\n\nfor df, ax, dic in zip(df_list, axs.transpose(), dics):\n    alphas = df['alpha']\n    Ds = df['D']\n    states = df['state']\n    for u in np.unique(states):\n        ax[0].hist(alphas[states == u], density = 1)\n        ax[1].hist(Ds[states == u], density = 1)\n    \n    ax[0].set_title(dic['model'])\nplt.setp(axs[:,0], ylabel = 'Frequency')\nplt.setp(axs[0,:], xlabel = r'$\\alpha$')\nplt.setp(axs[1,:], xlabel = r'$D$')\n;\n\n''\n\n\n\n\n\n\n\nFOVs\nWe can also check that generating multiple FOVS from every experiments actually choses random FOVs in the desired space.\n\nnum_fovs = 3\ndf_fov, _ , lab_e = challenge_2022_dataset(experiments = [1,2,3,4,5],\n                                           num_fovs =num_fovs, \n                                           return_timestep_labs = True\n                                           )"
  },
  {
    "objectID": "lib_nbs/datasets_phenom.html",
    "href": "lib_nbs/datasets_phenom.html",
    "title": "datasets_phenom",
    "section": "",
    "text": "datasets_phenom\n\n datasets_phenom (models_class=<andi_datasets.models_phenom.models_phenom\n                  object at 0x000002461C942170>)\n\nThis class generates, saves and loads datasets of trajectories simulated from various phenomenological diffusion models (available at andi_datasets.models_phenom).\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n---------- in \nGiven a list of dictionaries, generates trajectories of the demanded properties.\nThe only compulsory input for every dictionary is 'model', i.e. the model from which ...\n  else: warn(msg)\n\n\n\ndatasets_phenom.create_dataset\n\n datasets_phenom.create_dataset (dics:list|dict|bool=False,\n                                 T:None|int=None, N_model:None|int=None,\n                                 path:str='', save:bool=False,\n                                 load:bool=False)\n\nGiven a list of dictionaries, generates trajectories of the demanded properties. The only compulsory input for every dictionary is ‘model’, i.e. the model from which trajectories must be generated. The rest of inputs are optional. You can see the input parameters of the different models in andi_datasets.models_phenom, This function checks and handles the input dataset and the manages both the creation, loading and saving of trajectories.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndics\nlist | dict | bool\nFalse\n- if list or dictionary: the function generates trajectories with the properties stated in each dictionary.- if bool: the function generates trajectories with default parameters set for the ANDI2022 challenge for every available diffusion model.\n\n\nT\nNone | int\nNone\n- if int: overrides the values of trajectory length in the dictionaries.- if None: uses the trajectory length values in the dictionaries. Caution: the minim T of all dictionaries will be considered!\n\n\nN_model\nNone | int\nNone\n- if int: overrides the values of number of trajectories in the dictionaries.- if None: uses the number of trajectories in the dictionaries\n\n\npath\nstr\n\nPath from where to save or load the dataset.\n\n\nsave\nbool\nFalse\nIf True, saves the generated dataset (see self._save_trajectories).\n\n\nload\nbool\nFalse\nIf True, loads a dataset from path (see self._load_trajectories).\n\n\nReturns\ntuple\n\n- trajs (array TxNx2): particles’ position. N considers here the sum of all trajectories generated from the input dictionaries. - labels (array TxNx2): particles’ labels (see ._multi_state for details on labels) \n\n\n\nIn the example below we create two dictionaries and generate a dataset with it. See the corresponding tutorial for more details.\n\nL = 50\ndict_model3 = {'model': 'dimerization', \n               'L': L,\n               'Pu': 0.1, 'Pb': 1}\ndict_model5 = {'model': 'confinement',\n               'L': L, \n               'trans': 0.2}\n\ndict_all = [dict_model3, dict_model5]\n\ntrajs, labels = datasets_phenom().create_dataset(N_model = 10, # number of trajectories per model\n                                                 T = 200,\n                                                 dics = dict_all\n                                                )\nplot_trajs(trajs, L , N = 10, \n           num_to_plot = 3,\n           labels = labels,\n           plot_labels = True\n          )\n\n\n\n\n\n\nCreating, saving and loading trajectories\nThese auxiliary functions used in create_trajectories that allow for manipulate trajectories in various forms.\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n---------- in \nGiven a list of dictionaries, generates trajectories of the demanded properties.\nFirst checks in the .csv of each demanded model if a dataset of similar properties exists. ...\n  else: warn(msg)\n\n\n_create_trajectories\n\n _create_trajectories ()\n\nGiven a list of dictionaries, generates trajectories of the demanded properties. First checks in the .csv of each demanded model if a dataset of similar properties exists. If it does, it loads it from the corresponding file.\n\n\n\n_save_trajectories\n\n _save_trajectories (trajs, labels, dic, df, dataset_idx, path)\n\nGiven a set of trajectories and labels, saves two things:\n- In the .csv corresponding to the demanded model, all the input parameters of the generated dataset. This allows to keed that of what was created before. - In a .npy file, the trajectories and labels generated.\n\n\n\n_load_trajectories\n\n _load_trajectories (model_name, dataset_idx, path)\n\nGiven the path for a dataset, loads the trajectories and labels\n\n\n\nManaging parameters and dictionaries\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n----------- in \nChecks the information of the input dictionaries so that they fulfil the constraints of the program , completes missing information\nwith default values and then decides about loading/saving depending on parameters....\n  else: warn(msg)\n\n\n_inspect_dic\n\n _inspect_dic (dic)\n\nChecks the information of the input dictionaries so that they fulfil the constraints of the program , completes missing information with default values and then decides about loading/saving depending on parameters.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndic\ndict\nDictionary with the information of the trajectories we want to generate\n\n\nReturns\ntuple\ndf: dataframe collecting the information of the dataset to load.dataset_idx: location in the previous dataframe of the particular dataset we want to generate.\n\n\n\n\n\n\n_get_args\n\n _get_args (model, return_defaults=False)\n\nGiven the name of a diffusion model, return its inputs arguments.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nstr\n\nName of the diffusion model (see self.available_models_name)\n\n\nreturn_defaults\nbool\nFalse\nIf True, the function will also return the default values of each input argument.\n\n\nReturns\ntuple\n\nargs (list): list of input arguments.defaults (optional, list): list of default value for the input arguments."
  },
  {
    "objectID": "lib_nbs/datasets_theory.html",
    "href": "lib_nbs/datasets_theory.html",
    "title": "datasets_theory",
    "section": "",
    "text": "datasets_theory\n\n datasets_theory ()\n\nThis class generates, saves and loads datasets of theoretical trajectories simulated from various diffusion models (available at andi_datasets.models_theory).\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nInputs \n-------- in \nCreates a dataset of trajectories via the theoretical models defined in `.models_theory`. Check our tutorials for use cases of this function.\n...\n  else: warn(msg)\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: Unknown section Inputs\n  else: warn(msg)\n\n\n\ndatasets_theory.create_dataset\n\n datasets_theory.create_dataset (T, N_models, exponents, models,\n                                 dimension=1, save_trajectories=False,\n                                 load_trajectories=False,\n                                 path='datasets/', N_save=1000,\n                                 t_save=1000)\n\nCreates a dataset of trajectories via the theoretical models defined in .models_theory. Check our tutorials for use cases of this function.\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: Unknown section Arguments\n  else: warn(msg)\n\n\n\ndatasets_theory.create_segmented_dataset\n\n datasets_theory.create_segmented_dataset (dataset1, dataset2,\n                                           dimension=1, final_length=200,\n                                           random_shuffle=False)\n\nCreates a dataset with trajectories which change diffusive feature (either model or anomalous exponent) after a time ‘t_change’.\n\n\n\ndatasets_theory.create_noisy_diffusion_dataset\n\n datasets_theory.create_noisy_diffusion_dataset (dataset=False, T=False,\n                                                 N=False, exponents=False,\n                                                 models=False,\n                                                 dimension=1, diffusion_co\n                                                 efficients=False,\n                                                 save_trajectories=False,\n                                                 load_trajectories=False,\n                                                 path='datasets/',\n                                                 N_save=1000, t_save=1000)\n\nCreate a dataset of noisy trajectories. This function creates trajectories with _create_trajectories and then adds given noise to them."
  },
  {
    "objectID": "lib_nbs/index_docs.html",
    "href": "lib_nbs/index_docs.html",
    "title": "Documentation",
    "section": "",
    "text": "Theory datasets: motivated by our first AnDi Challenge, we gather here different theoretical anomalous diffusion models, as e.g. fractional Brownian motion or continuous time random walk.\nPhenomenological datasets: to closer simulate experimental trajectories, we consider also diffusion when interactions between particles and the environmnet are present. For instance, the appearance of comparments, trapping, dimerization but also changes in diffusion properties.\nAnDi Challenge datasets: this blocks manages the generation of datasets for the various AnDi Challenges. It allows to replicate the same datasets that will be given in the different editions of the challenge.\nTrajectory analysis: here we offer different statistical approaches to anomalous diffusion characterization. This will be a growing set of tools that will help the user to correctly characterize their trajectories with minimal coding.\n\nHere is a schematic representation of the library contents:\n\n\n\nA smooth introduction to each component of this library is done in the [Tutorials sections](tutorials/"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#single-trajectory-generator",
    "href": "lib_nbs/models_phenom.html#single-trajectory-generator",
    "title": "models_phenom",
    "section": "Single trajectory generator",
    "text": "Single trajectory generator\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nParameters \n----------- in \nGenerates a single state trajectory with given parameters. \n...\n  else: warn(msg)\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n---------- in \nGenerates a single state trajectory with given parameters. \n...\n  else: warn(msg)\n\n\n_single_state_traj\n\n _single_state_traj (T:int=200, D:float=1, alpha:float=1, L:float=None,\n                     deltaT:int=1)\n\nGenerates a single state trajectory with given parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nint\n200\nLength of the trajectory\n\n\nD\nfloat\n1\nDiffusion coefficient\n\n\nalpha\nfloat\n1\nAnomalous exponent\n\n\nL\nfloat\nNone\nLength of the box acting as the environment\n\n\ndeltaT\nint\n1\nSampling time\n\n\nReturns\ntuple\n\n- pos: position of the particle- labels: anomalous exponent, D and state at each timestep. State is always free here.\n\n\n\n\ntraj, labels = models_phenom._single_state_traj(D = 0.05,  alpha = 1.5, \n                                                T =1000, L = 10)\n\nfig, ax = plt.subplots(1, 3, figsize = (10, 2), tight_layout = True)\nax[0].plot(traj[:, 0], traj[:, 1], alpha = 0.5)\nplt.setp(ax[0], xlabel = 'X', ylabel = 'Y')\n\nax[1].plot(traj[:, 0], '.', label = 'X')\nax[1].plot(traj[:, 1], '.', label = 'Y', )\nplt.setp(ax[1], ylabel = 'Position', xlabel = 'Time')\nax[1].legend()\n\nax[2].plot(labels[:, 0], '.', label = r'$\\alpha$')\nax[2].plot(labels[:, 1], '.', label = r'$D$' )\nplt.setp(ax[2], ylabel = 'Label', xlabel = 'Time')\nax[2].legend()\n\nfor b in [0,10]:\n    ax[0].axhline(b, ls = '--', alpha = 0.3, c = 'k')\n    ax[0].axvline(b, ls = '--', alpha = 0.3, c = 'k')\n    ax[1].axhline(b, ls = '--', alpha = 0.3, c = 'k')"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#dataset-generation",
    "href": "lib_nbs/models_phenom.html#dataset-generation",
    "title": "models_phenom",
    "section": "Dataset generation",
    "text": "Dataset generation\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nParameters \n----------- in \nGenerates a dataset made of single state trajectories with given parameters.\n...\n  else: warn(msg)\nC:\\Users\\Gorka\\Anaconda3\\envs\\andi_dataset\\lib\\site-packages\\fastcore\\docscrape.py:225: UserWarning: potentially wrong underline length... \nReturns \n---------- in \nGenerates a dataset made of single state trajectories with given parameters.\n...\n  else: warn(msg)\n\n\nsingle_state\n\n single_state (N:int=10, T:int=200, Ds:list=[1, 0], alphas:list=[1, 0],\n               L:float=None)\n\nGenerates a dataset made of single state trajectories with given parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n10\nNumber of trajectories in the dataset\n\n\nT\nint\n200\nLength of the trajectory\n\n\nDs\nlist\n[1, 0]\nIf list, mean and variance from which to sample the diffusion coefficient. If float, we consider variance = 0.\n\n\nalphas\nlist\n[1, 0]\nIf list, mean and variance from which to sample the anomalous exponent. If float, we consider variance = 0.\n\n\nL\nfloat\nNone\nLength of the box acting as the environment\n\n\nReturns\ntuple\n\n- positions: position of the N trajectories.- labels: anomalous exponent, D and state at each timestep. State is always free here. \n\n\n\n\nN = 500; L = 5; T = 100;\nalpha = [0.8, 0.1]; D = 1.2\n\ntrajs, labels = models_phenom().single_state(N = N,\n                                           L = L,\n                                           T = T, \n                                           alphas = alpha,\n                                           Ds = D)\nfig, ax = plt.subplots()\nax.hist(labels[0,:,0], bins = 50)\nplt.setp(ax, title = r'Distribution of $\\alpha$', xlabel = r'$\\alpha$', ylabel = 'Frequency');\n\n\n\n\n\nplot_trajs(trajs, L, N)"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#single-trajectory-generator-1",
    "href": "lib_nbs/models_phenom.html#single-trajectory-generator-1",
    "title": "models_phenom",
    "section": "Single trajectory generator",
    "text": "Single trajectory generator\n\n\n_multiple_state_traj\n\n _multiple_state_traj (T=200, M=[[0.95, 0.05], [0.05, 0.95]], Ds=[1, 0.1],\n                       alphas=[1, 1], L=None, deltaT=1,\n                       return_state_num=False, init_state=None)\n\nGenerates a 2D multi state trajectory with given parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nint\n200\nLength of the trajectory\n\n\nM\nlist\n[[0.95, 0.05], [0.05, 0.95]]\nTransition matrix between diffusive states.\n\n\nDs\nlist\n[1, 0.1]\nDiffusion coefficients of the diffusive states. Must have as many Ds as states defined by M.\n\n\nalphas\nlist\n[1, 1]\nAnomalous exponents of the diffusive states. Must have as many alphas as states defined by M.\n\n\nL\nNoneType\nNone\nLength of the box acting as the environment\n\n\ndeltaT\nint\n1\nSampling time\n\n\nreturn_state_num\nbool\nFalse\nIf True, returns as label the number assigned to the state at each time step.\n\n\ninit_state\nNoneType\nNone\nIf True, the particle starts in state 0. If not, sample initial state.\n\n\nReturns\ntuple\n\n- pos: position of the particle- alphas_t: anomalous exponent at each step- Ds_t: diffusion coefficient at each step. - label_diff_state: particle’s state (can be either free or directed for alpha ~ 2) at each step.- state (optional): state label at each step. \n\n\n\n\nT = 1000; L = 100\ntraj, labels = models_phenom._multiple_state_traj(T = T,\n                                                  L = L,\n                                                  alphas = [0.2, 0.7], \n                                                  Ds = [1.5, 2],                                                  \n                                                  return_state_num=True)\n\nfig, ax = plt.subplots(1, 3, figsize = (9, 3), tight_layout = True)\nax[0].plot(traj[:, 0], traj[:, 1], alpha = 0.5)\nplt.setp(ax[0], xlabel = 'X', ylabel = 'Y')\n\nax[1].plot(traj[:, 0], label = 'X')\nax[1].plot(traj[:, 1], label = 'Y', )\nplt.setp(ax[1], ylabel = 'Position', xlabel = 'Time')\nax[1].legend()\n\nax[2].plot(labels[:, 0], '.', label = r'$\\alpha$')\nax[2].plot(labels[:, 1], '.', label = r'$D$' )\nax[2].plot(labels[:, 3], '.', label = r'$state \\#$', alpha = 0.3 )\nplt.setp(ax[2], ylabel = 'Label', xlabel = 'Time')\nax[2].legend()\n\nfor b in [0,L]:\n    ax[0].axhline(b, ls = '--', alpha = 0.3, c = 'k')\n    ax[0].axvline(b, ls = '--', alpha = 0.3, c = 'k')\n    ax[1].axhline(b, ls = '--', alpha = 0.3, c = 'k')"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#dataset-generation-1",
    "href": "lib_nbs/models_phenom.html#dataset-generation-1",
    "title": "models_phenom",
    "section": "Dataset generation",
    "text": "Dataset generation\n\n\nmulti_state\n\n multi_state (N=10, T=200, M:<built-infunctionarray>=[[0.9, 0.1], [0.1,\n              0.9]], Ds:<built-infunctionarray>=[[1, 0], [0.1, 0]],\n              alphas:<built-infunctionarray>=[[1, 0], [1, 0]],\n              gamma_d=[1], epsilon_a=[0], L=None, return_state_num=False,\n              init_state=None)\n\nGenerates a dataset of 2D multi state trajectory with given parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n10\nNumber of trajectories\n\n\nT\nint\n200\nLength of the trajectory\n\n\nM\narray\n[[0.9, 0.1], [0.1, 0.9]]\nTransition matrix between diffusive states\n\n\nDs\narray\n[[1, 0], [0.1, 0]]\nList of means and variances from which to sample the diffusion coefficient of each state. If element size is one, we consider variance = 0.\n\n\nalphas\narray\n[[1, 0], [1, 0]]\nList of means and variances from which to sample the anomalous exponent of each state. If element size is one, we consider variance = 0.\n\n\ngamma_d\nlist\n[1]\nMinimum factor between D of diffusive states (see ._sampling_diff_parameters)\n\n\nepsilon_a\nlist\n[0]\nDistance between alpha of diffusive states (see ._sampling_diff_parameters)\n\n\nL\nNoneType\nNone\nLength of the box acting as the environment\n\n\nreturn_state_num\nbool\nFalse\nIf True, returns as label the number assigned to the state at each time step.\n\n\ninit_state\nNoneType\nNone\nIf True, the particle starts in state 0. If not, sample initial state.\n\n\nReturns\ntuple\n\n- trajs (array TxNx2): particles’ position- labels (array TxNx2): particles’ labels (see ._multi_state for details on labels) \n\n\n\n\nN = 100; L = 50; T = 100;\n\ntrajs, labels = models_phenom().multi_state(N = N, T = T, L = L,\n                                            M = np.array([[0.95 , 0.05],[0.05, 0.95]]),\n                                            Ds = np.array([[1, 0], [1, 0.5]]), \n                                            alphas = np.array([[1, 0.01], [0.5, 0.02]]),\n                                            epsilon_a=[0.4], gamma_d = [0.75],\n                                            return_state_num=True)\n\nWe can first check the parameter distributions:\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3), tight_layout = True)\n\n# Diffusion coefficients\nax[0].hist(labels[labels[:,:,3] == 0, 1].flatten(), label = f'State 1 - % = {(labels[:,:,3] == 0).sum()/np.prod(labels.shape[:2])}')\nax[0].hist(labels[labels[:,:,3] == 1, 1].flatten(), label = f'State 2 - % = {(labels[:,:,3] == 1).sum()/np.prod(labels.shape[:2])}')\nplt.setp(ax[0], title = r'Distribution of $D$', xlabel = r'$D$', ylabel = 'Frequency');\n\n# Anomalous exponents\nax[1].hist(labels[labels[:,:,3] == 0, 0].flatten(), label = f'State 1 - # = {(labels[:,:,3] == 0).sum()}')\nax[1].hist(labels[labels[:,:,3] == 1, 0].flatten(), label = f'State 2 - # = {(labels[:,:,3] == 1).sum()}')\nplt.setp(ax[1], title = r'Distribution of $\\alpha$', xlabel = r'$\\alpha$', ylabel = 'Frequency');\nax[0].legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nAnd then see some examples of trajectories:\n\nplot_trajs(trajs, L, N, labels = labels, plot_labels = True)"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#auxiliary-functions",
    "href": "lib_nbs/models_phenom.html#auxiliary-functions",
    "title": "models_phenom",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\nDistance calculator\n\n\n_get_distance\n\n _get_distance (x)\n\nGiven a matrix of size Nx2, calculates the distance between the N particles.\n\n\n\n\nType\nDetails\n\n\n\n\nx\narray\nParticles’ positions\n\n\nReturns\narray\nDistance between particles \n\n\n\nEscaping dynamics\n\n\n\n_make_escape\n\n _make_escape (Pu, label, diff_state)\n\nGiven an unbinding probablity (Pu), the current labeling of particles (label) and the current state of particle (diff_state, either bound, 1, or unbound, 0), simulate an stochastic binding mechanism.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nPu\nfloat\nUnbinding probablity\n\n\nlabel\narray\nCurrent labeling of the particles (i.e. to which condensate they belong)\n\n\ndiff_state\narray\nCurrent state of the particles\n\n\nReturns\ntuple\nNew labeling and diffusive state of the particles \n\n\n\nClustering dynamics\n\n\n\n_make_condensates\n\n _make_condensates (Pb, label, diff_state, r, distance, max_label)\n\nGiven a binding probability Pb, the current label of particles (label), their current diffusive state (diff_state), the particle size (r), their distances (distance) and the label from which binding is not possible (max_label), simulates a binding mechanism.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nPb\nfloat\nBinding probablity.\n\n\nlabel\narray\nCurrent labeling of the particles (i.e. to which condensate they belong)\n\n\ndiff_state\narray\nCurrent state of the particles\n\n\nr\nfloat\nParticle size.\n\n\ndistance\narray\nDistance between particles\n\n\nmax_label\nint\nMaximum label from which particles will not be considered for binding\n\n\nReturns\ntuple\nNew labeling and diffusive state of the particles\n\n\n\nHere is a test in which some particles, distributed randomly through a bounded space, first bind and then unbind using the previous defined functions:\n\n# Binding and unbinding probabilities\nPb = 0.8; Pu = 0.5\n# Generate the particles\nN = 200; L = 10; r = 1; max_n = 2; Ds = np.ones(100)\npos = np.random.rand(N, 2)*L    \n# Put random labels (label = which condensate you belong). diff_state is zero because all are unbound)\nlabel = np.arange(N)#np.random.choice(range(500), N, replace = False)\ndiff_state = np.zeros(N).astype(int)\n# Define max_label bigger than max of label so everybody binds\nmax_label = max(label)+2\n# Calculate distance between particles\ndistance = models_phenom._get_distance(pos)\n\nprint('# of free particles:')\nprint(f'Before binding: {len(label)}')\n\n# First make particle bind:\nlab, ds = models_phenom._make_condensates(Pb, label, diff_state, r, distance, max_label)\nprint(f'After binding: {np.unique(lab[np.argwhere(ds == 0)], return_counts=True)[0].shape[0]}')\n\n# Then we do unbinding:\nlab, ds = models_phenom._make_escape(Pu, lab, ds)\nprint(f'After unbinding: {np.unique(lab[np.argwhere(ds == 0)], return_counts=True)[0].shape[0]}')\n\n# of free particles:\nBefore binding: 200\nAfter binding: 4\nAfter unbinding: 106\n\n\nStokes drag\n\n\n\n_stokes\n\n _stokes (D)\n\nApplies a Stokes-Einstein-like transformation to two diffusion coefficients.\n\n\n\n\nType\nDetails\n\n\n\n\nD\ntuple\nDiffusion coefficients of the two binding particles.\n\n\nReturns\nfloat\nResulting diffusion coefficient."
  },
  {
    "objectID": "lib_nbs/models_phenom.html#time-evolution",
    "href": "lib_nbs/models_phenom.html#time-evolution",
    "title": "models_phenom",
    "section": "Time evolution",
    "text": "Time evolution\n\n\ndimerization\n\n dimerization (N=10, T=200, L=100, r=1, Pu=0.1, Pb=0.01, Ds:<built-\n               infunctionarray>=[[1, 0], [0.1, 0]], alphas:<built-\n               infunctionarray>=[[1, 0], [1, 0]], epsilon_a=0,\n               stokes=False, return_state_num=False, deltaT=1)\n\nGenerates a dataset of 2D trajectories of particles perfoming stochastic dimerization.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n10\nNumber of trajectories\n\n\nT\nint\n200\nLength of the trajectory\n\n\nL\nint\n100\nLength of the box acting as the environment\n\n\nr\nint\n1\nRadius of particles.\n\n\nPu\nfloat\n0.1\nUnbinding probability.\n\n\nPb\nfloat\n0.01\nBinding probability.\n\n\nDs\narray\n[[1, 0], [0.1, 0]]\nList of means and variances from which to sample the diffusion coefficient of each state. If element size is one, we consider variance = 0.\n\n\nalphas\narray\n[[1, 0], [1, 0]]\nList of means and variances from which to sample the anomalous exponent of each state. If element size is one, we consider variance = 0.\n\n\nepsilon_a\nint\n0\nDistance between alpha of diffusive states (see ._sampling_diff_parameters)\n\n\nstokes\nbool\nFalse\nIf True, applies a Stokes-Einstein like coefficient to calculate the diffusion coefficient of dimerized particles. If False, we use as D resulting from the dimerization the D assigned to the dimerized state of one of the two particles.\n\n\nreturn_state_num\nbool\nFalse\nIf True, returns as label the number assigned to the state at each time step.\n\n\ndeltaT\nint\n1\nSampling time\n\n\nReturns\ntuple\n\n- trajs (array TxNx2): particles’ position- labels (array TxNx2): particles’ labels (see ._multi_state for details on labels)\n\n\n\n\nN = 500; L = 50; r = 1; T = 100\nPu = 0.1 # Unbinding probability\nPb = 1 # Binding probability\n# Diffusion coefficients of two states\nstokes = True\nDs = np.array([[2, 0.01], [1e-5, 0]]) # because stokes = True, we don't care about the second state\n# Anomalous exponents for two states\nalphas = np.array([[1, 0], [1, 0.2]]) \n\ntrajs, labels = models_phenom().dimerization(N = N,\n                                            L = L,\n                                            r = r,\n                                            T = T,\n                                            Pu = Pu, # Unbinding probability\n                                            Pb = Pb, # Binding probability\n                                            Ds = Ds, # Diffusion coefficients of two states\n                                            alphas = alphas, # Anomalous exponents for two states,\n                                            return_state_num = True,\n                                            stokes = True, epsilon_a=0.2\n                                            )\n\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3), tight_layout = True)\n\n# Diffusion coefficients\nax[0].hist(labels[labels[:,:,3] == 0, 1].flatten(), label = f'State 1 - % = {(labels[:,:,3] == 0).sum()/np.prod(labels.shape[:2])}')\nax[0].hist(labels[labels[:,:,3] == 1, 1].flatten(), label = f'State 2 - % = {(labels[:,:,3] == 1).sum()/np.prod(labels.shape[:2])}')\nplt.setp(ax[0], title = r'Distribution of $D$', xlabel = r'$D$', ylabel = 'Frequency');\n\n# Anomalous exponents\nax[1].hist(labels[labels[:,:,3] == 0, 0].flatten(), label = f'State 1 - # = {(labels[:,:,3] == 0).sum()}')\nax[1].hist(labels[labels[:,:,3] == 1, 0].flatten(), label = f'State 2 - # = {(labels[:,:,3] == 1).sum()}')\nplt.setp(ax[1], title = r'Distribution of $\\alpha$', xlabel = r'$\\alpha$', ylabel = 'Frequency');\nax[0].legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\nplot_trajs(trajs, L, N, labels = labels, plot_labels = True)"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#auxiliary-functions-1",
    "href": "lib_nbs/models_phenom.html#auxiliary-functions-1",
    "title": "models_phenom",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\nDistribute compartments\n\n\n_distribute_circular_compartments\n\n _distribute_circular_compartments (Nc, r, L)\n\nDistributes circular compartments over an environment without overlapping. Raises a warning and stops when no more compartments can be inserted.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nNc\nfloat\nNumber of compartments\n\n\nr\nfloat\nSize of the compartments\n\n\nL\nfloat\nSide length of the squared environment.\n\n\nReturns\narray\nPosition of the centers of the compartments\n\n\n\n\nfig, ax = plt.subplots(figsize = (5,5))\n\nNc = 60; r = 10; L = 256;\ncomp_center = models_phenom._distribute_circular_compartments(Nc, r, L)\n\nfor c in comp_center:\n    circle = plt.Circle((c[0], c[1]), r)\n    ax.add_patch(circle)\nax.set_xlim(0,L)\nax.set_ylim(0,L)\n\n(0.0, 256.0)\n\n\n\n\n\nReflection inside circles\n\n\n\n_reflected_position\n\n _reflected_position (circle_center, circle_radius, beg, end,\n                      precision_boundary=0.0001)\n\nGiven the begining and end of a segment crossing the boundary of a circle, calculates the new position considering that boundaries are fully reflective.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncircle_center\nfloat\n\nCenter of the circle\n\n\ncircle_radius\nfloat\n\nRadius of the circle\n\n\nbeg\ntuple\n\nPosition (in 2D) of the begining of the segment\n\n\nend\ntuple\n\nPosition (in 2D) of the begining of the segment\n\n\nprecision_boundary\nfloat\n0.0001\nSmall area around the real boundary which is also considered as boundary. For numerical stability\n\n\nReturns\ntuple\n\n- Reflected position- Intersection point\n\n\n\n\ncircle_radius = 2;\ncircle_center = [0,0]\nbeg = np.array([0.8, 0])+circle_center\nend = np.array([2.5, -0.8])+circle_center\n\nfinal_point, intersect = models_phenom._reflected_position(circle_center, circle_radius, beg, end)\n\nfig, ax = plt.subplots(figsize = (3, 3))\n\ncircle = plt.Circle(circle_center, circle_radius, facecolor = 'w', ec = 'C0', label = 'Compartment', zorder = -1)\nax.add_patch(circle)\nax.plot([beg[0],end[0]], [beg[1], end[1]], '-o', c = 'C1', label = 'Displacement segment')\nax.plot([circle_center[0], intersect[0]], [circle_center[1], intersect[1]], c = 'C2')\nax.plot([intersect[0], final_point[0]], [intersect[1], final_point[1]], '-o', c = 'C4', label = 'Resulting reflection')\nax.set_ylim(circle_center[1]-circle_radius*1.5, circle_center[1]+circle_radius*1.5)\nax.set_xlim(circle_center[0]-circle_radius*1.5, circle_center[0]+circle_radius*1.5)\nax.legend(fontsize = 8)\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#single-trajectory-generator-2",
    "href": "lib_nbs/models_phenom.html#single-trajectory-generator-2",
    "title": "models_phenom",
    "section": "Single trajectory generator",
    "text": "Single trajectory generator\n\n\n_confinement_traj\n\n _confinement_traj (T=200, L=100, Ds=[1, 0.1], alphas=[1, 1], r=1,\n                    comp_center=None, Nc=10, trans=0.1, deltaT=1)\n\nGenerates a 2D trajectory of particles diffusing in an environment with partially transmitting circular compartments.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nint\n200\nLength of the trajectory\n\n\nL\nint\n100\nLength of the box acting as the environment\n\n\nDs\nlist\n[1, 0.1]\nDiffusion coefficients of the two diffusive states (first free, the confined). Size must be 2.\n\n\nalphas\nlist\n[1, 1]\nAnomalous exponents of the two diffusive states (first free, the confined). Size must be 2.\n\n\nr\nint\n1\nRadius of the compartments.\n\n\ncomp_center\nNoneType\nNone\nIf given, center of the compartments. If None, centers are uniformly sampled.\n\n\nNc\nint\n10\nNumber of compartments\n\n\ntrans\nfloat\n0.1\nTransmittance of the boundaries\n\n\ndeltaT\nint\n1\nSampling time.\n\n\nReturns\ntuple\n\n- pos (array Tx2): particle’s position- labels (array Tx2): particle’s labels (see ._multi_state for details on labels)\n\n\n\n\nN = 50;  L = 20\nNc = 15; r = 1; L = 20\nDs = [1, 0.1]\nr , L, Nc = (20, 256, 20)\n\ncomp_center = models_phenom._distribute_circular_compartments(Nc = Nc, r = r, L = L)\ntrajs, labels = models_phenom()._confinement_traj(trans = 0.1, Nc = Nc, r = r, L = L, T =200, comp_center=comp_center, Ds = Ds)\n\nfig, axs = plt.subplots(1,2, figsize = (6,3), tight_layout = True)\n\nax = axs[0]\nfor c in comp_center:\n    circle = plt.Circle((c[0], c[1]), r, facecolor = 'None', edgecolor = 'C0')\n    ax.add_patch(circle)    \nax.scatter(trajs[:,0], trajs[:,1], c = plt.cm.cividis(labels[:,-1]/2), zorder = -1, s = 2)   \n\nplt.setp(axs[0], xlim = (0,L), ylim = (0,L), xlabel = 'X', ylabel = 'Y')\n\naxs[1].plot(trajs[:,0], label = 'x');\naxs[1].plot(trajs[:,1], label = 'y');\naxs[1].legend()\nplt.setp(axs[1], xlabel = 'Position', ylabel = 'Time')\n\n[Text(0.5, 0, 'Position'), Text(0, 0.5, 'Time')]"
  },
  {
    "objectID": "lib_nbs/models_phenom.html#dataset-generation-2",
    "href": "lib_nbs/models_phenom.html#dataset-generation-2",
    "title": "models_phenom",
    "section": "Dataset generation",
    "text": "Dataset generation\n\n\nconfinement\n\n confinement (N=10, T=200, L=100, Ds=[[1, 0], [0.1, 0]], alphas=[[1, 0],\n              [1, 0]], gamma_d=[1], epsilon_a=[0], r=1, comp_center=None,\n              Nc=10, trans=0.1, deltaT=1)\n\nGenerates a dataset of 2D trajectories of particles diffusing in an environment with partially transmitting circular compartments.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n10\nNumber of trajectories\n\n\nT\nint\n200\nLength of the trajectory\n\n\nL\nint\n100\nLength of the box acting as the environment\n\n\nDs\nlist\n[[1, 0], [0.1, 0]]\nList of means and variances from which to sample the diffusion coefficient of each state. If element size is one, we consider variance = 0.\n\n\nalphas\nlist\n[[1, 0], [1, 0]]\nList of means and variances from which to sample the anomalous exponent of each state. If element size is one, we consider variance = 0.\n\n\ngamma_d\nlist\n[1]\nMinimum factor between D of diffusive states (see ._sampling_diff_parameters). Size is number of states -1 (in this case size 1)\n\n\nepsilon_a\nlist\n[0]\nDistance between alpha of diffusive states (see ._sampling_diff_parameters). Size is number of states -1 (in this case size 1)\n\n\nr\nint\n1\nRadius of the compartments.\n\n\ncomp_center\nNoneType\nNone\nIf given, center of the compartments. If None, centers are uniformly sampled.\n\n\nNc\nint\n10\nNumber of compartments\n\n\ntrans\nfloat\n0.1\nTransmittance of the boundaries\n\n\ndeltaT\nint\n1\nSampling time.\n\n\nReturns\ntuple\n\n- pos (array Tx2): particle’s position- labels (array Tx2): particle’s labels (see ._multi_state for details on labels)\n\n\n\n\nN = 50;  L = 20\nNc = 15; r = 1; L = 20\nDs = [[1,0], [150,0.1]]\nalphas = [[1, 0], [1.5,0.1]]\nr , L, Nc = (20, 256, 15)\ncomp_center = models_phenom._distribute_circular_compartments(Nc = Nc, r = r, L = L)\ntrajs, labels = models_phenom().confinement(N = N, L = L, comp_center = comp_center, trans = 0.1, Ds = Ds, \n                                            r = r, alphas = alphas, epsilon_a = [0])\n\n\nplot_trajs(trajs, L, N, labels = labels, plot_labels = True, comp_center = comp_center, r_cercle=r)"
  },
  {
    "objectID": "lib_nbs/models_theory.html",
    "href": "lib_nbs/models_theory.html",
    "title": "models_theory",
    "section": "",
    "text": "Currently the library containts the following models:\n\n\n\n\n\n\n\n\nFunction\nDimensions\nDescription\n\n\n\n\nbm\n(1D)\nBrownian motion\n\n\nfbm\n(1D/2D/3D)\nFractional browian motion, simulated via the stochastic Python library\n\n\nctrw\n(1D/2D/3D)\nContinuous time random walks\n\n\nlw\n(1D/2D/3D)\nLevy walks\n\n\nattm\n(1D/2D/3D)\nAnnealed transit time\n\n\nsbm\n(1D/2D/3D)\nScaled brownian motion\n\n\n\nThe class is organized as follows:\n\nFor every model in the previous list, there exists a function models_theory.name_walk which generates diffusion following that model. All functions have the same inputs and outputs:\nInputs\n\nT (int): lenght of the trajectory. Gets transformed to int if input is float.\nalpha (float): anomalous exponent\nD ({1,2,3}): dimension of the walk.\n\nOutputs\n\nnumpy.array of lenght DxT.\n\nThis is the recommended way of generating trajectories.\nFor every dimension, there exists a subclass models_theory._dimensionD, which gives access to the walk generators in each dimensions. Every walk is a function of these subclasses. This allows to access more features of the diffusion models:\n\nctrw\n\nregular_time (bool): if true, regularizes the trajectory so that every element in the output corresponds to a time step. If false, returns the positions and times at which steps were done.\n\nattm:\n\nregime ({1,2,3}): allows to chose the regime of ATTM model. See the original paper Phys. Rev. Lett. 112, 150603 (2014) for further details.\n\nsbm:\n\nsigma (float): variance of the noise generator (similar to a local diffusion coefficient).\n\n\n\nTo learn more on the use of this class, please visit this tutorial"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html",
    "href": "lib_nbs/utils_challenge.html",
    "title": "Help functions",
    "section": "",
    "text": "These functions are used to smooth a given vector of labels of heterogeneous processes by means of majority filter. It allows to define a minimum segment length.\n\n\n\n\n label_filter (label, window_size=5, min_seg=3)\n\nGiven a vector of changing labels, applies a majority filter such that the minimum segment of a particular label is bigger than the minimum set segment.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlabel\nlist\n\nlabel vector to filter.\n\n\nwindow_size\nint\n5\nSize of the window in which the majority filter is applied.\n\n\nmin_seg\nint\n3\nMinimum segment size after filtering.\n\n\nReturns\nnp.array\n\nFiltered label vector\n\n\n\n\n\n\n\n\n majority_filter (seq, width)\n\nGiven a vector, applies a majority filter of given width.\n\n\n\n\nType\nDetails\n\n\n\n\nseq\nlist\nVector to filter.\n\n\nwidth\nint\nSize of the window in which the filter is applied.\n\n\nReturns\nlist\nFiltered vector\n\n\n\n\nfig, axs = plt.subplots(3, 3, figsize = (9, 5), tight_layout = True)\n\nfor ax in axs.flatten():    \n    traj, labs = models_phenom()._multiple_state_traj(alphas = [0.7, 0.8], Ds = [0.01, 0.1])\n    filtered_d = label_filter(labs[:,1])\n    \n    ax.plot(labs[:, 1], '.', label = 'True label')\n    ax.plot(filtered_d, label = r'Filtered label')\n    \naxs[0,0].set_title(f'Majority filter with window size = {window_size}')\naxs[0,0].legend()\nplt.setp(axs, xticklabels = [], yticklabels = []);\n\n\n\n\n\n\n\nNote that smoothing the signal will have an effect on the actual proportion of time a particle spends in each state. This will be taken into account in the challenge. Here we showcase this effect:\n\nT = 100\ntraj, labs = models_phenom().multi_state(N = 500, alphas = [[0.7, 1],[0.4,2]], Ds = [[0, 1], [1, 0]], T = T)\n\n\nres_t = np.array([])\nres_ft = np.array([])\nfor label in tqdm(labs.transpose(1,0,2)[:,:,0]):\n    \n    # raw labels\n    CP = np.argwhere(label[1:] != label[:-1]).flatten()\n    if CP[-1] != 199: CP = np.append(CP, T-1)\n    CP = np.append(0, CP)\n\n    res_t = np.append(res_t, CP[1:] - CP[:-1])\n    \n    \n    # filtered labels\n    filt = label_filter(label)\n    \n    CP_f = np.argwhere(filt[1:] != filt[:-1]).flatten()\n    if CP_f[-1] != 199: CP_f = np.append(CP_f, T-1)\n    CP_f = np.append(0, CP_f)\n\n    res_ft = np.append(res_ft, CP_f[1:] - CP_f[:-1])\n\n\n\n\nWe show now the new transition rates (e.g. 1 over the residence time of a given state). Because we are minimum segment length of 3, we can actually approximate the filtered transition rate as the original times 2/3:\n\nprint(f' True transition rate: {1/np.mean(res_t)}\\n',\n      f'Filtered transition rate: {1/np.mean(res_ft)}\\n',\n      f'True rate x 2/3: {1/np.mean(res_t)*(2/3)}')\n\n True transition rate: 0.10947474747474747\n Filtered transition rate: 0.07402020202020201\n True rate x 2/3: 0.07298316498316498\n\n\n\n\n\n\nThe labels in the challenge will be the list of \\(n\\) changepoints as well as the \\(n+1\\) diffusion properties (\\(D\\) and \\(\\alpha\\)) for each segment. This function transforms the stepwise labels into three lists: CPs, \\(\\alpha\\)s and \\(D\\)s.\n\n\n\n\n label_continuous_to_list (labs)\n\nGiven an array of T x 2 labels containing the anomalous exponent and diffusion coefficient at each timestep, returns 3 arrays, each containing the changepoints, exponents and coefficient, respectively. If labs is size T x 3, then we consider that diffusive states are given and also return those.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nlabs\narray\nT x 2 or T x 3 labels containing the anomalous exponent, diffusion and diffusive state.\n\n\nReturns\ntuple\n- First element is the list of change points- The rest are corresponding segment properties (order: alpha, Ds and states) \n\n\n\n\n# Generate the trajectory\ntrajs, labels = models_phenom().multi_state(N = 1, T = 50)\n\n# Transform the labels:\nCP, alphas, Ds, _ = label_continuous _to_list(labels[:,-1,:])\n\nplt.figure(figsize=(5, 3))\nplt.plot(labels[:, -1, 1], 'o', alpha = 0.4, label = 'Continuous label')\nplt.scatter(CP-1, Ds, c = 'C1', label = 'CP-1 and value of previous segment')\nplt.legend(); plt.xlabel('T'); plt.ylabel(r'$\\alpha$')\n\nText(0, 0.5, '$\\\\alpha$')\n\n\n\n\n\n\n\n\n\nThis function does the opposite from than label_continuous _to_list. From a list of properties as the one used in ANDI 2022, creates continuous labels\n\n\n\n\n label_list_to_continuous (CP, label)\n\nGiven a list of change points and the labels of the diffusion properties of the resulting segments, generates and array of continuous labels. The last change point indicates the array length.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nCP\narray, list\nlist of change points. Last change point indicates label length.\n\n\nlabel\narray, list\nlist of segment properties\n\n\nReturns\narray\nContinuous label created from the given change points and segment properties\n\n\n\n\nCP = [3,24,34]\nlabel = [0.5, 0.4, 1]\ncont = label_list_to_continuous(CP, label)\nplt.figure(figsize = (3,1))\nplt.plot(cont, c = 'C1')\n[plt.axvline(c, c = 'k', ls = '--') for c in CP[:-1]];\n\n\n\n\n\n\n\n\n\n\n\n\n array_to_df (trajs, labels, min_length=10, fov_origin=[0, 0],\n              fov_length=100.0, cutoff_length=10)\n\nGiven arrays for the position and labels of trajectories, creates a dataframe with that data. The function also applies the demanded FOV. If you don’t want a field of view, chose a FOV length bigger (smaller) that your maximum (minimum) trajectory position.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrajs\narray\n\nTrajectories to store in the df (dimension: T x N x 3)\n\n\nlabels\narray\n\nLabels to store in the df (dimension: T x N x 3)\n\n\nmin_length\nint\n10\n\n\n\nfov_origin\nlist\n[0, 0]\nBottom left point of the square defining the FOV.\n\n\nfov_length\nfloat\n100.0\nSize of the box defining the FOV.\n\n\ncutoff_length\nint\n10\nMinimum length of a trajectory inside the FOV to be considered in the output dataset.\n\n\nReturns\ntuple\n\n- df_in (dataframe): dataframe with trajectories- df_out (datafram): dataframe with labels \n\n\n\n\n#trajs, labels = models_phenom().multi_state(T = 200, N = 10, alphas=[0.5, 1], Ds = [1,1], L = 100)\ntrajs, labels = models_phenom().single_state(T = 200, N = 10)\n\n# Changing dimensions\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_out = array_to_df(trajs, labels)\n\n\ndf_out.head()\n\n\n\n\n\n  \n    \n      \n      traj_idx\n      Ds\n      alphas\n      states\n      changepoints\n    \n  \n  \n    \n      0\n      0\n      [1.0]\n      [1.0]\n      [2.0]\n      [115]\n    \n    \n      1\n      1\n      [1.0]\n      [1.0]\n      [2.0]\n      [22]\n    \n    \n      2\n      2\n      [1.0]\n      [1.0]\n      [2.0]\n      [10]\n    \n    \n      3\n      3\n      [1.0]\n      [1.0]\n      [2.0]\n      [39]\n    \n    \n      4\n      4\n      [1.0]\n      [1.0]\n      [2.0]\n      [28]\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n df_to_array (df, pad=-1)\n\nTransform a dataframe as the ones given in the ANDI challenge 2 (i.e. 4 columns: traj_idx, frame, x, y) into a numpy array. To deal with irregular temporal supports, we pad the array whenever the trajectory is not present. The output array has the typical shape of ANDI datasets: TxNx2\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ndataframe\n\nDataframe with four columns ‘traj_idx’: the trajectory index, ‘frame’ the time frame and ‘x’ and ‘y’ the positions of the particle.\n\n\npad\nint\n-1\nNumber to use as padding.\n\n\nReturns\narray\n\nArray containing the trajectories from the dataframe, with usual ANDI shape (TxNx2)."
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#changepoint-pairing",
    "href": "lib_nbs/utils_challenge.html#changepoint-pairing",
    "title": "Help functions",
    "section": "Changepoint pairing",
    "text": "Changepoint pairing\nWe use an assignment algorithm to pair predicted and groundtruth changepoints. From there, we will calculate the various metrics of the challenge.\n\n\nchangepoint_assignment\n\n changepoint_assignment (GT, preds)\n\nGiven a list of groundtruth and predicted changepoints, solves the assignment problem via the Munkres algorithm (aka Hungarian algorithm) and returns two arrays containing the index of the paired groundtruth and predicted changepoints, respectively.\nThe distance between change point is the Euclidean distance.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nGT\nlist\nList of groundtruth change points.\n\n\npreds\nlist\nList of predicted change points.\n\n\nReturns\ntuple\n- tuple of two arrays, each corresponding to the assigned GT and pred changepoints- Cost matrix\n\n\n\n\nngts = 10; npreds = 6; T = 100\nGT = np.sort(np.random.choice(np.arange(1,T), ngts, replace = False))\npreds = np.sort(np.random.choice(np.arange(1,T)*0.5, npreds, replace = False)).astype(int)\nprint('GT:', GT)\nprint('Pred:', preds)\nchangepoint_assignment(GT, preds)[0]\n\nGT: [ 2  8 24 33 34 54 55 64 73 85]\nPred: [ 8 11 16 30 36 47]\n\n\n(array([0, 1, 2, 3, 4, 5], dtype=int64),\n array([1, 0, 2, 3, 4, 5], dtype=int64))\n\n\n\n\n\nchangepoint_alpha_beta\n\n changepoint_alpha_beta (GT, preds, threshold=10)\n\nCalculate the alpha and beta measure of paired changepoints. Inspired from Supplemantary Note 3 in https://www.nature.com/articles/nmeth.2808\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT\nlist\n\nList of groundtruth change points.\n\n\npreds\nlist\n\nList of predicted change points.\n\n\nthreshold\nint\n10\nDistance from which predictions are considered to have failed. They are then assigned this number.\n\n\nReturns\ntuple\n\nalpha, beta\n\n\n\n\nlabels = [r'Random Guess + $N_p>N_{gt}$',\n          r'Random Guess + $N_p<N_{gt}$',\n          r'GT + rand $\\in [-3, 3]$',\n          r'GT + rand $\\in [-1, 1]$']\n\nfig, ax = plt.subplots(figsize = (4,3))\nalpha = 0.2\n\nT = 200; ngts = 15; \n\nfor case, (label, color) in enumerate(zip(labels, ['C0', 'C1', 'C2', 'C3'])):\n\n    alphas, betas = [], []\n    for _ in range(100):\n        \n        GT = np.sort(np.random.choice(np.arange(1,T), ngts, replace = False))\n        if case == 0:\n            npreds = np.random.randint(low = ngts, high = ngts*2)\n            preds = np.sort(np.random.choice(np.arange(1,T), npreds, replace = False)) \n        elif case == 1:\n            npreds = np.random.randint(low = 1, high = ngts)\n            preds = np.sort(np.random.choice(np.arange(1,T), npreds, replace = False))     \n        elif case == 2:\n            preds = GT + np.random.randint(-3, 3, ngts)\n        elif case == 3:\n            preds = GT + np.random.randint(-1, 1, ngts)\n            \n        alpha, beta = changepoint_alpha_beta(GT, preds)\n        \n        alphas.append(alpha)\n        betas.append(beta)\n     \n    \n    ax.scatter(alphas, betas, c = color, alpha = alpha)\n    ax.scatter(np.mean(alphas), np.mean(betas), c = color, label = label, s = 50, marker = 's', edgecolors = 'k')\nplt.setp(ax, xlabel = r'$\\alpha$', ylabel = r'$\\beta$')\nax.legend(loc = (1.01,0.4))\n\n<matplotlib.legend.Legend at 0x16d9be73f70>\n\n\n\n\n\n\n\n\njaccard_index\n\n jaccard_index (TP:int, FP:int, FN:int)\n\nGiven the true positive, false positive and false negative rates, calculates the Jaccard Index\n\n\n\n\nType\nDetails\n\n\n\n\nTP\nint\ntrue positive\n\n\nFP\nint\nfalse positive\n\n\nFN\nint\nfalse negative\n\n\nReturns\nfloat\nJaccard Index\n\n\n\n\n\n\nsingle_changepoint_error\n\n single_changepoint_error (GT, preds, threshold=5)\n\nGiven the groundtruth and predicted changepoints for a single trajectory, first solves the assignment problem between changepoints, then calculates the RMSE of the true positive pairs and the Jaccard index.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT\nlist\n\nList of groundtruth change points.\n\n\npreds\nlist\n\nList of predicted change points.\n\n\nthreshold\nint\n5\nDistance from which predictions are considered to have failed. They are then assigned this number.\n\n\nReturns\ntuple\n\n- TP_rmse: root mean square error of the true positive change points.- Jaccard Index of the ensemble predictions \n\n\n\n\n\n\nensemble_changepoint_error\n\n ensemble_changepoint_error (GT_ensemble, pred_ensemble, threshold=5)\n\nGiven an ensemble of groundtruth and predicted change points, iterates over each trajectory’s changepoints. For each, it solves the assignment problem between changepoints. Then, calculates the RMSE of the true positive pairs and the Jaccard index over the ensemble of changepoints (i.e. not the mean of them w.r.t. to the trajectories)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT_ensemble\nlist, array\n\nEnsemble of groutruth change points.\n\n\npred_ensemble\nlist\n\nEnsemble of predicted change points.\n\n\nthreshold\nint\n5\nDistance from which predictions are considered to have failed. They are then assigned this number.\n\n\nReturns\ntuple\n\n- TP_rmse: root mean square error of the true positive change points.- Jaccard Index of the ensemble predictions\n\n\n\n\nlabels = ['Random Guess + Incorrect number',\n          r'GT + rand $\\in [-3, 3]$',\n          r'GT + rand $\\in [-1, 1]$']\n\nfig, ax = plt.subplots(figsize = (3,3))\nalpha = 0.2\n\nT = 200; ngts = 10; npreds = 8\n\nfor case, (label, color) in enumerate(zip(labels, ['C0', 'C1', 'C2'])):\n    \n    rmse, ji = [], []\n    GT, preds = [], []\n    for _ in range(100):\n\n        GT.append(np.sort(np.random.choice(np.arange(1,T), ngts, replace = False)))\n        if case == 0:\n            preds.append(np.sort(np.random.choice(np.arange(1,T), npreds, replace = False)))                  \n        elif case == 1:\n            preds.append(GT[-1] + np.random.randint(-3, 3, ngts))\n        elif case == 2:\n            preds.append(GT[-1] + np.random.randint(-1, 1, ngts))\n\n        assignment, _ = changepoint_assignment(GT[-1], preds[-1])\n        assignment = np.array(assignment)\n\n        RMSE, JI = single_changepoint_error(GT[-1], preds[-1], threshold = 5)     \n        \n        rmse.append(RMSE)\n        ji.append(JI)\n\n    rmse_e, ji_e = ensemble_changepoint_error(GT, preds, threshold = 5)\n    \n    ax.scatter(rmse, ji, c = color, alpha = alpha)\n    ax.scatter(rmse_e, ji_e, c = color, label = label, s = 50, marker = 's', edgecolors = 'k')\nplt.setp(ax, xlabel = 'TP RMSE', ylabel = 'Jaccard')\nax.legend(loc = (0.91,0.4))\n\n<matplotlib.legend.Legend at 0x16d9e1d3910>"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#segments-pairing",
    "href": "lib_nbs/utils_challenge.html#segments-pairing",
    "title": "Help functions",
    "section": "Segments pairing",
    "text": "Segments pairing\nHere we focus on pairing the segments arising from a list of changepoints. We will use this to latter compare the predicted physical properties for each segment\n\n\ncreate_binary_segment\n\n create_binary_segment (CP:list, T:int)\n\nGiven a set of changepoints and the lenght of the trajectory, create segments which are equal to one if the segment takes place at that position and zero otherwise.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nCP\nlist\nlist of changepoints\n\n\nT\nint\nlength of the trajectory\n\n\nReturns\nlist\nlist of arrays with value 1 in the temporal support of the current segment.\n\n\n\n\nT= 50\nGT = np.sort(np.random.choice(np.arange(1,T), 10, replace = False))\nplt.figure(figsize = (4,3))\nfor idx, x in enumerate(create_binary_segment(GT, T)):\n    plt.plot(x*idx, 'o')\n\n\n\n\n\n\n\njaccard_between_segments\n\n jaccard_between_segments (gt, pred)\n\nGiven two segments, calculates the Jaccard index between them by considering TP as correct labeling, FN as missed events and FP leftover predictions.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ngt\narray\ngroundtruth segment, equal to one in the temporal support of the given segment, zero otherwise.\n\n\npred\narray\npredicted segment, equal to one in the temporal support of the given segment, zero otherwise.\n\n\nReturns\nfloat\nJaccard index between the given segments.\n\n\n\n\n\n\nsegment_assignment\n\n segment_assignment (GT, preds, T:int=None)\n\nGiven a list of groundtruth and predicted changepoints, generates a set of segments. Then constructs a cost matrix by calculting the Jaccard Index between segments. From this cost matrix, we solve the assignment problem via the Munkres algorithm (aka Hungarian algorithm) and returns two arrays containing the index of the groundtruth and predicted segments, respectively.\nIf T = None, then we consider that GT and preds may have different lenghts. In that case, the end of the segments is the the last CP of each set of CPs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT\nlist\n\nList of groundtruth change points.\n\n\npreds\nlist\n\nList of predicted change points.\n\n\nT\nint\nNone\nLength of the trajectory. If None, considers different GT and preds length.\n\n\nReturns\ntuple\n\n- tuple of two arrays, each corresponding to the assigned GT and pred changepoints- Cost matrix calculated via JI of segments \n\n\n\n\nExamples\nPredictions close to groundtruth\n\nT = 200; \nngts = 10; \nGT = np.sort(np.random.choice(np.arange(1,T), ngts, replace = False))\npreds = np.sort(GT + np.random.randint(-5, 5, 1) )\n\nseg_GT = create_binary_segment(GT, T)\nseg_preds = create_binary_segment(preds, T)   \n\n[row_ind, col_ind], cost_matrix = segment_assignment(GT, preds, T)\n\nfig, axs = plt.subplots(2, 5, figsize = (15, 6))\nfor r, c, ax in zip(row_ind, col_ind, axs.flatten()):\n    ax.set_title(f'1 - JI = {np.round(cost_matrix[r, c], 2)}')\n    ax.plot(seg_GT[r], label = 'Groundtruth')\n    ax.plot(seg_preds[c], label = 'Prediction')\naxs[0,0].legend()\n\n<matplotlib.legend.Legend at 0x16d9feff970>\n\n\n\n\n\nDifferent size between predictions and trues\n\nT1 = 200; T2 = 100\nngts = 10; \nGT = np.sort(np.random.choice(np.arange(1,T1), ngts, replace = False))\npreds = np.sort(np.random.choice(np.arange(1,T2), 5, replace = False))\n\nseg_GT = create_binary_segment(GT, T1)\nseg_preds = create_binary_segment(preds, T2)   \n\n[row_ind, col_ind], cost_matrix = segment_assignment(GT, preds)\n\nfig, axs = plt.subplots(1, 5, figsize = (15, 3))\nfor r, c, ax in zip(row_ind, col_ind, axs.flatten()):\n    ax.set_title(f'1 - JI = {np.round(cost_matrix[r, c], 2)}')\n    ax.plot(seg_GT[r], label = 'Groundtruth')\n    ax.plot(seg_preds[c], label = 'Prediction')\naxs[0].legend()\n\n<matplotlib.legend.Legend at 0x16d9bf016f0>\n\n\n\n\n\nPredictions very different to groundtruth\n\nT = 200;\nngts = 5; npreds = 5;\nGT = np.sort(np.random.choice(np.arange(1,T), ngts, replace = False))\npreds = np.sort(np.random.choice(np.arange(1,T), npreds, replace = False))  \n\nseg_GT = create_binary_segment(GT, T)\nseg_preds = create_binary_segment(preds, T)\n\n[row_ind, col_ind], cost_matrix = segment_assignment(GT, preds, T)\n\nfig, axs = plt.subplots(1, 5, figsize = (15, 3))\nfor r, c, ax in zip(row_ind, col_ind, axs.flatten()):\n    ax.set_title(f'1 - JI = {np.round(cost_matrix[r, c], 2)}')\n    ax.plot(seg_GT[r], label = 'Groundtruth')\n    ax.plot(seg_preds[c], label = 'Prediction')\naxs[0].legend()\n\n<matplotlib.legend.Legend at 0x16da063bc70>"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#segment-properties-comparison",
    "href": "lib_nbs/utils_challenge.html#segment-properties-comparison",
    "title": "Help functions",
    "section": "Segment properties comparison",
    "text": "Segment properties comparison\nWe use the segment pairing functions that we have defined above to compute various metrics between the properties of predicted and groundtruth segments.\n\nMetrics of segment properties\n\n\n\nmetric_diffusive_state\n\n metric_diffusive_state (gt=None, pred=None, max_error=False)\n\nCompute the F1 score between diffusive states.\n\n\n\nmetric_diffusion_coefficient\n\n metric_diffusion_coefficient (gt=None, pred=None, threshold_min=1e-12,\n                               max_error=190.86835960820298)\n\nCompute the mean squared log error (msle) between diffusion coefficients. Checks the current bounds of diffusion from models_phenom to calculate the maximum error.\n\n\n\nmetric_anomalous_exponent\n\n metric_anomalous_exponent (gt=None, pred=None, max_error=1.999)\n\nCompute the mean absolute error (mae) between anomalous exponents. Checks the current bounds of anomalous exponents from models_phenom to calculate the maximum error.\n\nx = np.random.rand(100)\ny = np.random.rand(100)\n\n\nmetric_diffusion_coefficient(x+2,y+2, threshold_min=-2)\n\n0.014261449910975834\n\n\n\n\nPairing and metrics calculation\n\n\n\ncheck_no_changepoints\n\n check_no_changepoints (GT_cp, GT_alpha, GT_D, GT_s, preds_cp,\n                        preds_alpha, preds_D, preds_s, T:bool|int=None)\n\nGiven predicionts over changepoints and variables, checks if in both GT and preds there is an absence of change point. If so, takes that into account to pair variables.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT_cp\nlist, int, float\n\nGroundtruth change points\n\n\nGT_alpha\nlist, float\n\nGroundtruth anomalous exponent\n\n\nGT_D\nlist, float\n\nGroundtruth diffusion coefficient\n\n\nGT_s\nlist, float\n\nGroundtruth diffusive state\n\n\npreds_cp\nlist, int, float\n\nPredicted change points\n\n\npreds_alpha\nlist, float\n\nPredicted anomalous exponent\n\n\npreds_D\nlist, float\n\nPredicted diffusion coefficient\n\n\npreds_s\nlist, float\n\nPredicted diffusive state\n\n\nT\nbool | int\nNone\n(optional) Length of the trajectories. If none, last change point is length.\n\n\nReturns\ntuple\n\n- False if there are change points. True if there were missing change points.- Next three are either all Nones if change points were detected, or paired exponents, coefficient and states if some change points were missing.\n\n\n\n\n\n\nsegment_property_errors\n\n segment_property_errors (GT_cp, GT_alpha, GT_D, GT_s, preds_cp,\n                          preds_alpha, preds_D, preds_s,\n                          return_pairs=False, T=None)\n\nGiven predicionts over change points and the value of diffusion parameters in the generated segments, computes the defined metrics.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nGT_cp\nlist, int, float\n\nGroundtruth change points\n\n\nGT_alpha\nlist, float\n\nGroundtruth anomalous exponent\n\n\nGT_D\nlist, float\n\nGroundtruth diffusion coefficient\n\n\nGT_s\nlist, float\n\nGroundtruth diffusive state\n\n\npreds_cp\nlist, int, float\n\nPredicted change points\n\n\npreds_alpha\nlist, float\n\nPredicted anomalous exponent\n\n\npreds_D\nlist, float\n\nPredicted diffusion coefficient\n\n\npreds_s\nlist, float\n\nPredicted diffusive state\n\n\nreturn_pairs\nbool\nFalse\nIf True, returns the assigment pairs for each diffusive property.\n\n\nT\nNoneType\nNone\n(optional) Length of the trajectories. If none, last change point is length.\n\n\nReturns\ntuple\n\n- if return_pairs = True, returns the assigned pairs of diffusive properties- if return_pairs = False, returns the errors for each diffusive property\n\n\n\nWe generate some random predictions to check how the metrics behave. We consider errors also in the change point predictions, hence there will be some segment mismatchings, which will affect the diffusive properties predictions:\n\nT = 200; \nngts = 10; \nerrors_alpha = np.linspace(0, 1, ngts)\nerrors_d = np.linspace(0, 10, ngts)\nerrors_s = np.linspace(0, 1, ngts)\n\nmetric_a, metric_d, metric_s = [], [], []\nfor error_a, error_d, error_s in zip(errors_alpha, errors_d, errors_s):\n    la, ld, ls = [], [], []\n    for _ in range(100):\n\n        GT_cp = np.sort(np.random.choice(np.arange(1,T-1), ngts, replace = False))\n        preds_cp = np.sort(np.random.choice(np.arange(1,T-1), ngts, replace = False)) \n\n        GT_alpha = np.random.rand(GT_cp.shape[0]+1)\n        preds_alpha = GT_alpha + np.random.randn(preds_cp.shape[0]+1)*error_a\n\n        GT_D = np.abs(np.random.randn(GT_cp.shape[0]+1)*10)\n        preds_D = GT_D + np.abs(np.random.randn(preds_cp.shape[0]+1))*error_d\n        \n        GT_s = np.random.randint(0, 5, GT_cp.shape[0]+1)\n        coin = np.random.rand(len(GT_s))\n        preds_s = GT_s.copy()\n        preds_s[coin < error_s] = np.random.randint(0, 5, len(coin[coin < error_s]))\n\n        m_a, m_d, m_s = segment_property_errors(GT_cp, GT_alpha, GT_D, GT_s, preds_cp, preds_alpha, preds_D, preds_s, T = T)\n        \n        la.append(m_a); ld.append(m_d); ls.append(m_s)\n    \n    metric_a.append(np.mean(la))\n    metric_d.append(np.mean(ld))    \n    metric_s.append(np.mean(ls))\n\nWith no error in the changepoint predicitions:\n\nfig, ax = plt.subplots(1, 3, figsize = (9, 3), tight_layout = True)\n\nax[0].plot(np.arange(ngts), errors_alpha, c = 'C0', ls = '--', label = 'Expected with no assigment error')\nax[0].plot(np.arange(ngts), metric_a, c = 'C0')\nax[0].set_title(r'Error in $\\alpha$ (MAE)')\n\n#ax[1].plot(np.arange(ngts), errors_d, c = 'C1', ls = '--')\nax[1].plot(np.arange(ngts), metric_d, c = 'C1')\nax[1].set_title(r'Error in $D$ (MSLE)')\n\nax[2].plot(np.arange(ngts), metric_s, c = 'C1')\nax[2].set_title(r'Error in states (JI)')\n\nplt.setp(ax, xlabel = 'Error magnitude')\n\n[Text(0.5, 0, 'Error magnitude'),\n Text(0.5, 0, 'Error magnitude'),\n Text(0.5, 0, 'Error magnitude')]\n\n\n\n\n\nWith error in the changepoint predicitions:\n\nfig, ax = plt.subplots(1, 3, figsize = (9, 3), tight_layout = True)\n\nax[0].plot(np.arange(ngts), errors_alpha, c = 'C0', ls = '--', label = 'Expected with no assigment error')\nax[0].plot(np.arange(ngts), metric_a, c = 'C0')\nax[0].set_title(r'Error in $\\alpha$ (MAE)')\n\n#ax[1].plot(np.arange(ngts), errors_d, c = 'C1', ls = '--')\nax[1].plot(np.arange(ngts), metric_d, c = 'C1')\nax[1].set_title(r'Error in $D$ (MSLE)')\n\nax[2].plot(np.arange(ngts), metric_s, c = 'C1')\nax[2].set_title(r'Error in states (JI)')\n\nplt.setp(ax, xlabel = 'Error magnitude')\n\n[Text(0.5, 0, 'Error magnitude'),\n Text(0.5, 0, 'Error magnitude'),\n Text(0.5, 0, 'Error magnitude')]"
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#ensemble-metrics",
    "href": "lib_nbs/utils_challenge.html#ensemble-metrics",
    "title": "Help functions",
    "section": "Ensemble metrics",
    "text": "Ensemble metrics\n\n\nextract_ensemble\n\n extract_ensemble (state_label, dic)\n\nGiven an array of the diffusive state and a dictionary with the diffusion information, returns a summary of the ensemble properties for the current dataset.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nstate_label\narray\nArray containing the diffusive state of the particles in the dataset.For multi-state and dimerization, this must be the number associated to thestate (for dimerization, 0 is free, 1 is dimerized). For the rest, we followthe numeration of models_phenom().lab_state.\n\n\ndic\ndict\nDictionary containing the information of the input dataset.\n\n\nReturns\narray\nMatrix containing the ensemble information of the input dataset. It has the following shape: |mu_alpha1 mu_alpha2 … | |sigma_alpha1 sigma_alpha2 … | |mu_D1 mu_D1 … |  |sigma_D1 sigma_D2 … | |counts_state1 counts_state2 … |\n\n\n\n\n\n\nmultimode_dist\n\n multimode_dist (params, weights, bound, x, normalized=False)\n\nGenerates a multimodal distribution with given parameters. Also accounts for single mode if weight is float or int.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nlist\n\nMean and variances of every mode.\n\n\nweights\nlist, float\n\nWeight of every mode. If float, we consider a single mode.\n\n\nbound\ntuple\n\nBounds (min, max) of the functions support.\n\n\nx\narray\n\nSupport upon which the distribution is created.\n\n\nnormalized\nbool\nFalse\n\n\n\nReturns\narray\n\nValue of the distribution in each point of the given support\n\n\n\n\n\n\ndistribution_distance\n\n distribution_distance (p:<built-infunctionarray>, q:<built-\n                        infunctionarray>)\n\nCalculates mean absolute error between two distributions.\n\n\n\n\nType\nDetails\n\n\n\n\np\narray\ndistribution 1\n\n\nq\narray\ndistribution 2\n\n\nReturns\nfloat\ndistance between distributions\n\n\n\n\nmeans = np.linspace(0, 2, 30)\nnormalize = False\nfig = plt.figure(figsize=(20, 5))\ngs = fig.add_gridspec(2, 10)\n\n# True distribution\nx = np.arange(0, 3, 0.01)\nparams = [[1.7,0.01]]\nweights = [1]\ntrue = multimode_dist(params, weights, bound = [0, 3], x = x, normalized = normalize)\n\n\n\nKL = []\nfor idx, mean in enumerate(means):\n    params = [[mean, 0.01]]\n    weights = [1]\n    pred = multimode_dist(params, weights, bound = [0, 3], x = x, normalized = normalize)  \n    KL.append(distribution_distance(true, pred))  \n    \n    if idx % 3 == 0:\n        \n        ax = fig.add_subplot(gs[0, int(idx/3)])\n        ax.plot(x, true, label = 'True')\n        ax.plot(x, pred, label = 'Predicted')        \n        plt.setp(ax, yticks = []);\n        \n      \n    if idx == 0:\n        ax.legend()\n    \nax_kl = fig.add_subplot(gs[1, :])\nax_kl.plot(KL, '-o')\nplt.setp(ax_kl, ylabel = 'MAE')\nax_kl.grid()\n\n\n\n\n\n\nCalculate ensemble metric\n\n\n\nerror_Ensemble_dataset\n\n error_Ensemble_dataset (true_data, pred_data, return_distributions=False)\n\nCalculates the ensemble metrics for the ANDI 2022 challenge. The input are matrices of shape:\n\n\n\ncol1 (state 1)\ncol2 (state 2)\ncol3 (state 3)\n…\n\n\n\n\n\\(\\mu_a^1\\)\n\\(\\mu_a^2\\)\n\\(\\mu_a^3\\)\n…\n\n\n\\(\\sigma_a^1\\)\n\\(\\sigma_a^2\\)\n\\(\\sigma_a^3\\)\n…\n\n\n\\(\\mu_D^1\\)\n\\(\\mu_D^2\\)\n\\(\\mu_D^3\\)\n…\n\n\n\\(\\sigma_D^1\\)\n\\(\\sigma_D^2\\)\n\\(\\sigma_D^3\\)\n…\n\n\n\\(N_1\\)\n\\(N_2\\)\n\\(N_3\\)\n…\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrue_data\narray\n\nMatrix containing the groundtruth data.\n\n\npred_data\narray\n\nMatrix containing the predicted data.\n\n\nreturn_distributions\nbool\nFalse\nIf True, the function also outputs the generated distributions.\n\n\nReturns\ntuple\n\n- distance_alpha: distance between anomalous exponents- distance_D: distance between diffusion coefficients- dists (if asked): distributions of both groundtruth and predicted data."
  },
  {
    "objectID": "lib_nbs/utils_challenge.html#single-trajectory-metrics",
    "href": "lib_nbs/utils_challenge.html#single-trajectory-metrics",
    "title": "Help functions",
    "section": "Single trajectory metrics",
    "text": "Single trajectory metrics\nThe participants will have to output predictions in a .txt file were each line corresponds to the predictions of a trajectory. The latter have to be ordered as:\n0, d\\(_0\\), a\\(_0\\), s\\(_0\\), t\\(_1\\), d\\(_1\\), a\\(_1\\), s\\(_1\\), t\\(_2\\), d\\(_2\\), a\\(_2\\), s\\(_2\\), …. t\\(_n\\), d\\(_n\\), a\\(_n\\), s\\(_n\\),\\(T\\)\nwhere the first number corresponds to the trajectory index, then d\\(_i\\), a\\(_i\\), s\\(_i\\) correspond to the diffusion coefficient, anomalous exponent and diffusive state of the \\(i\\)-th segment. For the latter, we have the following code: - 0: immobile - 1: confined - 2: brownian - 3: anomalous\nLast, t\\(_j\\) corresponds to the \\(j\\)-th changepoints. The last changepoint \\(T\\) corresponds to the length of the trajectory. Each prediction must contain \\(C\\) changepoints and \\(C\\) segments property values. If this is not fulfilled, the whole trajectory is considered as mispredicted.\nThe .txt file will be first inspected. The data will then be collected into a dataframe\n\n\ncheck_prediction_length\n\n check_prediction_length (pred)\n\nGiven a trajectory segments prediction, checks whether it has C changepoints and C+1 segments properties values. As it must also contain the index of the trajectory, this is summarized by being multiple of 4. In some cases, the user needs to also predict the final point of the trajectory. In this case, we will have a residu of 1.\n\n\n\nseparate_prediction_values\n\n separate_prediction_values (pred)\n\nGiven a prediction over trjaectory segments, extracts the predictions for each segment property as well as the changepoint values.\n\n\n\nload_file_to_df\n\n load_file_to_df (path_file, columns=['traj_idx', 'Ds', 'alphas',\n                  'states', 'changepoints'])\n\nGiven the path of a .txt file, extract the segmentation predictions based on the rules of the ANDI Challenge 2022\nSaving fake data for test\n\nfile_gt, file_p = [], []\nT = 200; ngts = 10;\nfor traj in range(100):\n    GT_cp = np.sort(np.random.choice(np.arange(1,T), ngts, replace = False))\n    preds_cp = np.sort(np.random.choice(np.arange(1,T+50), ngts, replace = False)) \n\n    GT_alpha = np.random.rand(GT_cp.shape[0]+1)\n    preds_alpha = GT_alpha# + 0.1 #np.random.randn(preds_cp.shape[0]+1)*0.1\n\n    GT_D = np.abs(np.random.randn(GT_cp.shape[0]+1)*10)\n    preds_D = GT_D + 1.5 #np.abs(np.random.randn(preds_cp.shape[0]+1))*1.6\n    \n    GT_state = np.random.randint(0, high = 5, size = GT_cp.shape[0]+1)\n    preds_state = np.random.randint(0, high = 5, size = preds_cp.shape[0]+1)\n    \n    list_gt, list_p = [traj, GT_D[0], GT_alpha[0], GT_state[0]], [traj, preds_D[0], preds_alpha[0], preds_state[0]]\n    for gtc, gta, gtd, gts, pc, pa, pd, ps in zip(GT_cp, GT_alpha[1:], GT_D[1:], GT_state[1:], preds_cp, preds_alpha[1:], preds_D[1:], preds_state[1:]):\n        list_gt += [gtc, gtd, gta, gts]\n        list_p += [pc, pd, pa, ps]\n        \n    file_gt.append(list_gt)\n    if traj != 6:\n        file_p.append(list_p)\n        \npred_path, true_path = 'pred_test.txt', 'true_test.txt'\nnp.savetxt(true_path, file_gt, delimiter=',')\nnp.savetxt(pred_path, file_p, delimiter=',')\n\nRecovering the data\n\npred_path, true_path = 'pred_test.txt', 'true_test.txt'\n\ndf_pred = load_file_to_df(pred_path)\ndf_true = load_file_to_df(true_path)\n\n\n\n\nerror_SingleTraj_dataset\n\n error_SingleTraj_dataset (df_pred, df_true, threshold_error_alpha=2,\n                           max_val_alpha=2, min_val_alpha=0,\n                           threshold_error_D=100000.0,\n                           max_val_D=1000000.0, min_val_D=1e-06,\n                           threshold_error_s=-1, threshold_cp=10,\n                           prints=True, disable_tqdm=False)\n\nGiven two dataframes, corresponding to the predictions and true labels of a set of trajectories from the ANDI challenge 2022, calculates the corresponding metrics Columns must be for both (no order needed): traj_idx | alphas | Ds | changepoints | states df_true must also contain a column ‘T’.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf_pred\ndataframe\n\nPredictions\n\n\ndf_true\ndataframe\n\nGroundtruth\n\n\nthreshold_error_alpha\nint\n2\n(same for D, s, cp) Maximum possible error allowed. If bigger, it is substituted by this error.\n\n\nmax_val_alpha\nint\n2\n(same for D, s, cp) Maximum value of the parameter.\n\n\nmin_val_alpha\nint\n0\n(same for D, s, cp) Minimum value of the parameter.\n\n\nthreshold_error_D\nfloat\n100000.0\n\n\n\nmax_val_D\nfloat\n1000000.0\n\n\n\nmin_val_D\nfloat\n1e-06\n\n\n\nthreshold_error_s\nint\n-1\n\n\n\nthreshold_cp\nint\n10\n\n\n\nprints\nbool\nTrue\n\n\n\ndisable_tqdm\nbool\nFalse\nIf True, disables the progress bar.\n\n\nReturns\ntuple\n\n- rmse_CP: root mean squared error change points- JI: Jaccard index change points- error_alpha: mean absolute error anomalous exponents- error_D: mean square log error diffusion coefficients- error_s: Jaccar index diffusive states\n\n\n\n\nTest\nTwo datasets with same number of trajs\n\ntrajs, labels = models_phenom().immobile_traps(T = 200, N = 250, alphas=0.5, Ds = 1, L = 20, Nt = 100, Pb = 1, Pu = 0.5)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_trues = array_to_df(trajs, labels)\n\ntrajs, labels = models_phenom().immobile_traps(T = 200, N = 250, alphas=[0.5, 0.1], Ds = 1, L = 20, Nt = 100, Pb = 1, Pu = 0.5)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_preds = array_to_df(trajs, labels)\n\n\n\n\n\n\n\n\nerror_SingleTraj_dataset(df_preds, df_trues, prints = True, disable_tqdm=True);\n\nSummary of metrics assesments:\n\nChangepoint Metrics \nRMSE: 4.222 \nJaccard Index: 0.402 \n\nDiffusion property metrics \nMetric anomalous exponent: 0.3187475166785641 \nMetric diffusion coefficient: 0.2407373554580533 \nMetric diffusive state: 0.49893673577884107\n\n\nTwo datasets with different number of trajectories\n\ntrajs, labels = models_phenom().immobile_traps(T = 200, N = 350, alphas=[0.5,0.01], Ds = [1., 0.1], L = 20, Nt = 100, Pb = 1, Pu = 0.5)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_trues = array_to_df(trajs, labels, label_values=[0.5, 1], diff_states=[3, 2])\n\ntrajs, labels = models_phenom().immobile_traps(T = 200, N = 250, alphas=[0.5, 0.1], Ds = 1, L = 20, Nt = 100, Pb = 1, Pu = 0.5)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_preds = array_to_df(trajs, labels, label_values=[0.5, 1], diff_states=[3, 2])\n\n\n\n\n\n\n\n\nerror_SingleTraj_dataset(df_preds, df_trues, prints = True)\n\n\n\n\nSummary of metrics assesments:\n\n100 missing trajectory/ies. \n\nChangepoint Metrics \nRMSE: 4.051 \nJaccard Index: 0.441 \n\nDiffusion property metrics \nMetric anomalous exponent: 0.35483874584715985 \nMetric diffusion coefficient: 3.1690909054732668 \nMetric diffusive state: 0.4913685263947961\n\n\n(4.050708208970335,\n 0.4407643312101911,\n 0.35483874584715985,\n 3.1690909054732668,\n 0.4913685263947961)\n\n\n\ntrajs, labels = models_phenom().immobile_traps(T = 200, N = 5, alphas=[0.5,0.01], Ds = [1., 0.1], L = 20, Nt = 100, Pb = 1, Pu = 0.5)\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_preds  = array_to_df(trajs, labels, label_values=[0.5, 1], diff_states=[3, 2])\n\ntrajs, labels = models_phenom().multi_state(T = 200, N = 7, L = 20, M = np.array([[0.9,0.1],[0.9,0.1]]))\n\ntrajs = trajs.transpose((1, 0, 2)).copy()\nlabels = labels.transpose(1, 0, 2)\n\ndf_in, df_trues  = array_to_df(trajs, labels, label_values=[0.5, 1], diff_states=[3, 2])\n\n\n\n\n\n\n\n\nerror_SingleTraj_dataset(df_preds, df_trues, prints = True);\n\n\n\n\nSummary of metrics assesments:\n\n2 missing trajectory/ies. \n\nChangepoint Metrics \nRMSE: 2.903 \nJaccard Index: 0.188 \n\nDiffusion property metrics \nMetric anomalous exponent: 0.8269399281523714 \nMetric diffusion coefficient: 8.262443034681892 \nMetric diffusive state: 0.41379310344827586"
  },
  {
    "objectID": "lib_nbs/utils_trajectories.html",
    "href": "lib_nbs/utils_trajectories.html",
    "title": "utils_trajectories",
    "section": "",
    "text": "Trigonometry functions\nNeeded for the correct calculation of confined diffusion in circular compartments.\n\n\ntrigo\n\n trigo ()\n\nExtracted from https://stackoverflow.com/questions/30844482/what-is-most-efficient-way-to-find-the-intersection-of-a-line-and-a-circle-in-py\n\n\n\nAdding field of view (FOV)\n\n\nfind_nan_segments\n\n find_nan_segments (a, cutoff_length)\n\nExtract all segments of nans bigger than the set cutoff_length. If no segments are found, returns None. For each segments, returns the begining and end index of it.\nOutput: array of size (number of segments) x 2.\n\n\n\nsegs_inside_fov\n\n segs_inside_fov (traj, fov_origin, fov_length, cutoff_length)\n\nGiven a trajectory, finds the segments inside the field of view (FOV).\nArgs: :traj (array): set of trajectories of size N x T (N: number trajectories, T: length) :fov_origin (tuple): bottom right point of the square defining the FOV :fov_length (scalar): size of the box defining the FOV :cutoff_length (scalar): minimum length of a trajectory inside the FOV to be considered in the output dataset Return :segs_fov: set of segments inside the FOV\n\n\n\ninside_fov_dataset\n\n inside_fov_dataset (trajs, labels, fov_origin, fov_length,\n                     cutoff_length=10, func_labels=None,\n                     return_frames=False)\n\nGiven a dataset of trajectories with labels and a FOV parameters, returns a list of trajectories with the corresponding labels inside the FOV Args: :trajs (array): set of trajectories with shape T x N x 2. :labels (array): set of labels with shape T x N x 2. :fov_origin (tuple): bottom left point of the square defining the FOV. :fov_length (scalar): size of the box defining the FOV. :cutoff_length (scalar): minimum length of a trajectory inside the FOV to be considered in the output dataset. :func_labels (func): optinal function to be applied to the labels to take advantage of the loop Return: :trajs_fov (list): list 2D arrays containing the trajectories inside the field of view. :labels_fov (list): corresponding labels of the trajectories.\n\n\nTest\n\nL = 200; T = 100\nNs = [20,10,10]\nalphas = [1,1.5]\nD = 1   \n\ntrajs, labels = models_phenom().multi_state(N = 500, L = L, T = 50)\n\n\nfov_origin = [50,50]; fov_length = L*0.1\ntrajs_fov, labels_fov = inside_fov_dataset(trajs, labels, fov_origin, fov_length)\n\n\n\n\nPlotting trajectories\n\n\nplot_trajs\n\n plot_trajs (trajs, L, N, num_to_plot=3, labels=None, plot_labels=False,\n             traps_positions=None, comp_center=None, r_cercle=None)\n\n\nT = 500; N = 50; L = 1.2*128; D = 0.1\n\ntrajs_model1, labels = models_phenom().single_state(N = N, \n                                            L = L,\n                                            T = T,\n                                            Ds = D,\n                                            alphas = 0.5\n                                            )\n\nplot_trajs(trajs_model1, L, N)\n\n\n\n\n\n\n\nnbdev\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "lib_nbs/utils_videos.html",
    "href": "lib_nbs/utils_videos.html",
    "title": "utils_videos",
    "section": "",
    "text": "transform_to_video\n\n\ntransform_to_video\n\n transform_to_video (trajectory_data, particle_props={}, optics_props={},\n                     background_props={}, get_vip_particles=[],\n                     with_masks=False, save_video=False, path='')\n\nGenerates a video from a trajectory data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrajectory_data\nnp.ndarray\n\nGenerated through models_phenom. Array of shape (T, N, 2) containing the trajectories.\n\n\nparticle_props\ndict\n{}\nDictionary of properties for the particles.\n\n\noptics_props\ndict\n{}\nDictionary of properties for the optics.\n\n\nbackground_props\ndict\n{}\nDictionary of properties for the background.\n\n\nget_vip_particles\nlist\n[]\n\n\n\nwith_masks\nbool\nFalse\n\n\n\nsave_video\nbool\nFalse\n\n\n\npath\nstr\n\n\n\n\n\n\n\nTesting PSF size\n\nT = 10 # number of time steps (frames)\nN = 50 # number of particles (trajectories)\nL = 1* 128 # length of the box (pixels) -> exteneding fov by 1.5 times\nD = 0.1 # diffusion coefficient (pixels^2/frame)\ntrajs_test, labels = models_phenom().single_state(N=N, L=L, T=T, Ds=D, alphas=0.5)\n\n\nvideo = transform_to_video(\n    trajs_test,\n    optics_props={\"NA\": 1.46,\n                  \"wavelength\":500e-9},\n    particle_props={\"particle_intensity\": [100, 0], \"z\":0},\n    get_vip_particles= [1,2,3],\n    background_props={\"background_mean\":0, \"background_std\":0},\n    save_video=True, path = 'video_test.tiff', with_masks = False\n)\n\n\ndetections = np.array(video[1].get_property(\"position\", get_one=False))\nplt.figure(figsize=(7,7))\nplt.imshow(video[1], cmap=\"gray\")\nplt.scatter(detections[:,1], detections[:,0], marker='o', s=500, facecolors=\"none\", edgecolors=\"orange\")\n[plt.text(y+2, x, str(i), color=\"orange\") for i, (x, y) in enumerate(detections)]\nplt.show()\n\n\n\n\n\ncrop = croppedimage(video[0], list(reversed(detections[46])), window=8)\n\n\nplt.imshow(crop, cmap=\"gray\")\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar>\n\n\n\n\n\n\n\nTesting generation\n\nfrom andi_datasets.models_phenom import models_phenom\n\n\ntrajs_test, _ = models_phenom().single_state(N = 50, T = 100, L = 128, Ds = 1)\nvideo, masks = transform_to_video(trajs_test,\n                                  with_masks=True,\n                                  get_vip_particles=np.arange(55).tolist(),\n                                  particle_props = {\"z\": lambda: 0 + np.random.rand() * 1},\n                                  background_props = {\"background_mean\": 0,      # Mean background intensity\n                                                      \"background_std\": 0}, \n                                 optics_props = {'origin': [0,0,20,20]})\n\n\nplay_video(video)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nimg2 = ax1.imshow(video[1], cmap=\"gray\")\nax1.set_title(\"Frame 0\")\nax2.imshow(masks[1], cmap=\"gray\")\nax2.set_title(\"Mask 0\")\n\nText(0.5, 1.0, 'Mask 0')\n\n\n\n\n\n\nplt.figure(figsize=(5, 5))\nplt.imshow(video[0], cmap=\"gray\", zorder = -1)\nfor traj in np.moveaxis(trajs_test, 0, 1):\n    plt.plot(traj[:,1], traj[:,0], alpha=0.2)\nplt.xlim(0,L); plt.ylim(0,L)\nplt.show()\n\n\n\n\n\n\n\nNbdev\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "tutorials/andi2_tutorial_video_datasets.html",
    "href": "tutorials/andi2_tutorial_video_datasets.html",
    "title": "andi_datasets",
    "section": "",
    "text": "1. Single state diffusion\n\nT = 50 # number of time steps (frames)\nN = 50 # number of particles (trajectories)\nL = 1.5 * 128 # length of the box (pixels) -> exteneding fov by 1.5 times\nD = 1 # diffusion coefficient (pixels^2/frame)\n\n\ntrajs_model1, labels = models_phenom().single_state(N=N, L=L, T=T, Ds=D, alphas=0.5)\n\n\n\n6.1. Generating videos\nget_video_andi will create a feature required to generate videos in deeptrack.\n\ndic = {'snr': 10}\n\nvideo = get_video_andi(trajs_model1, noise_kwargs=dic)\n\n10\n\n\nIn order to generate the video into an image stack, one can use update().resolve() methods as shown below\n\nvideo_frames = video.update().resolve()\n\nC:\\Users\\Gorka\\anaconda3\\lib\\site-packages\\deeptrack\\optics.py:221: RuntimeWarning: invalid value encountered in sqrt\n  * np.sqrt(1 - (NA / refractive_index_medium) ** 2 * RHO),\n\n\n\nvideo_frames_uint8 = convert_uint8(video_frames)\nimageio.mimwrite(\"video2.mp4\", video_frames_uint8, fps=10)\n\n\nvideo_frames[0].shape\n\n(128, 128, 1)\n\n\n\nplt.imshow(video_frames[0], cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x21e38a89490>\n\n\n\n\n\nAlternatively the videos can played directly in jupyter notebook, using plot() method after update()\n\nvideo.update().plot(cmap=\"gray\")\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n<matplotlib.animation.ArtistAnimation at 0x207da9acf70>"
  },
  {
    "objectID": "tutorials/challenge_one_datasets.html",
    "href": "tutorials/challenge_one_datasets.html",
    "title": "Theory models and AnDi 2020",
    "section": "",
    "text": "Before starting, make sure that you know the details of the competition. You can learn about every in the associated webpage. A quick overview: there are three tasks: 1) inference of the anomalous diffusion exponent, 2) classification of the diffusion model and 3) segmentation of trajectories. For each task, there are three subtasks, one per dimension. To create a dataset for every task and dimension, each e.g. of \\(N=10\\) trajectories, you just need to run\n\nfrom andi_datasets.datasets_challenge import challenge_2020_dataset\n\n\nX1, Y1, X2, Y2, X3, Y3 = challenge_2020_dataset(N = 10)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\nGenerating dataset for dimension 1.\nGenerating dataset for dimension 2.\nGenerating dataset for dimension 3.\n\n\nX1, X2 and X3 correspond to the trajectories of Task 1, 2 and 3, respectively, while Y1, Y2 and Y_3 correspond to the labels. Each of these list contains three list, each one containing the trajectories for each dimension, in ascending order. For example, X2[2] contains the trajectories for Task 2 for dimension 3. If you only want to generate trajectories for a specific Task and dimensions, you can specify it as\n\nX1, Y1, X2, Y2, X3, Y3 = challenge_2020_dataset(N = 10, tasks = 1, dimensions = 3)\n\nCreating a dataset for task(s) 1 and dimension(s) 3.\nGenerating dataset for dimension 3.\n\n\nIn this case all the lists but the ones specified will be empty. For example:\n\nprint('Task 3, dimension 1 is empty: len(X3[0]) = '+ str(len(X3[0])))\nprint('But Task 1, dimension 3 has our 10 trajectories: len(X1[2]) = '+str(len(X1[2])))\n\nTask 3, dimension 1 is empty: len(X3[0]) = 0\nBut Task 1, dimension 3 has our 10 trajectories: len(X1[2]) = 10\n\n\nTo avoiding having to create a dataset every time we want to use it, you can use the option save_dataset, which will save a file for every dataset of trajectories (named task1.txt, task2.txt and task3.txt) and their corresponding labels (named ref1.txt, ref2.txt and ref3.txt). The first column of each file corresponds to the dimension of the trajectory.\n\nX1, Y1, X2, Y2, X3, Y3 = challenge_2020_dataset(N = 10, save_dataset = True)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\nGenerating dataset for dimension 1.\nGenerating dataset for dimension 2.\nGenerating dataset for dimension 3.\n\n\nNow that we saved the dataset, we can load it with the option load_dataset:\n\nX1, Y1, X2, Y2, X3, Y3 = challenge_2020_dataset(N = 10, load_dataset = True)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\n\n\nTake into account that if you saved \\(N=10\\) trajectories, you will only be able to load this 10, even if you ask for \\(N=20\\).\nBy default, the trajectories in this dataset will have a maximum length of 1000 steps and a minimum length of 10. To change that, you can use the options max_T and min_T. Take into account that you will need trajectories of 200 steps for task 3. Shorter trajectories will raise an error.\n\nX1, Y1, X2, Y2, X3, Y3 = challenge_2020_dataset(N = 10, min_T = 15, max_T = 200)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\nGenerating dataset for dimension 1.\nGenerating dataset for dimension 2.\nGenerating dataset for dimension 3.\n\n\nThe rests of the optional inputs of the function (load_trajectories, save_trajectories, N_save and t_save) refer to the saving/loading of the trajectories used to generate the datasets, in numpy format. See Section 2 for more details on this."
  },
  {
    "objectID": "tutorials/challenge_one_datasets.html#creating-the-andi-2020-challenge-dataset",
    "href": "tutorials/challenge_one_datasets.html#creating-the-andi-2020-challenge-dataset",
    "title": "Examples of use of ANDI",
    "section": "Creating the ANDI 2020 challenge dataset",
    "text": "Creating the ANDI 2020 challenge dataset\nBefore starting, make sure that you know the details of the competition. You can learn about every in the associated webpage. A quick overview: there are three tasks: 1) inference of the anomalous diffusion exponent, 2) classification of the diffusion model and 3) segmentation of trajectories. For each task, there are three subtasks, one per dimension. To create a dataset for every task and dimension, each e.g. of \\(N=10\\) trajectories, you just need to run\n\nfrom andi_datasets.datasets_theory import datasets_theory\n\n\nAD = datasets_theory()\nX1, Y1, X2, Y2, X3, Y3 = AD.challenge_2020_dataset(N = 10)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\nGenerating dataset for dimension 1.\n\n\nC:\\Users\\Gorka\\anaconda3\\lib\\site-packages\\fbm\\fbm.py:171: UserWarning: Combination of increments n and Hurst value H invalid for Davies-Harte method. Reverting to Hosking method. Occurs when n is small and Hurst is close to 1. \n  warnings.warn(\n\n\nGenerating dataset for dimension 2.\nGenerating dataset for dimension 3.\n\n\nX1, X2 and X3 correspond to the trajectories of Task 1, 2 and 3, respectively, while Y1, Y2 and Y_3 correspond to the labels. Each of these list contains three list, each one containing the trajectories for each dimension, in ascending order. For example, X2[2] contains the trajectories for Task 2 for dimension 3. If you only want to generate trajectories for a specific Task and dimensions, you can specify it as\n\nX1, Y1, X2, Y2, X3, Y3 = AD.challenge_2020_dataset(N = 10, tasks = 1, dimensions = 3)\n\nCreating a dataset for task(s) 1 and dimension(s) 3.\nGenerating dataset for dimension 3.\n\n\nIn this case all the lists but the ones specified will be empty. For example:\n\nprint('Task 3, dimension 1 is empty: len(X3[0]) = '+ str(len(X3[0])))\nprint('But Task 1, dimension 3 has our 10 trajectories: len(X1[2]) = '+str(len(X1[2])))\n\nTask 3, dimension 1 is empty: len(X3[0]) = 0\nBut Task 1, dimension 3 has our 10 trajectories: len(X1[2]) = 10\n\n\nTo avoiding having to create a dataset every time we want to use it, you can use the option save_dataset, which will save a file for every dataset of trajectories (named task1.txt, task2.txt and task3.txt) and their corresponding labels (named ref1.txt, ref2.txt and ref3.txt). The first column of each file corresponds to the dimension of the trajectory.\n\nX1, Y1, X2, Y2, X3, Y3 = AD.challenge_2020_dataset(N = 10, save_dataset = True)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\nGenerating dataset for dimension 1.\n\n\nC:\\Users\\Gorka\\anaconda3\\lib\\site-packages\\fbm\\fbm.py:171: UserWarning: Combination of increments n and Hurst value H invalid for Davies-Harte method. Reverting to Hosking method. Occurs when n is small and Hurst is close to 1. \n  warnings.warn(\n\n\nGenerating dataset for dimension 2.\nGenerating dataset for dimension 3.\n\n\nNow that we saved the dataset, we can load it with the option load_dataset:\n\nX1, Y1, X2, Y2, X3, Y3 = AD.challenge_2020_dataset(N = 10, load_dataset = True)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\n\n\nTake into account that if you saved \\(N=10\\) trajectories, you will only be able to load this 10, even if you ask for \\(N=20\\).\nBy default, the trajectories in this dataset will have a maximum length of 1000 steps and a minimum length of 10. To change that, you can use the options max_T and min_T. Take into account that you will need trajectories of 200 steps for task 3. Shorter trajectories will raise an error.\n\nX1, Y1, X2, Y2, X3, Y3 = AD.challenge_2020_dataset(N = 10, min_T = 15, max_T = 200)\n\nCreating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\nGenerating dataset for dimension 1.\nGenerating dataset for dimension 2.\nGenerating dataset for dimension 3.\n\n\nThe rests of the optional inputs of the function (load_trajectories, save_trajectories, N_save and t_save) refer to the saving/loading of the trajectories used to generate the datasets, in numpy format. See Section 2 for more details on this."
  },
  {
    "objectID": "tutorials/challenge_one_datasets.html#creating-trajectories-from-theoretical-models",
    "href": "tutorials/challenge_one_datasets.html#creating-trajectories-from-theoretical-models",
    "title": "Theory models and AnDi 2020",
    "section": "Creating trajectories from theoretical models",
    "text": "Creating trajectories from theoretical models\nFor this, we will the class andi_datasets.datasets_theory class. First, lets import some auxiliary packages:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nFirst, let’s define the class. One thing we can do is to access the available diffusion models:\n\nfrom andi_datasets.datasets_theory import datasets_theory\n\nAD = datasets_theory()\nAD.avail_models_name\n\n['attm', 'ctrw', 'fbm', 'lw', 'sbm']\n\n\nThe label asigned to each model is given by its position in the previous list. Now, we will generate a dataset containing \\(N=2\\) trajectories per model and exponent. We will consider anomalous exponents \\(\\alpha = [0.7, 0.9]\\) and lenghts \\(T=5\\) from the models attm and fbm.\n\ndataset = AD.create_dataset(T = 4, N_models = 2, exponents = [0.7, 0.9], models = [0, 2])\n\nprint(np.round(dataset,2))\n\n[[ 0.    0.7   0.    0.4  -0.15  0.04]\n [ 0.    0.7   0.   -0.49 -1.02 -0.87]\n [ 0.    0.9   0.    0.04  0.85  0.64]\n [ 0.    0.9   0.    1.26  2.33  2.38]\n [ 2.    0.7   0.    0.08  2.09  1.03]\n [ 2.    0.7   0.    0.18  0.61  1.12]\n [ 2.    0.9   0.    0.12  1.12  0.33]\n [ 2.    0.9   0.    0.08 -0.59 -0.45]]\n\n\nEach row corresponds to a trajectory. The first column is the model label, while the second column labels the exponent. The rest of the row is the trajectory. The default dimension for the trajectories is one. We will see later how to create trajectories with more dimensions.\nTo create datasets of trajectories with higher dimensions, you can use the input dimensions. The trajectories will be outputed as a single row of the dataset matrix. This means that if you create a dataset of trajectories of length \\(T = 10\\) and dimensions 2, the number of columns of the dataset will be 2 (the labels) + \\(2\\cdot 10\\) (the trajectory). Here is an example:\n\ndataset = AD.create_dataset(T = 10, N_models = 1, exponents = [0.7], models = [2], dimension = 2)\nprint(np.round(dataset[0], 2))\n\n[ 2.    0.7   0.   -1.14 -1.17 -1.36 -1.62 -0.88 -0.85 -1.05 -1.23 -0.57\n  0.   -0.37 -0.33 -0.71 -0.25  0.39  0.7   0.05 -0.18 -0.24]\n\n\n\nplt.plot(dataset[0,2:12], dataset[0, 12:], '-')\nplt.scatter(dataset[0,2:12], dataset[0,12:], c=np.arange(10), s = 250)\nplt.colorbar().set_label('Time'); plt.xlabel('X'); plt.ylabel('Y');\n\n\n\n\n\nSaving and loading datasets\ndatasets_theory allows also to save and load datasets, avoiding the need of generate new trajectories each time we want to create a dataset. There exist two optional inputs to the function create_datasets: save_trajectories and load_trajectories, which if True, save and load datasets, respectively. A dataset for each exponent and model considered are saved in a .h5py file whose name is the model considered. Each file contains datasets for each exponent.\nIn these cases, two important variables take also into play, t_save and N_save. These set the lenght and number of trajectories to save for each exponent and model. They are set as default to t_save\\(=10^3\\) and N_save\\(=10^4\\) as this allows to create any other combinantion of dataset. See that in the ANDI challenge we will never consider trajectories longer than \\(T=10^3\\).\nThe name of the datasets inside the .h5py file is then (exponent)_(t_save)_(N_save).\nUsing the default values for this two variables, let’s save datasets for the previous examples:\n\ndataset = AD.create_dataset(T = 4, N_models = 2, exponents = [0.7, 0.9], models = [0, 2], \n                              save_trajectories = True, path = 'datasets/')\n\nprint(np.round(dataset,2))\n\n100%|█████████████████████████████████████████| 1000/1000 [00:02<00:00, 365.01it/s, exponent=0.7, model=attm, saving=1]\n100%|█████████████████████████████████████████| 1000/1000 [00:06<00:00, 156.58it/s, exponent=0.9, model=attm, saving=1]\n100%|█████████████████████████████████████████| 1000/1000 [00:00<00:00, 2349.36it/s, exponent=0.7, model=fbm, saving=1]\n100%|█████████████████████████████████████████| 1000/1000 [00:00<00:00, 2394.90it/s, exponent=0.9, model=fbm, saving=1]\n\n\n[[ 0.    0.7   0.   -0.16 -1.69 -0.53]\n [ 0.    0.7   0.   -0.45 -0.9  -0.39]\n [ 0.    0.9   0.    0.08  0.69  0.15]\n [ 0.    0.9   0.   -1.65  0.29  2.47]\n [ 2.    0.7   0.   -0.02  0.11  0.15]\n [ 2.    0.7   0.    0.01  0.02  0.05]\n [ 2.    0.9   0.    0.   -0.04 -0.06]\n [ 2.    0.9   0.   -0.03  0.05  0.03]]\n\n\n\n\n\nIf the datasets were already saved, we can load them instead:\n\ndataset = AD.create_dataset(T = 4, N_models = 2, exponents = [0.7, 0.9], models = [0, 2], \n                              load_trajectories = True, path = 'datasets/')\n\nprint(np.round(dataset,2))\n\n[[ 0.    0.7   0.   -0.16 -1.69 -0.53]\n [ 0.    0.7   0.   -0.45 -0.9  -0.39]\n [ 0.    0.9   0.    0.08  0.69  0.15]\n [ 0.    0.9   0.   -1.65  0.29  2.47]\n [ 2.    0.7   0.   -0.02  0.11  0.15]\n [ 2.    0.7   0.    0.01  0.02  0.05]\n [ 2.    0.9   0.    0.   -0.04 -0.06]\n [ 2.    0.9   0.   -0.03  0.05  0.03]]\n\n\n\n\nCreating noisy datasets\nThere are two ways of adding noise to the trajectories: either by varying their diffusion coefficient or by adding noise to each of the trahectory’s points. You can do it with the functions create_noisy_diffusion_dataset or create_noisy_localization_dataset, respectively. These functions allow to either create a new dataset to which we will add the noise or give as input the dataset. We will use now the dataset we created in previous cells.\nNoisy diffusion coefficients The function create_noisy_diffusion_dataset changes the diffusion coefficient of a given trajectory. This function works similarly to create_dataset, but has as optional input diffusion_coefficients. You can use this variable to change the diffusion coefficient of the input trajectories at your please. If you don’t give an input to it, they will be drawn randomly from a Gaussian distribution with variance 1. See that, as \\(D\\) may be negative, the trajectories can be reversed.\n\ndataset_noise_D = AD.create_noisy_diffusion_dataset(dataset.copy(), T = 4)\n\n\nplt.plot(dataset[1,2:],'o-', label = 'Original trajectory')\nplt.plot(dataset_noise_D[1,2:],'o-', label = r'Trajectory with changed $D$')\nplt.xlabel('Time'); plt.ylabel('Position')\nplt.legend();\n\n\n\n\nLocalization noise The class also allows to create noisy trajectories via create_noisy_localization_dataset. This functions works as create_dataset but has three extra optional input parameters: noise_func, sigma and mu. If noise_func is False, the function generates a dataset with Gaussian noise with variance sigma and mean mu. Keep in mind that the function will never save noisy trajectories. The saved/loaded trajectories will never have noise, which will be added a posteriori. Here is an example with Gaussian noise:\n\n# The default value of noise_func is False, so we don't need to input it. \n# Default value of sigma is 1 and mu is 0.\ndataset_noisy = AD.create_noisy_localization_dataset(dataset.copy(), T = 4)\n\nprint(np.round(dataset_noisy,2))\n\n[[ 0.    0.7   0.46 -1.3  -1.02 -0.32]\n [ 0.    0.7   0.34  0.5  -0.23 -1.24]\n [ 0.    0.9   0.04 -0.48 -0.9   0.79]\n [ 0.    0.9  -0.4  -3.01  0.58  0.02]\n [ 2.    0.7   0.81  0.23  1.45 -1.03]\n [ 2.    0.7  -0.15  0.43  1.02  0.98]\n [ 2.    0.9  -0.09 -0.25 -0.39 -0.57]\n [ 2.    0.9  -0.12 -0.36  0.68 -0.74]]\n\n\n\nplt.plot(dataset[1,2:],'o-', label = 'Original trajectory')\nplt.plot(dataset_noisy[1,2:],'o-', label = 'Trajectory + Gaussian noise')\nplt.legend();\n\n\n\n\nYou can also define your own noise function. The noise created by this functions will be added to each of the trajectories of the dataset. The ouput must be a matrix of size given as input:\n\n# Uniformly distributed noise\ndef uniform_noise(N, M):\n    return np.random.rand(N, M)*(2*np.random.randint(2, size = (N, M))-1) \n\ndataset_uniform = AD.create_noisy_localization_dataset(dataset.copy(), T = 4, \n                                                       noise_func=uniform_noise)\n\nprint(np.round(dataset_uniform,2))\n\n[[ 0.    0.7  -0.02 -0.47 -1.31 -1.44]\n [ 0.    0.7   0.13 -0.34 -1.54 -1.1 ]\n [ 0.    0.9   0.13 -0.14  0.43  0.1 ]\n [ 0.    0.9  -0.37 -1.06  0.64  3.03]\n [ 2.    0.7   0.12 -0.79  1.02  0.59]\n [ 2.    0.7   0.71 -0.51 -0.94 -0.65]\n [ 2.    0.9  -0.78  0.47  0.46 -0.99]\n [ 2.    0.9  -0.25 -0.83 -0.7  -0.53]]\n\n\n\nplt.plot(dataset[1,2:],'o-', label = 'Original trajectory')\nplt.plot(dataset_noisy[1,2:],'o-', label = 'Trajectory + Gaussian noise')\nplt.plot(dataset_uniform[1,2:],'o-', label = 'Trajectory + uniform noise')\nplt.legend();\n\n\n\n\n\n\nCreating segmented datasets\nThe class datasets_theory also allows to create trajectories which are made of two different diffusion models or anomalous exponent, at a given changing point. Such function has the name create_segmented_dataset. Following the ANDI challenge guidelines, the output trajectories will have length 200. However, you can choose an arbitrary length with the variable final_length. The input datasets must have at least the same size as final_length. There is also the option to randomly shuffle the input datasets from which the segmented output dataset will be generate, throught the variable random_shuffle.\n\n# First we define two datasets. Let's take same anomalous exponent but various models\ndataset1 = AD.create_dataset(T = 200, N_models = 2, exponents = [0.7], models = [0, 1, 2, 4])\ndataset2 = AD.create_dataset(T = 200, N_models = 2, exponents = [0.7], models = [0, 1, 2, 4])\n# Now we give them to the desired function\nseg = AD.create_segmented_dataset(dataset1, dataset2)\n\nThe first 5 elements of every row of the variable seg label the trajectory. First point is the changing time, second and third, and fourth and fifthe are the labels of the model and exponent of the input trajectories, respectevely.\n\nseg[:, :5]\n\narray([[145. ,   0. ,   0.7,   0. ,   0.7],\n       [ 91. ,   0. ,   0.7,   0. ,   0.7],\n       [ 16. ,   1. ,   0.7,   1. ,   0.7],\n       [ 88. ,   1. ,   0.7,   1. ,   0.7],\n       [100. ,   2. ,   0.7,   2. ,   0.7],\n       [ 82. ,   2. ,   0.7,   2. ,   0.7],\n       [175. ,   4. ,   0.7,   4. ,   0.7],\n       [189. ,   4. ,   0.7,   4. ,   0.7]])\n\n\nAs we did not shuffle data and dataset1 and dataset2 have the trajectories ordered by model, there are no mixes between different models. This can be done thanks to the random_shuffle variable:\n\nseg = AD.create_segmented_dataset(dataset1, dataset2, random_shuffle = True)\nseg[:, :5]\n\narray([[ 33. ,   0. ,   0.7,   2. ,   0.7],\n       [ 86. ,   2. ,   0.7,   1. ,   0.7],\n       [ 74. ,   0. ,   0.7,   0. ,   0.7],\n       [132. ,   1. ,   0.7,   2. ,   0.7],\n       [140. ,   1. ,   0.7,   4. ,   0.7],\n       [ 75. ,   4. ,   0.7,   0. ,   0.7],\n       [ 12. ,   2. ,   0.7,   4. ,   0.7],\n       [163. ,   4. ,   0.7,   1. ,   0.7]])\n\n\n\nplt.plot(seg[0,5:], label = 'Model 1 = '+AD.avail_models_name[int(seg[0, 1])]+', Model 2 = '+AD.avail_models_name[int(seg[0, 3])])\nplt.axvline(seg[0,0], c = 'C1', ls = '--', label = 'Changing point')\nplt.legend();"
  },
  {
    "objectID": "tutorials/challenge_one_datasets.html#accesing-the-diffusion-models-from-models_theory",
    "href": "tutorials/challenge_one_datasets.html#accesing-the-diffusion-models-from-models_theory",
    "title": "Theory models and AnDi 2020",
    "section": "Accesing the diffusion models from models_theory",
    "text": "Accesing the diffusion models from models_theory\nAll the theoretical diffusion models available in andi are collected in the class models_theory(). We can access them with a simple import :\n\nfrom andi_datasets.models_theory import models_theory\n# We will also need the following libraries\nimport inspect\n\nThere exist three subclasses, one for each dimensions. To see the available models for each dimension, you can use the following:\n\nfor dimensions in range(3):\n    if dimensions == 0:\n        models = models_theory()._oneD()\n    elif dimensions == 1:\n        models = models_theory()._twoD()\n    elif dimensions == 2:\n        models = models_theory()._threeD()\n        \n    available_models = inspect.getmembers(models, inspect.ismethod)\n        \n    print('\\nThe availailabe models for dimension '+str(dimensions+1)+' are:')\n    [print('- '+x[0]) for x in available_models]\n\n\nThe availailabe models for dimension 1 are:\n- attm\n- ctrw\n- fbm\n- lw\n- sbm\n\nThe availailabe models for dimension 2 are:\n- attm\n- ctrw\n- fbm\n- lw\n- sbm\n\nThe availailabe models for dimension 3 are:\n- attm\n- ctrw\n- fbm\n- lw\n- sbm\n\n\nOne dimensional trajectories\nFor this example, we will create a trajectory of a 1D continuous time random walk (ctrw). All the model functions must have as inputs the length of the trajectories and the anomalous exponent \\(\\alpha\\):\n\noneD = models_theory()._oneD()\n\ntraj = oneD.ctrw(T = 100, alpha = 0.8)\nplt.plot(traj)\n\n\n\n\nSome diffusion models are constructed from a collection of positions at different sampling times. This means that their output array does not have \\(T\\) points, but rather number \\(m\\) of sampled posisition. In order to acces such kind of information, some model generators have the optional parameter regulare_time, which if False, makes the function to return an array with two rows and \\(m\\) columns. The first row are the sampling times and the second are the positions. CTRW is an example of these models:\n\ntraj_nonR = oneD.ctrw(T = 100, alpha = 0.8, regular_time = False)\ntraj_nonR.shape\n\n(2, 13)\n\n\nIf regulare_time = True (default value), the trajectory is fed to the function regularize from utils_trajectories, which transforms the trajectory to regular sampling times. Below is an example with the previously defined trajectory. The offset is due to the need of rounding the times coming from the variable traj_nonR[1].\n\nfrom andi_datasets.utils_trajectories import regularize\n\ntraj_R = regularize(positions = traj_nonR[1,:], times = traj_nonR[0, :], T = 100)\n\nfig, ax = plt.subplots()\nax.scatter(traj_nonR[0,:], traj_nonR[1, :], c = 'C0', label = 'Irregular sampling')\nax.plot(traj_R, c = 'C1', label = 'Regularized sampling')\nax.set_xlabel('Positions'); ax.set_ylabel('Time'); ax.axvline(100,ls = '--', c = 'C2')\nax.legend();\n\n\n\n\nTwo dimensional trajectories\nWe will proceed with an example of a 2D trajectory coming from the ATTM model. 2D trajectories are output as an array of length \\(2\\times T\\).\n\nT = 20\ntraj = models_theory()._twoD().attm(T = T, alpha = 0.8)\nprint(traj.shape)\n\n(40,)\n\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize = (6,5))\nax = fig.gca(projection='3d')\nax.plot(traj[:T], traj[T:], np.arange(T))\nscat = ax.scatter3D(traj[:T], traj[T:], np.arange(T), c=np.arange(T), s = 250);\nax.set_xlabel('Position X')\nax.set_ylabel('Position Y')\nax.set_zlabel('Time')\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\nplt.tight_layout();\n\n\n\n\nEnsemble averages of diffusion_models trajectories\nIn the ANDI challenge, one of the parameters to be extracted is the anomalous exponent \\(\\alpha\\). This exponents corresponds to the proportionality relation between the mean squared displacement averaged over \\(N\\) trajectories (eMSD) and time: \\(\\left< x^2(t) \\right>\\sim t^\\alpha\\). Here are a series of examples of eMSD calculated for various models:\n(If your viewer does not allow you to see the following images, you can see them in the folder figures/)\nSubdiffusive trajectories:\n\nSuperdiffusive trajectories:"
  },
  {
    "objectID": "tutorials/challenge_one_submission.html",
    "href": "tutorials/challenge_one_submission.html",
    "title": "ANDI 2020 submission: from trajectories to predictions",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nThe best way of dealing with the available datasets is by means of the andi-datasets python package, which can be installed using pip install andi-datasets. Then, the dataset can be created from the function challenge_2020_dataset, allocated in the datasets_theory module.\nFor this tutorial, I have first downloaded the datasets available in the Challenge webpage (www.andi-challenge.org). Note that you need to register before being able to access the data. You will have access to two datasets, one for training, which is labeled, and one for scoring, which is used to rank the participants in the competition. In this case, I downloaded each of the datasets in the following folders:\nTo load the training dataset, we only need to do:\nIn the case you are working with a validation dataset, and do not have the labels (in this case stored in the files refX.txt), you can use the optional parameter load_labels = False, as follows:"
  },
  {
    "objectID": "tutorials/challenge_one_submission.html#the-good-old-way-the-tmsd-fitting",
    "href": "tutorials/challenge_one_submission.html#the-good-old-way-the-tmsd-fitting",
    "title": "ANDI 2020 submission: from trajectories to predictions",
    "section": "The ‘good old’ way: the tMSD fitting",
    "text": "The ‘good old’ way: the tMSD fitting\nOne way to extract the anomalous exponent is by fitting the tMSD: \\[\n\\mbox{tMSD}(\\Delta) = \\frac{1}{T-\\Delta} \\sum_{i=1}^{T-\\Delta}(x(t_i + \\Delta)-x(t_i))^2,\n\\] where \\(\\Delta\\) is defined as the time lag and \\(T\\) is length of the trajectory.\n\ndef TMSD(traj, t_lags):\n    ttt = np.zeros_like(t_lags, dtype= float)\n    for idx, t in enumerate(t_lags): \n        for p in range(len(traj)-t):\n            ttt[idx] += (traj[p]-traj[p+t])**2            \n        ttt[idx] /= len(traj)-t    \n    return ttt\n\nWe know that (usually) \\[\\mbox{tMSD}(\\Delta) \\sim \\Delta ^ \\alpha,\\] hence we can use it to extract the anomalous exponent. Let us check this on trajectories from two models: ATTM and FBM. For that we can again use the andi package, and access the diffusion models directly:\n\nfrom andi.models_theory import models_theory\nMT = models_theory()\n\n# We create one ATTM and one FBM trajectory with alpha = 0.2\nattm = MT._oneD().attm(T = 1000, alpha = 0.2)\nfbm = MT._oneD().fbm(T = 1000, alpha = 0.2)\n\n# We calculate their tMSD\nt_lags = np.arange(2, 20)\nattm_tmsd = TMSD(attm, t_lags = t_lags)\nfbm_tmsd = TMSD(fbm, t_lags = t_lags)\n\nLet’s plot the tMSD:\n\nfig, ax = plt.subplots(1,2, figsize = (10, 4))\n\nax[0].loglog(t_lags, fbm_tmsd, '-o', lw = 1)\nax[0].loglog(t_lags, t_lags**0.2/(t_lags[0]**0.2)*fbm_tmsd[0], ls = '--')\nax[0].loglog(t_lags, t_lags/(t_lags[0])*fbm_tmsd[0], ls = '--')\nax[0].set_title(r'FBM $\\rightarrow$ Ergodic process')\n\nax[1].loglog(t_lags, attm_tmsd, '-o', lw = 1,label = 'tMSD')\nax[1].loglog(t_lags, t_lags**0.2/(t_lags[0]**0.2)*attm_tmsd[0], ls = '--', label = r'$\\sim \\Delta^{0.2}$')\nax[1].loglog(t_lags, t_lags/(t_lags[0])*attm_tmsd[0], ls = '--', label = r'$\\sim \\Delta$')\nax[1].set_title(r'ATTM $\\rightarrow$ Non-ergodic process')\nax[1].legend(fontsize = 16)\n\nplt.setp(ax, xlabel = r'$\\Delta$', ylabel = 'tMSD');\nfig.tight_layout()\n\n\n\n\nWe see that the tMSD works very well for ergodic processes, but fails horribly for non-ergodic, for which we usually have that \\(tMSD\\sim\\Delta\\). Nevertheless, let’s use it to fit the exponent of the 1D training dataset:\n\nt_lags = np.arange(2,10)\npredictions = []\n\nfor traj in X1[0]:\n    tmsd = TMSD(traj, t_lags)\n    predictions.append(np.polyfit(np.log(t_lags), np.log(tmsd),1)[0])\n    \nprint('MAE = '+str(np.round(np.mean(np.abs(np.array(predictions)-Y1[0])), 4)))\n\nMAE = 0.3342\n\n\nLet’s see how is the error distributed:\n\nplt.hist(np.array(predictions)-Y1[0], bins = 50);\nplt.xlabel('Error')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\nWe can now use the same method to predict the exponent of the validation dataset V1, for 1D\n\nt_lags = np.arange(1,10)\npredictions_task1_1d = []\n\nfor traj in validation[0][0]:\n    tmsd = TMSD(traj, t_lags)\n    predictions_task1_1d.append(np.polyfit(np.log(t_lags), np.log(tmsd),1)[0])\n\nTo make a submission, you only need to write a .txt file for which: - The name is the task: task1.txt, task2.txt, task3.txt - The first column is the dimension (1,2 or 3) - The following columns are the results - Delimiter should be ;\n\npred_to_txt = np.ones((len(predictions_task1_1d), 2))\npred_to_txt[:, 1] = predictions_task1_1d\n\nnp.savetxt('task1.txt', pred_to_txt.astype(float), fmt = '%1.5f', delimiter = ';')\n\n\nThen, we zip it and submit!"
  },
  {
    "objectID": "tutorials/challenge_one_submission.html#the-new-trend-machine-learning",
    "href": "tutorials/challenge_one_submission.html#the-new-trend-machine-learning",
    "title": "ANDI 2020 submission: from trajectories to predictions",
    "section": "The new trend: machine learning",
    "text": "The new trend: machine learning\nThere are various approaches to model classification: statistical tests to differentiate between CTRW and FBM, Bayesian inference,…etc. In this example we will use the latest proposal: Machine Learning.\nOne of the main difficulties of the ANDI challenge is that we have trajectories of all lengths! Having ML models able to accomodate such feature is one of the main challenges the participants will face.\nFor the sake of simplicity, I will solve here an easier problem: classifying between the subdiffusive models (ATTM, FBM, CTRW, SBM), with exponents \\(\\in \\ [0.1, 1]\\), with trajectories of all \\(30\\) points. To generate such dataset, I can use another function from the andi-datasets package: create_dataset from the datasets_theory class. You can check all the details of this function in this tutorial notebook.\n\nfrom andi.datasets_theory import datasets_theory\nDT = datasets_theory()\n# Here I load a dataset that I have already generated. To create a new one, you just new to put load_trajectories = False\n# Check the tutorials in the github for all the details\ndataset = DT.create_dataset(T = 30, N_models = 1000, exponents = np.arange(0.1,1,0.05), models = [0,1,2,4], \n                              load_trajectories = True, path = '/home/gmunoz/andi_data/datasets/')\n\nAs usually done in Machine Learning, we shuffle and create trainina/test set with 80-20% ratios:\n\nnp.random.shuffle(dataset)\n\nratio = int(0.8*dataset.shape[0])\n# We normalize the trajectories so all of them are in the same 'scale'\nfrom andi.utils_trajectories import normalize\nX_a = andi.normalize(dataset[:ratio, 2:]).reshape(ratio, T, 1)\nX_e = andi.normalize(dataset[ratio:, 2:]).reshape(N-ratio, T, 1)\n\ndataset[dataset[:,0] == 4, 0] = 3\nY_a = to_categorical(dataset[:ratio, 0])\nY_e = to_categorical(dataset[ratio:, 0])\n\n(14400, 30, 1)\n\n\nWe import the necessary packages for creating our neural network:\n\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Conv1D, Dropout, BatchNormalization, Flatten\n\nfrom keras.regularizers import l2 as regularizer_l2\n\nfrom keras.optimizers import Adam\n\nNow let’s create a typical Convolutional neural network with keras, with some L2 regularizers and Dropout and Batch Normalization layers.\n\nmodel = Sequential()\n\n# Here we define the architecture of the Neural Network\nmodel.add(Conv1D(filters=3, kernel_size=3 ,strides=1,   \n                 input_shape=(T, 1),\n                 kernel_initializer= 'uniform',      \n                 activation= 'relu', kernel_regularizer = regularizer_l2(l = 0.001)))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\nmodel.add(Conv1D(filters=8, kernel_size=5 ,strides=1,  \n                 kernel_initializer= 'uniform',      \n                 activation= 'relu', kernel_regularizer = regularizer_l2(l = 0.001)))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))   \nmodel.add(Conv1D(filters=3, kernel_size=2 ,strides=1,  \n                 kernel_initializer= 'uniform',      \n                 activation= 'relu', kernel_regularizer = regularizer_l2(l = 0.001)))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64*2, activation='sigmoid', kernel_regularizer = regularizer_l2(l = 0.001)))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='sigmoid'))\nmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))\n\n# Last layer needs to have same size as number of processes\nnumber_process = 4\n\nmodel.add(Dense(number_process, activation='softmax'))\n\n# We add loss function + Adam optimizer\nmodel.compile(loss='binary_crossentropy',\n              optimizer=Adam(),\n              metrics=['accuracy'])\n\nLet’s train the model:\n\nbatch_size = 200\nepochs = 150\n\nhistory = model.fit(X_a, Y_a,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=2,\n                    validation_data=(X_e, Y_e))\n\nmodel.save('model_classification_subdiffusive.h5')\n\nTrain on 57600 samples, validate on 14400 samples\nEpoch 1/150\n - 5s - loss: 0.5244 - accuracy: 0.7870 - val_loss: 0.6578 - val_accuracy: 0.7500\nEpoch 2/150\n - 4s - loss: 0.4146 - accuracy: 0.8187 - val_loss: 0.6470 - val_accuracy: 0.7402\nEpoch 3/150\n - 3s - loss: 0.3785 - accuracy: 0.8263 - val_loss: 0.4752 - val_accuracy: 0.7945\nEpoch 4/150\n - 4s - loss: 0.3654 - accuracy: 0.8290 - val_loss: 0.4057 - val_accuracy: 0.8157\nEpoch 5/150\n - 4s - loss: 0.3511 - accuracy: 0.8332 - val_loss: 0.3586 - val_accuracy: 0.8340\nEpoch 6/150\n - 3s - loss: 0.3471 - accuracy: 0.8351 - val_loss: 0.3385 - val_accuracy: 0.8392\nEpoch 7/150\n - 4s - loss: 0.3395 - accuracy: 0.8380 - val_loss: 0.3169 - val_accuracy: 0.8479\nEpoch 8/150\n - 4s - loss: 0.3346 - accuracy: 0.8406 - val_loss: 0.3121 - val_accuracy: 0.8524\nEpoch 9/150\n - 3s - loss: 0.3311 - accuracy: 0.8421 - val_loss: 0.3325 - val_accuracy: 0.8401\nEpoch 10/150\n - 4s - loss: 0.3319 - accuracy: 0.8413 - val_loss: 0.3093 - val_accuracy: 0.8509\nEpoch 11/150\n - 4s - loss: 0.3314 - accuracy: 0.8418 - val_loss: 0.3074 - val_accuracy: 0.8524\nEpoch 12/150\n - 3s - loss: 0.3286 - accuracy: 0.8427 - val_loss: 0.3098 - val_accuracy: 0.8501\nEpoch 13/150\n - 2s - loss: 0.3309 - accuracy: 0.8420 - val_loss: 0.3088 - val_accuracy: 0.8518\nEpoch 14/150\n - 2s - loss: 0.3356 - accuracy: 0.8399 - val_loss: 0.3089 - val_accuracy: 0.8519\nEpoch 15/150\n - 4s - loss: 0.3245 - accuracy: 0.8447 - val_loss: 0.3044 - val_accuracy: 0.8540\nEpoch 16/150\n - 3s - loss: 0.3288 - accuracy: 0.8427 - val_loss: 0.3072 - val_accuracy: 0.8522\nEpoch 17/150\n - 4s - loss: 0.3319 - accuracy: 0.8416 - val_loss: 0.3087 - val_accuracy: 0.8508\nEpoch 18/150\n - 4s - loss: 0.3237 - accuracy: 0.8450 - val_loss: 0.3048 - val_accuracy: 0.8534\nEpoch 19/150\n - 4s - loss: 0.3255 - accuracy: 0.8445 - val_loss: 0.3117 - val_accuracy: 0.8490\nEpoch 20/150\n - 4s - loss: 0.3257 - accuracy: 0.8443 - val_loss: 0.3086 - val_accuracy: 0.8512\nEpoch 21/150\n - 3s - loss: 0.3266 - accuracy: 0.8440 - val_loss: 0.3075 - val_accuracy: 0.8525\nEpoch 22/150\n - 4s - loss: 0.3264 - accuracy: 0.8438 - val_loss: 0.3026 - val_accuracy: 0.8560\nEpoch 23/150\n - 4s - loss: 0.3234 - accuracy: 0.8460 - val_loss: 0.3101 - val_accuracy: 0.8514\nEpoch 24/150\n - 4s - loss: 0.3223 - accuracy: 0.8466 - val_loss: 0.3036 - val_accuracy: 0.8531\nEpoch 25/150\n - 4s - loss: 0.3303 - accuracy: 0.8427 - val_loss: 0.3171 - val_accuracy: 0.8473\nEpoch 26/150\n - 4s - loss: 0.3215 - accuracy: 0.8468 - val_loss: 0.3066 - val_accuracy: 0.8532\nEpoch 27/150\n - 3s - loss: 0.3219 - accuracy: 0.8457 - val_loss: 0.3066 - val_accuracy: 0.8514\nEpoch 28/150\n - 4s - loss: 0.3239 - accuracy: 0.8452 - val_loss: 0.3014 - val_accuracy: 0.8575\nEpoch 29/150\n - 4s - loss: 0.3219 - accuracy: 0.8470 - val_loss: 0.3125 - val_accuracy: 0.8499\nEpoch 30/150\n - 3s - loss: 0.3225 - accuracy: 0.8461 - val_loss: 0.3702 - val_accuracy: 0.8294\nEpoch 31/150\n - 2s - loss: 0.3262 - accuracy: 0.8450 - val_loss: 0.3037 - val_accuracy: 0.8542\nEpoch 32/150\n - 4s - loss: 0.3222 - accuracy: 0.8465 - val_loss: 0.3134 - val_accuracy: 0.8510\nEpoch 33/150\n - 3s - loss: 0.3216 - accuracy: 0.8467 - val_loss: 0.3010 - val_accuracy: 0.8572\nEpoch 34/150\n - 4s - loss: 0.3206 - accuracy: 0.8478 - val_loss: 0.3023 - val_accuracy: 0.8550\nEpoch 35/150\n - 4s - loss: 0.3196 - accuracy: 0.8477 - val_loss: 0.2991 - val_accuracy: 0.8566\nEpoch 36/150\n - 3s - loss: 0.3197 - accuracy: 0.8477 - val_loss: 0.3001 - val_accuracy: 0.8576\nEpoch 37/150\n - 4s - loss: 0.3201 - accuracy: 0.8470 - val_loss: 0.3039 - val_accuracy: 0.8560\nEpoch 38/150\n - 4s - loss: 0.3187 - accuracy: 0.8483 - val_loss: 0.3007 - val_accuracy: 0.8572\nEpoch 39/150\n - 4s - loss: 0.3200 - accuracy: 0.8475 - val_loss: 0.3024 - val_accuracy: 0.8569\nEpoch 40/150\n - 4s - loss: 0.3199 - accuracy: 0.8477 - val_loss: 0.3166 - val_accuracy: 0.8492\nEpoch 41/150\n - 3s - loss: 0.3218 - accuracy: 0.8473 - val_loss: 0.3144 - val_accuracy: 0.8480\nEpoch 42/150\n - 4s - loss: 0.3185 - accuracy: 0.8486 - val_loss: 0.3139 - val_accuracy: 0.8473\nEpoch 43/150\n - 4s - loss: 0.3186 - accuracy: 0.8481 - val_loss: 0.2953 - val_accuracy: 0.8595\nEpoch 44/150\n - 3s - loss: 0.3180 - accuracy: 0.8488 - val_loss: 0.2994 - val_accuracy: 0.8583\nEpoch 45/150\n - 4s - loss: 0.3182 - accuracy: 0.8488 - val_loss: 0.2968 - val_accuracy: 0.8581\nEpoch 46/150\n - 2s - loss: 0.3193 - accuracy: 0.8476 - val_loss: 0.3008 - val_accuracy: 0.8561\nEpoch 47/150\n - 2s - loss: 0.3204 - accuracy: 0.8477 - val_loss: 0.2987 - val_accuracy: 0.8558\nEpoch 48/150\n - 4s - loss: 0.3174 - accuracy: 0.8495 - val_loss: 0.2963 - val_accuracy: 0.8597\nEpoch 49/150\n - 5s - loss: 0.3170 - accuracy: 0.8487 - val_loss: 0.3264 - val_accuracy: 0.8434\nEpoch 50/150\n - 4s - loss: 0.3187 - accuracy: 0.8490 - val_loss: 0.2968 - val_accuracy: 0.8581\nEpoch 51/150\n - 4s - loss: 0.3192 - accuracy: 0.8483 - val_loss: 0.2976 - val_accuracy: 0.8597\nEpoch 52/150\n - 4s - loss: 0.3177 - accuracy: 0.8491 - val_loss: 0.2996 - val_accuracy: 0.8561\nEpoch 53/150\n - 4s - loss: 0.3169 - accuracy: 0.8493 - val_loss: 0.2977 - val_accuracy: 0.8579\nEpoch 54/150\n - 4s - loss: 0.3175 - accuracy: 0.8501 - val_loss: 0.3034 - val_accuracy: 0.8580\nEpoch 55/150\n - 4s - loss: 0.3179 - accuracy: 0.8492 - val_loss: 0.2970 - val_accuracy: 0.8572\nEpoch 56/150\n - 4s - loss: 0.3175 - accuracy: 0.8496 - val_loss: 0.3085 - val_accuracy: 0.8523\nEpoch 57/150\n - 4s - loss: 0.3168 - accuracy: 0.8501 - val_loss: 0.3300 - val_accuracy: 0.8453\nEpoch 58/150\n - 4s - loss: 0.3172 - accuracy: 0.8493 - val_loss: 0.2985 - val_accuracy: 0.8569\nEpoch 59/150\n - 4s - loss: 0.3156 - accuracy: 0.8506 - val_loss: 0.3153 - val_accuracy: 0.8484\nEpoch 60/150\n - 4s - loss: 0.3189 - accuracy: 0.8485 - val_loss: 0.2975 - val_accuracy: 0.8592\nEpoch 61/150\n - 3s - loss: 0.3171 - accuracy: 0.8491 - val_loss: 0.3077 - val_accuracy: 0.8539\nEpoch 62/150\n - 2s - loss: 0.3167 - accuracy: 0.8496 - val_loss: 0.3044 - val_accuracy: 0.8539\nEpoch 63/150\n - 4s - loss: 0.3172 - accuracy: 0.8488 - val_loss: 0.2994 - val_accuracy: 0.8580\nEpoch 64/150\n - 3s - loss: 0.3143 - accuracy: 0.8507 - val_loss: 0.2979 - val_accuracy: 0.8574\nEpoch 65/150\n - 4s - loss: 0.3159 - accuracy: 0.8500 - val_loss: 0.3046 - val_accuracy: 0.8561\nEpoch 66/150\n - 4s - loss: 0.3196 - accuracy: 0.8486 - val_loss: 0.3017 - val_accuracy: 0.8551\nEpoch 67/150\n - 4s - loss: 0.3187 - accuracy: 0.8485 - val_loss: 0.3205 - val_accuracy: 0.8495\nEpoch 68/150\n - 4s - loss: 0.3154 - accuracy: 0.8506 - val_loss: 0.2971 - val_accuracy: 0.8579\nEpoch 69/150\n - 4s - loss: 0.3151 - accuracy: 0.8512 - val_loss: 0.2971 - val_accuracy: 0.8592\nEpoch 70/150\n - 3s - loss: 0.3140 - accuracy: 0.8510 - val_loss: 0.3051 - val_accuracy: 0.8550\nEpoch 71/150\n - 4s - loss: 0.3161 - accuracy: 0.8501 - val_loss: 0.2938 - val_accuracy: 0.8603\nEpoch 72/150\n - 4s - loss: 0.3155 - accuracy: 0.8509 - val_loss: 0.2961 - val_accuracy: 0.8602\nEpoch 73/150\n - 3s - loss: 0.3159 - accuracy: 0.8498 - val_loss: 0.2990 - val_accuracy: 0.8581\nEpoch 74/150\n - 4s - loss: 0.3154 - accuracy: 0.8508 - val_loss: 0.2952 - val_accuracy: 0.8597\nEpoch 75/150\n - 4s - loss: 0.3149 - accuracy: 0.8512 - val_loss: 0.2982 - val_accuracy: 0.8591\nEpoch 76/150\n - 4s - loss: 0.3155 - accuracy: 0.8505 - val_loss: 0.2949 - val_accuracy: 0.8606\nEpoch 77/150\n - 3s - loss: 0.3179 - accuracy: 0.8495 - val_loss: 0.3051 - val_accuracy: 0.8532\nEpoch 78/150\n - 3s - loss: 0.3147 - accuracy: 0.8513 - val_loss: 0.2938 - val_accuracy: 0.8601\nEpoch 79/150\n - 3s - loss: 0.3147 - accuracy: 0.8508 - val_loss: 0.2997 - val_accuracy: 0.8584\nEpoch 80/150\n - 4s - loss: 0.3169 - accuracy: 0.8498 - val_loss: 0.2966 - val_accuracy: 0.8597\nEpoch 81/150\n - 4s - loss: 0.3141 - accuracy: 0.8503 - val_loss: 0.3137 - val_accuracy: 0.8490\nEpoch 82/150\n - 4s - loss: 0.3181 - accuracy: 0.8492 - val_loss: 0.2938 - val_accuracy: 0.8609\nEpoch 83/150\n - 4s - loss: 0.3155 - accuracy: 0.8505 - val_loss: 0.2962 - val_accuracy: 0.8601\nEpoch 84/150\n - 3s - loss: 0.3173 - accuracy: 0.8490 - val_loss: 0.3018 - val_accuracy: 0.8548\nEpoch 85/150\n - 4s - loss: 0.3174 - accuracy: 0.8496 - val_loss: 0.2923 - val_accuracy: 0.8619\nEpoch 86/150\n - 4s - loss: 0.3150 - accuracy: 0.8500 - val_loss: 0.3091 - val_accuracy: 0.8556\nEpoch 87/150\n - 3s - loss: 0.3158 - accuracy: 0.8504 - val_loss: 0.3082 - val_accuracy: 0.8545\nEpoch 88/150\n - 4s - loss: 0.3151 - accuracy: 0.8511 - val_loss: 0.3004 - val_accuracy: 0.8587\nEpoch 89/150\n - 4s - loss: 0.3145 - accuracy: 0.8511 - val_loss: 0.2922 - val_accuracy: 0.8614\nEpoch 90/150\n - 3s - loss: 0.3149 - accuracy: 0.8508 - val_loss: 0.2940 - val_accuracy: 0.8599\nEpoch 91/150\n - 4s - loss: 0.3132 - accuracy: 0.8523 - val_loss: 0.3229 - val_accuracy: 0.8477\nEpoch 92/150\n - 3s - loss: 0.3151 - accuracy: 0.8505 - val_loss: 0.2908 - val_accuracy: 0.8624\nEpoch 93/150\n - 2s - loss: 0.3164 - accuracy: 0.8501 - val_loss: 0.2949 - val_accuracy: 0.8609\nEpoch 94/150\n - 2s - loss: 0.3148 - accuracy: 0.8511 - val_loss: 0.3067 - val_accuracy: 0.8551\nEpoch 95/150\n - 4s - loss: 0.3161 - accuracy: 0.8498 - val_loss: 0.3004 - val_accuracy: 0.8574\nEpoch 96/150\n - 4s - loss: 0.3167 - accuracy: 0.8503 - val_loss: 0.2928 - val_accuracy: 0.8610\nEpoch 97/150\n - 4s - loss: 0.3150 - accuracy: 0.8508 - val_loss: 0.3004 - val_accuracy: 0.8580\nEpoch 98/150\n - 4s - loss: 0.3116 - accuracy: 0.8521 - val_loss: 0.2929 - val_accuracy: 0.8602\nEpoch 99/150\n - 4s - loss: 0.3157 - accuracy: 0.8505 - val_loss: 0.3035 - val_accuracy: 0.8582\nEpoch 100/150\n - 4s - loss: 0.3158 - accuracy: 0.8506 - val_loss: 0.3158 - val_accuracy: 0.8476\nEpoch 101/150\n - 4s - loss: 0.3153 - accuracy: 0.8498 - val_loss: 0.3024 - val_accuracy: 0.8580\nEpoch 102/150\n - 3s - loss: 0.3130 - accuracy: 0.8520 - val_loss: 0.2985 - val_accuracy: 0.8602\nEpoch 103/150\n - 4s - loss: 0.3136 - accuracy: 0.8516 - val_loss: 0.2933 - val_accuracy: 0.8605\nEpoch 104/150\n - 4s - loss: 0.3134 - accuracy: 0.8509 - val_loss: 0.2964 - val_accuracy: 0.8572\nEpoch 105/150\n - 4s - loss: 0.3121 - accuracy: 0.8512 - val_loss: 0.2975 - val_accuracy: 0.8592\nEpoch 106/150\n - 4s - loss: 0.3176 - accuracy: 0.8502 - val_loss: 0.4506 - val_accuracy: 0.8130\nEpoch 107/150\n - 4s - loss: 0.3171 - accuracy: 0.8506 - val_loss: 0.2937 - val_accuracy: 0.8611\nEpoch 108/150\n - 4s - loss: 0.3136 - accuracy: 0.8519 - val_loss: 0.2912 - val_accuracy: 0.8632\nEpoch 109/150\n - 4s - loss: 0.3152 - accuracy: 0.8504 - val_loss: 0.3002 - val_accuracy: 0.8585\nEpoch 110/150\n - 2s - loss: 0.3157 - accuracy: 0.8503 - val_loss: 0.2989 - val_accuracy: 0.8569\nEpoch 111/150\n - 3s - loss: 0.3119 - accuracy: 0.8523 - val_loss: 0.2939 - val_accuracy: 0.8615\nEpoch 112/150\n - 4s - loss: 0.3102 - accuracy: 0.8533 - val_loss: 0.2886 - val_accuracy: 0.8641\nEpoch 113/150\n - 3s - loss: 0.3140 - accuracy: 0.8520 - val_loss: 0.2975 - val_accuracy: 0.8585\nEpoch 114/150\n - 4s - loss: 0.3124 - accuracy: 0.8522 - val_loss: 0.2976 - val_accuracy: 0.8593\nEpoch 115/150\n - 4s - loss: 0.3133 - accuracy: 0.8519 - val_loss: 0.2893 - val_accuracy: 0.8635\nEpoch 116/150\n - 4s - loss: 0.3138 - accuracy: 0.8516 - val_loss: 0.3008 - val_accuracy: 0.8551\nEpoch 117/150\n - 4s - loss: 0.3183 - accuracy: 0.8494 - val_loss: 0.2989 - val_accuracy: 0.8586\nEpoch 118/150\n - 4s - loss: 0.3137 - accuracy: 0.8517 - val_loss: 0.3018 - val_accuracy: 0.8577\nEpoch 119/150\n - 3s - loss: 0.3101 - accuracy: 0.8538 - val_loss: 0.2981 - val_accuracy: 0.8569\nEpoch 120/150\n - 2s - loss: 0.3133 - accuracy: 0.8523 - val_loss: 0.2976 - val_accuracy: 0.8567\nEpoch 121/150\n - 2s - loss: 0.3109 - accuracy: 0.8529 - val_loss: 0.2930 - val_accuracy: 0.8607\nEpoch 122/150\n - 2s - loss: 0.3101 - accuracy: 0.8535 - val_loss: 0.2965 - val_accuracy: 0.8584\nEpoch 123/150\n - 3s - loss: 0.3139 - accuracy: 0.8516 - val_loss: 0.3024 - val_accuracy: 0.8576\nEpoch 124/150\n - 4s - loss: 0.3115 - accuracy: 0.8529 - val_loss: 0.2907 - val_accuracy: 0.8611\nEpoch 125/150\n - 4s - loss: 0.3100 - accuracy: 0.8532 - val_loss: 0.2922 - val_accuracy: 0.8626\nEpoch 126/150\n - 3s - loss: 0.3153 - accuracy: 0.8517 - val_loss: 0.3035 - val_accuracy: 0.8557\nEpoch 127/150\n - 3s - loss: 0.3101 - accuracy: 0.8535 - val_loss: 0.2904 - val_accuracy: 0.8624\nEpoch 128/150\n - 2s - loss: 0.3115 - accuracy: 0.8526 - val_loss: 0.2980 - val_accuracy: 0.8591\nEpoch 129/150\n - 4s - loss: 0.3098 - accuracy: 0.8536 - val_loss: 0.2916 - val_accuracy: 0.8622\nEpoch 130/150\n - 4s - loss: 0.3099 - accuracy: 0.8537 - val_loss: 0.2956 - val_accuracy: 0.8584\nEpoch 131/150\n - 4s - loss: 0.3089 - accuracy: 0.8541 - val_loss: 0.2937 - val_accuracy: 0.8602\nEpoch 132/150\n - 3s - loss: 0.3103 - accuracy: 0.8531 - val_loss: 0.2922 - val_accuracy: 0.8601\nEpoch 133/150\n - 4s - loss: 0.3151 - accuracy: 0.8515 - val_loss: 0.3039 - val_accuracy: 0.8567\nEpoch 134/150\n - 4s - loss: 0.3139 - accuracy: 0.8521 - val_loss: 0.2920 - val_accuracy: 0.8620\nEpoch 135/150\n - 4s - loss: 0.3099 - accuracy: 0.8528 - val_loss: 0.2908 - val_accuracy: 0.8626\nEpoch 136/150\n - 4s - loss: 0.3093 - accuracy: 0.8537 - val_loss: 0.2933 - val_accuracy: 0.8631\nEpoch 137/150\n - 4s - loss: 0.3106 - accuracy: 0.8536 - val_loss: 0.3051 - val_accuracy: 0.8544\nEpoch 138/150\n - 4s - loss: 0.3107 - accuracy: 0.8530 - val_loss: 0.3040 - val_accuracy: 0.8549\nEpoch 139/150\n - 4s - loss: 0.3100 - accuracy: 0.8532 - val_loss: 0.2892 - val_accuracy: 0.8628\nEpoch 140/150\n - 4s - loss: 0.3092 - accuracy: 0.8533 - val_loss: 0.2935 - val_accuracy: 0.8603\nEpoch 141/150\n - 4s - loss: 0.3157 - accuracy: 0.8509 - val_loss: 0.5051 - val_accuracy: 0.7945\nEpoch 142/150\n - 4s - loss: 0.3150 - accuracy: 0.8513 - val_loss: 0.2948 - val_accuracy: 0.8604\nEpoch 143/150\n - 2s - loss: 0.3111 - accuracy: 0.8529 - val_loss: 0.2884 - val_accuracy: 0.8630\nEpoch 144/150\n - 4s - loss: 0.3094 - accuracy: 0.8537 - val_loss: 0.2892 - val_accuracy: 0.8624\nEpoch 145/150\n - 4s - loss: 0.3102 - accuracy: 0.8530 - val_loss: 0.3069 - val_accuracy: 0.8549\nEpoch 146/150\n - 4s - loss: 0.3101 - accuracy: 0.8533 - val_loss: 0.2947 - val_accuracy: 0.8606\nEpoch 147/150\n - 4s - loss: 0.3125 - accuracy: 0.8525 - val_loss: 0.2926 - val_accuracy: 0.8598\nEpoch 148/150\n - 5s - loss: 0.3079 - accuracy: 0.8543 - val_loss: 0.2931 - val_accuracy: 0.8595\nEpoch 149/150\n - 3s - loss: 0.3107 - accuracy: 0.8533 - val_loss: 0.2940 - val_accuracy: 0.8615\nEpoch 150/150\n - 4s - loss: 0.3084 - accuracy: 0.8529 - val_loss: 0.2865 - val_accuracy: 0.8646\n\n\n\nacc = history.history['loss']\nval_acc = history.history['val_loss']\n\nplt.plot(np.arange(len(history.history['accuracy'])), acc, label='Training loss')\nplt.plot(np.arange(len(history.history['accuracy'])), val_acc,label='Validation loss')\nplt.title('FCN - Training and validation accuracy T ='+str(T))\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n\n\n\nIn the ANDI challenge, the rank of task 2 is evaluated with the F1 score. Let’s see how well/bad we did (best value is 1). Recall that this is a toy example, as we are not considergin LW.\n\nfrom sklearn.metrics import f1_score\n\ngroundtruth = np.argmax(Y_e, axis = 1)\npredictions = np.argmax(model.predict(X_e), axis = 1)\n\nf1_score(groundtruth, predictions, average='micro')\n\n0.7168055555555556\n\n\nNot that bad! To analyze a bit more the predictions, we can use the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\nconf = confusion_matrix(groundtruth, predictions)/(predictions.shape[0]/2)\n\n\nfig, ax = plt.subplots(figsize = (7,7))\nax.matshow(conf)\nfor (i, j), z in np.ndenumerate(conf):\n    ax.text(j, i, '{:0.3f}'.format(z), ha='center', va='center', fontsize = 16)\nax.set_xticklabels(['c','ATTM','CTRW','FBM','SBM'], fontsize = 16)\nax.set_yticklabels(['a','ATTM','CTRW','FBM','SBM'], fontsize = 16)\nax.set_xlabel('Predicted class', fontsize = 16)\nax.set_ylabel('Groundtruth', fontsize = 16)\nax.xaxis.set_ticks_position('bottom')\n\n\n\n\nWe see here that the method is not perfect. For instance, it has a very hard time correctly classifying trajectories ATTM trajectories. For CTRW, the job is easier! Take into account that here we are working with trajectories without noise, contrary to what we have in the Challenge.\n\nNow you are ready to use your favourite ML architecture on the true ANDI dataset! Can you do better?"
  },
  {
    "objectID": "tutorials/challenge_two_datasets.html",
    "href": "tutorials/challenge_two_datasets.html",
    "title": "Phenom models and ANDI 2022",
    "section": "",
    "text": "Model 1 - Single state diffusion\nThis is the most boring model of all… It simulates a particle with a single diffusive state, with an associated \\(D\\) and \\(\\alpha\\). This kind of trajectories can also be generated with the class andi_datasets.models_theory, check its associated tutorial to know more.\nAll models have similar inputs, with some changing to accomodate the particularities of each model. In general, we input the number of trajectories to be generated N, the number of time steps T, and then length of the box acting as environment L. Let’s us define these and keep them for the rest of the notebook. We will also define here the diffusion coefficient.\n\n# number of time steps per trajectory (frames)\nT = 500\n# number of trajectories\nN = 50\n# Length of box (pixels)\nL = 1.5*128\n# diffusion coefficient (pixels^2 / frame)\nD = 0.1\n\nLet’s us generate some trajectories with anomalous exponent alpha = 0.5. Remember that all trajectories follow FBM! The output of all the functions is the same: a tuple whose first elements are the trajectories and second are its labels. Let’s focus on the first now:\n\ntrajs_model1, _ = models_phenom().single_state(N = N, \n                                                L = L,\n                                                T = T,\n                                                D = D,\n                                                alpha = 0.5\n                                                )\n\nThe trajectories will have always the same shape: time, number of trajectories and number of dimensions (two for this challenge).\n\ntrajs_model1.shape\n\n(500, 50, 2)\n\n\nLet’s take a look at some of the trajectories with a built-in functions from the library:\n\nfrom andi_datasets.utils_trajectories import plot_trajs\n\nplot_trajs(trajs_model1, L, N, num_to_plot = 4)\n\n\n\n\n\n\nModel 2 - Multiple state diffusion\nNow we consider the motion of a particle that changes between different diffusive state. We are free to choose the number of states. The probability of jumping from one state to another is given by the transition matrix \\(M\\). Let’s consider a two-state diffusion process. It’s transition matrix is\n\\(M = \\begin{pmatrix} P_{11} & P_{12} \\\\ P_{21} & P_{22} \\end{pmatrix} = \\begin{pmatrix} 0.98 & 0.02 \\\\ 0.02 & 0.98 \\end{pmatrix},\\)\nwhere \\(P_{ij}\\) is the probability of changing from state \\(i\\) to state \\(j\\) at every time step. Each diffusive state can have its own diffusion coefficient and anomalous exponent. For instance, we consider a fast state with \\(D_f = 10 D\\) and \\(\\alpha_f = 1.2\\) and a slow state with \\(D_s = 0.1 D\\) and \\(\\alpha_s = 0.7\\). Given this, let’s create some trajectories!\n\ntrajs_model2, labels_model2 = models_phenom().multi_state(N = N, \n                                                          L = L,\n                                                          T = T,\n                                                          alphas = [1.2, 0.7],\n                                                          Ds = [10*D, 0.1*D],\n                                                          M = np.array([[0.98, 0.02], [0.02, 0.98]])\n                                                            )\n\nSee that now we have define the labels of the trajectories. This contain very important information: the value of \\(\\alpha\\) and \\(D\\) at each time step. For instance, let’s check the values of the labels for the first particle:\n\nprint(r'alphas:', np.unique(labels_model2[:, 0, 0]), 'D:', np.unique(labels_model2[:, 0, 1]) )\n\nalphas: [0.7 1.2] D: [0.01 1.  ]\n\n\nNow, let’s plot them to see how they change over time. You can see them in third row of the next plot:\n\nplot_trajs(trajs_model2, L, N, labels = labels_model2, plot_labels = True)\n\n\n\n\nAs you can see, \\(\\alpha\\) and \\(D\\) change over time randomly, following the transition matrix \\(M\\). If you prefer to work with changepoints and physical properties in the different systems, you can use the following function for it:\n\nfrom andi_datasets.datasets_phenom import continuous_label_to_list\n\nchangepoints, alphas, Ds = continuous_label_to_list(labels_model2[:, 0, :])\n\nprint('changepoints:', changepoints,\n      '\\nalphas:', alphas,\n      '\\nDs:', Ds)\n\nchangepoints: [  7  41  62 129 182 278 296 319 341 348 451] \nalphas: [1.2 0.7 1.2 0.7 1.2 0.7 1.2 0.7 1.2 0.7 1.2 0.7] \nDs: [1.   0.01 1.   0.01 1.   0.01 1.   0.01 1.   0.01 1.   0.01]\n\n\nThe previous gives \\(C\\) changepoints, which creates \\(C+1\\) segments, and hence \\(C+1\\) \\(\\alpha\\)s and \\(D\\)s.\n\n\nModel 3 - Dimerization\nThe third model considers a set of trajectories moving all in the same environment. They have all the same properties: diffusion coefficient, anomalous exponent and radius. If two particles get closer than a distance \\(2r\\), they bind with probability \\(P_b\\) creating a dimer. Similarly, two dimerized particles may unbind with probability \\(P_u\\). The latter defines two diffusive states. Just as before, we can define the diffusion coefficients and exponents of both states. Here is an example of such process:\n\ntrajs_model3, labels_model3 = models_phenom().dimerization(N = N, \n                                                           L = L,\n                                                           T = T,\n                                                           alphas = [1.2, 0.7],\n                                                           Ds = [10*D, 0.1*D],\n                                                           r = 1, # radius of the particles\n                                                           Pb = 1, # binding probability\n                                                           Pu = 0 # unbinding probability\n                                                           )\n\n\n\n\nNow, let’s plot them to see how they change over time. You can see them in third row of the next plot:\n\nplot_trajs(trajs_model3, L, N, labels = labels_model3, plot_labels = True)\n\n\n\n\nAs you can see, particle 2 and 3 met and change their diffusive state!\n\n\nModel 4 - Immobile traps\nThe forth model consider the presence of immobile traps of radius \\(r\\) which completely immobilize the particles. Similar to the dimerization model, the particles have a probability \\(P_b\\) of getting trapped if closer to the trap than a distance \\(r\\) and a probability \\(P_u\\) of escaping from it. The generator of this trajectories allows us (optional) to set the position of the traps. Let’s throw them randomly over the box:\n\nnumber_traps = 100\ntraps_positions = np.random.rand(number_traps, 2)*L\n\nNow, let’s put some particles in the system, with usual diffusion coefficient \\(D\\) and an \\(\\alpha = 1.6\\).\n\ntrajs_model4, labels_model4 = models_phenom().immobile_traps(N = N,\n                                                             T = T,                \n                                                             L = L,\n                                                             r = 1, # radius of the traps\n                                                             Pu = 0.01, # Unbinding probability\n                                                             Pb = 1, # Binding probability\n                                                             D = D, # Diffusion coefficients of moving state\n                                                             alpha = 1.6, # Anomalous exponents of moving state\n                                                             Nt = number_traps, # number of traps\n                                                             traps_pos = traps_positions\n                                                             )\n\n\n\n\n\nplot_trajs(trajs_model4, L, N, \n           labels = labels_model4, plot_labels = True,\n           traps_positions = traps_positions)\n\n\n\n\nAs you can see, in this case the particles superdiffuse until the hit a trap, moment in which they get immobilized!\n\n\nModel 5 - Confinement\nLast but not least, we have one of the most relevant models of the challenge: the presence of compartments, whose boundaries can prevent particles to exit them. Note that we consider here the case of osmotic boundaries, which means that the particles will always enter the compartment. Once inside, the boundaries have a certain transmittance trans, i.e. the probability of the particle exiting the compartment. Just as the case of immobile traps, we can define (optionally) a priori the compartments’ distribution. Note that we always consider here circular compartment of given radius r. You can use the following built-in function to distribute cercles over the environment, without overlap. If the algorithm does manage to place them all, a warning is printed, and only the circles states are placed.\n\nnumber_compartments = 100;\nradius_compartments = 10\ncompartments_center = models_phenom._distribute_circular_compartments(Nc = number_compartments, \n                                                                      r = radius_compartments,\n                                                                      L = L # size of the environment\n                                                                      )\n\nc:\\users\\gorka\\github\\andi_datasets_dev\\andi_datasets\\models_phenom.py:556: UserWarning: Could accomodate 54 circles of the 100 requested. Increase size of environment or decrease radius of compartments.\n  warnings.warn(warn_str)\n\n\nWe can check the distribution of the compartments like below. As you can see, the environment is quite dense!\n\nfig, ax = plt.subplots(figsize = (4,4))\nfor c in comp_center:\n    circle = plt.Circle((c[0], c[1]), radius_compartments, facecolor = 'None', edgecolor = 'C1', zorder = 10)\n    ax.add_patch(circle) \nplt.setp(ax, xlim = (0, L), ylim = (0, L))\n\n[0.0, 192.0, 0.0, 192.0]\n\n\n\n\n\nNow, let’s introduce some diffusive particles in this environment. We will consider a quite extreme case, to nicely show the effect of the compartments. Outside the comparments, the particles will move very, very fast, at \\(D_f = 1500D\\). Replicating what we see in many biological scenarios, inside the compartments the particles will move much slower, \\(D_s = 50 D\\). Let’s keep the both diffusive with same anomalous exponent, \\(\\alpha = 1\\).\nComment on default values: the default values for alpha are always 1 for all methods. Hence, if no exponents are given, all diffusive states are normally diffusing. Most of the variables of the functions presented here have default values, with similar values as the ones used in this notebook. If you want to know more, you can check their source code.\nFor the boundaries of the compartments, let’s consider a transmittance trans = 0.2:\n\ntrajs_model5, labels_model5 = models_phenom().confinement(N = N,\n                                                          L = L,\n                                                          Ds = [1500*D, 50*D],\n                                                          comp_center = compartments_center,\n                                                          r = radius_compartments,\n                                                          trans = 0.2 # boundary transmittance\n                                                           )\n\nLet’s see the resulting trajectories, superimposed to the cercles we create just before.\n\nplot_trajs(trajs_model5, L, N, \n           comp_center = compartments_center,\n           r_cercle = radius_compartments,\n           plot_labels = True, labels = labels_model5\n           )\n\n\n\n\nWith these extreme diffusion coefficient, he make it such that the particles move very fast from one compartment to the other. Once inside, the low transmittance of the boundaries make it such that they stay quite some time inside!\n\n\nCreating datasets with multiple models\nThe library allows to generate simultaneously trajectories from all the previous models, and gather them in a single dataset. To do so, we have created the the class datasets_phenom. Let’s import it and see some of its properties:\n\nfrom andi_datasets.datasets_phenom import datasets_phenom\n\nOne of the first things we can check is the models that we can access with this class. Spoiler: they are the same we just reviewed above!\n\ndatasets_phenom().avail_models_name\n\n['confinement',\n 'dimerization',\n 'immobile_traps',\n 'multi_state',\n 'single_state']\n\n\nAs we have seen previously, each model has its own parameters. As we commented, most of them have some meaningful default values, so we can skip them when generating trajectories. Nonetheless, if you want to know the parameters for a given model, you can use:\n\nmodel = 'confinement'\ndatasets_phenom()._get_inputs_models(model)\n\n['N', 'T', 'Ds', 'alphas', 'L', 'deltaT', 'r', 'comp_center', 'Nc', 'trans']\n\n\nOf course, you can also check the source code of the models to inspect them.\nIn order to generate trajectories, we first need to set: 1) the models we want to generate trajectories from; 2) the properties of each of the models. For that, we need to create a dictionary that stores all the needed information. For each model, a dictionary is needed. Hence, for multiple models, we will need to input a list of dictionaries. For example, let’s create a dataset with trajectories from the dimerization (model 3) and confinement (model 5) models. We will make use of the default values of the previous models, and only input the binding/unbinding probability for the former and the transmittance for the latter.\n\ndict_model3 = {'model': 'dimerization', \n               'L': L,\n               'Pu': 0.1, 'Pb': 1}\n\ndict_model5 = {'model': 'confinement', \n               'L': L,\n               'trans': 0.2}\n\ndict_all = [dict_model3, dict_model5]\n\ntrajs, labels = datasets_phenom().create_dataset(N_model = N, # number of trajectories per model\n                                                 T = T,\n                                                 dics = dict_all\n                                                )\n\n\n\n\nThe trajectories arising from the previous function are ordered following the order of the input list of dictionaries. We requested N_model = N = 50 trajectories per model, hence the dataset will have a total of 100 trajectories. The first 50 will come from the dimerization model and the last 50 from the confinement model.\n\nfrom andi_datasets.utils_trajectories import plot_trajs\n\nplot_trajs(trajs, L, N, \n           plot_labels = True, labels = labels\n           )\n\n\n\n\n\nSaving and loading datasets\nJust as with the datasets_theory class (check here the details), you can save and load datasets, so that you avoid creating trajectories every time you need them. The same function presented above has such options:\n\ntrajs, labels = datasets_phenom().create_dataset(N_model = N, # number of trajectories per model\n                                                 T = T,\n                                                 dics = dict_all,\n                                                 save = True, path = 'datasets_folder'\n                                                )\n\n\n\n\nSaving creates two files per model:\n\na .csv file, containing the details of the dataset generated. This file stores the values of the parameters of the saved dataset. This file helps organizing all the saved datasets and is used later to know, given some parameters, from where to load the trajectories.\na .npy file, containing the trajectories and labels of the generated trajectories. Each file will have an integer number at end, which is used, together with the csv file, in order to know the properties of the dataset.\n\nAfter saving, we can just load the dataset with the same function:\n\ntrajs, labels = datasets_phenom().create_dataset(N_model = N, # number of trajectories per model\n                                                 T = T,\n                                                 dics = dict_all,\n                                                 save = True, path = 'datasets_folder/'\n                                                )"
  },
  {
    "objectID": "tutorials/changes v2.html",
    "href": "tutorials/changes v2.html",
    "title": "andi_datasets",
    "section": "",
    "text": "class andi \\(\\rightarrow\\) andi_theory\nfunction andi.andi_dataset \\(\\rightarrow\\) andi_theory.challenge_2020_dataset\nvariable name change in andi_theory.create_dataset: N \\(\\rightarrow\\) N_model. In this way, it is more clear that N_model refers to number of trajectories per model and we use N for total number of trajectories in other functions\ncorrected how noise is applied in Task3. Now the noise is added after the segmentation.\nAdded an extra variable in andi_theory.challenge_2020_dataset, return_noise which, if True, makes the function output the noise amplitudes added to each trajectory\nChange how noise is applied in andi_theory.challenge_2020_dataset, such that all components of trajectories with dimension 2 and 3 have the same noise amplitude.\n\n\n\n\nsimplifiy num_per_class such that N_models can only be int (see datasets_phenom)"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html",
    "href": "tutorials/creating_videos_phenom.html",
    "title": "Creating videos tutorial",
    "section": "",
    "text": "import sys\nsys.path.append('..')"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#single-state-diffusion",
    "href": "tutorials/creating_videos_phenom.html#single-state-diffusion",
    "title": "Creating videos tutorial",
    "section": "1. Single state diffusion",
    "text": "1. Single state diffusion\n\nT = 10 # number of time steps (frames)\nN = 50 # number of particles (trajectories)\nL = 1.5 * 128 # length of the box (pixels) -> exteneding fov by 1.5 times\nD = 0.1 # diffusion coefficient (pixels^2/frame)\n\n\ntrajs_model1, labels = models_phenom().single_state(N=N, L=L, T=T, Ds=D, alphas=0.5)"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#multi-state-diffusion",
    "href": "tutorials/creating_videos_phenom.html#multi-state-diffusion",
    "title": "Creating videos tutorial",
    "section": "2. Multi state diffusion",
    "text": "2. Multi state diffusion\n\ntrajs_model2, labels_model2 = models_phenom().multi_state(\n    N=N,\n    L=L,\n    T=T,\n    alphas=[1.2, 0.7],\n    Ds=[10 * D, 0.1 * D],\n    M=np.array([[0.98, 0.02], [0.02, 0.98]]),\n)"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#dimerization",
    "href": "tutorials/creating_videos_phenom.html#dimerization",
    "title": "Creating videos tutorial",
    "section": "3. Dimerization",
    "text": "3. Dimerization\n\ntrajs_model3, labels_model3 = models_phenom().dimerization(\n    N=N,\n    L=L,\n    T=T,\n    alphas=[1.2, 0.7],\n    Ds=[10 * D, 0.1 * D],\n    r=1,  # radius of the particles\n    Pb=1,  # binding probability\n    Pu=0,  # unbinding probability\n)\n\n\nt3 = np.moveaxis(trajs_model3, 0, 1)\nfor traj in t3:\n    plt.plot(traj[:,0], traj[:,1])\nplt.show()"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#immobile-traps",
    "href": "tutorials/creating_videos_phenom.html#immobile-traps",
    "title": "Creating videos tutorial",
    "section": "4. Immobile traps",
    "text": "4. Immobile traps\n\nnumber_traps = 100\ntraps_positions = np.random.rand(number_traps, 2)*L/2\n\n\ntrajs_model4, labels_model4 = models_phenom().immobile_traps(\n    N=N,\n    T=T,\n    L=L,\n    r=1,  # radius of the traps\n    Pu=0.01,  # Unbinding probability\n    Pb=1,  # Binding probability\n    Ds=D,  # Diffusion coefficients of moving state\n    alphas=1.6,  # Anomalous exponents of moving state\n    Nt=number_traps,  # number of traps\n    traps_pos=traps_positions,\n)"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#confinement",
    "href": "tutorials/creating_videos_phenom.html#confinement",
    "title": "Creating videos tutorial",
    "section": "5. Confinement",
    "text": "5. Confinement\n\nnumber_compartments = 100\nradius_compartments = 10\ncompartments_center = models_phenom._distribute_circular_compartments(\n    Nc=number_compartments, r=radius_compartments, L=L  # size of the environment\n)\n\n\ntrajs_model5, labels_model5 = models_phenom().confinement(\n    N=N,\n    L=L,\n    Ds=[1500 * D, 50 * D],\n    comp_center=compartments_center,\n    r=radius_compartments,\n    trans=0.2,  # boundary transmittance\n)"
  },
  {
    "objectID": "tutorials/creating_videos_phenom.html#videos",
    "href": "tutorials/creating_videos_phenom.html#videos",
    "title": "Creating videos tutorial",
    "section": "6. Videos",
    "text": "6. Videos\nFor generating videos we import get_video_andi function from andi_datasets package\n\nfrom andi_datasets.utils_videos import transform_to_video, play_video\n\n\n6.1. Generating videos\nThe trajectory data generated through models_phenom() can be directly passed through transform_to_video function to generate fluorescence images of the particles (and the particle masks). By default, transform_to_video will output just the videos. If the list get_vip_particles is non-empty, the masks of the vip particles in the first frame are appended to the first frame of the output. If the with_masks is set to True, the function will output both the video and masks. In this case, the function can be called as video, masks = transform_to_video(...., with_masks=True).\nThe properites of the videos can be controlled by the dictionaries particle_props, optics_props, and background_props.\n\nparticle_props: The intensity of various particles in the final image can be controlled with particle_intensity by giving a mean intensity and standard deviation. The intensity variation of a single particle within multiple frames can be controlled by giving a standard deviation in intensity_variation. Note that the intensity variation will be applied on the randomly sampled particle intensities from the first frame\noptics_props: Most of the optics properties are fixed to match the experimental conditions. But they can still be modified accordingly. Please check the available properties for optics_props in utils_videos.ipynb\nbackground_props: The mean background offset and the standard deviation of background within multiple frames can be controlled with background_mean and background_stdev. A combination of particle_intensity and background_meancan be used to tune the noise in the final image. For example., a low singal to noise ratio can be obtained by bringing the particle intensities closer to the background value\n\n\norigin = 0\nwidth = 200\nvideo, masks = transform_to_video(\n    trajs_model1,\n\n    particle_props={\n        \"particle_intensity\": [100, 10],  # [mean, stdev]\n        \"intensity_variation\": 0         # intensity variation\n    },\n\n    optics_props={\n        \"output_region\": [origin, origin, width+origin,width+origin], # Default region is [0, 0, 128, 128]\n    },\n\n    background_props={\n        \"background_mean\": 10,  # Mean background\n        \"background_stdev\": 5   # Stdev background\n    },\n\n    with_masks=True\n    \n)\n\n\nfrom matplotlib.patches import Rectangle\nplt.figure(figsize=(10,10))\nplt.imshow(video[0], cmap=\"gray\")\nplt.show()\n\n\n\n\n\nfig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(20, 10))\nax0.imshow(video[0], cmap=\"gray\")\nax0.set_title(\"Frame\")\nax1.imshow(masks[0], cmap=\"gray\")\nax1.set_title(\"Mask\")\n\nText(0.5, 1.0, 'Mask')\n\n\n\n\n\n\n\n6.2 Visualizing the paricles and masks in the first frame\nPlotting the first frame of the video along with the particle mask\nOverlaying the particle trajectories on the first frame of the video\n\nplt.figure(figsize=(10, 10))\nplt.imshow(video[0], cmap=\"gray\")\nfor traj in np.moveaxis(trajs_model1, 0, 1):\n    plt.plot(traj[:,1], traj[:,0], alpha=0.5)\nplt.show()\n\n\n\n\nVisualzing the generated videos using play_video function\n\nplay_video(video, figsize=(10, 10)) # can also be used with masks\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n6.2 Save the videos\nConvert the video to 8bit and save it as an .MP4 file. Alternatively they can direclty be saved as numpy arrays with np.save(...)\n\ndef convert_uint8(vid):\n    new_vid = []\n    for im in vid:\n        im = im[:,:,0]\n        im = im / im.max()\n        im = im * 255\n        im = im.astype(np.uint8)\n        new_vid.append(im)\n    return new_vid\n\n\nvideo_frames_uint8\n\n[array([[13, 18, 20, ..., 17,  9, 18],\n        [ 4, 24, 12, ...,  9, 13, 12],\n        [15, 13, 18, ..., 15, 13, 15],\n        ...,\n        [17, 10, 17, ..., 12, 23, 10],\n        [20, 18, 20, ...,  7,  7, 20],\n        [10, 15, 13, ..., 15, 10, 15]], dtype=uint8),\n array([[23, 16, 14, ..., 22, 17, 19],\n        [13, 14, 25, ..., 11, 26, 19],\n        [19, 14,  8, ..., 11, 14, 19],\n        ...,\n        [14, 16, 20, ..., 16, 22, 19],\n        [16, 22, 17, ..., 17, 25, 37],\n        [10, 20, 14, ..., 17, 11, 26]], dtype=uint8),\n array([[22, 24, 18, ..., 20, 19, 23],\n        [23, 15, 16, ..., 18, 18, 15],\n        [16, 11, 16, ..., 18, 11, 18],\n        ...,\n        [13, 15, 11, ..., 22, 23, 20],\n        [16, 15, 23, ..., 15, 16, 19],\n        [19, 18, 24, ..., 20, 15, 12]], dtype=uint8),\n array([[11, 12, 21, ..., 25, 12,  9],\n        [ 9,  8,  5, ..., 14, 19,  7],\n        [ 9, 12, 16, ..., 21, 22,  7],\n        ...,\n        [ 5,  9, 18, ..., 16, 15, 15],\n        [21, 19, 11, ..., 12, 21, 12],\n        [ 5, 12, 16, ...,  8, 11,  9]], dtype=uint8),\n array([[ 9, 11,  9, ...,  9,  8,  6],\n        [ 9, 14,  4, ..., 14,  6, 12],\n        [16,  6, 12, ...,  6,  6, 12],\n        ...,\n        [ 6,  8, 16, ...,  4,  8, 12],\n        [11, 14,  9, ...,  9,  9,  9],\n        [ 6, 12, 12, ...,  4,  8, 11]], dtype=uint8),\n array([[ 5, 20,  8, ..., 13, 11, 18],\n        [17, 11, 20, ..., 11, 21, 11],\n        [13, 14, 13, ..., 11,  8, 10],\n        ...,\n        [15,  7,  8, ..., 11, 17, 20],\n        [26, 15, 21, ..., 17, 15, 18],\n        [17, 11, 17, ..., 10, 10, 17]], dtype=uint8),\n array([[14, 15, 15, ..., 14, 20, 22],\n        [11,  8, 21, ..., 10, 12, 18],\n        [15, 15,  5, ..., 10, 11, 14],\n        ...,\n        [ 8, 17,  8, ..., 11, 15,  7],\n        [17,  4, 21, ..., 12, 20, 11],\n        [17, 17, 12, ..., 18, 20, 15]], dtype=uint8),\n array([[10, 16, 13, ...,  7, 10, 13],\n        [10,  8, 23, ..., 11, 13,  8],\n        [14, 16, 11, ..., 13, 11, 17],\n        ...,\n        [10, 11, 16, ...,  5, 16, 22],\n        [20, 25, 19, ..., 14, 16, 11],\n        [11,  8, 16, ..., 16, 22, 13]], dtype=uint8),\n array([[ 6, 26, 10, ..., 16, 15,  6],\n        [15, 15, 23, ..., 12, 15, 10],\n        [23, 13, 15, ..., 16, 16, 12],\n        ...,\n        [15, 12, 13, ..., 12, 12, 18],\n        [15, 10,  9, ..., 19, 19, 16],\n        [10, 15, 16, ..., 18, 10, 15]], dtype=uint8),\n array([[12,  6, 13, ..., 13, 19,  9],\n        [16, 19,  9, ..., 24,  9,  6],\n        [ 9, 18, 18, ..., 10,  9, 18],\n        ...,\n        [13, 10, 16, ..., 10, 13,  7],\n        [ 9, 10,  9, ..., 15, 12, 12],\n        [15, 16, 10, ..., 18, 19, 10]], dtype=uint8)]\n\n\n\n# change the path accordingly\nsave_path = \"phenom_model1.tiff\"\n\n\nvideo_frames_uint8 = convert_uint8(video)\nimageio.mimwrite(save_path, video_frames_uint8)\n\nThe following code will not work anymore - Needs to be deleted"
  },
  {
    "objectID": "tutorials/index_tutorials.html",
    "href": "tutorials/index_tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Theory models and AnDi 2020: this tutorials introduced the theoretical models contained in this library, as well as the generation of datasets for the AnDi 2020 challenge.\nANDI 2020 submission: from trajectories to predictions: this tutorial shows how to manage the datasets given in the AnDi 2020 challenge and shows how to do predictions both with an statistical approach as with machine learning method.\nPhenom models and ANDI 2023: this tutorial overviews the phenomenological models contained in the library, as well as how to generate datasets for the AnDi 2023 challenge.\nCreating videos tutorial: this tutorial shows how to generate videos using combining the deep-track library together with andi-datasets."
  }
]